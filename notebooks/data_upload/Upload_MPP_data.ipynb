{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283d56a9",
   "metadata": {},
   "source": [
    "# MPP Data Upload Notebook\n",
    "\n",
    "This notebook processes and uploads Maximum Power Point (MPP) tracking data from text files to the TimescaleDB database.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "- Scan directories for MPP data files (output_board{X}_channel{Y}.txt files)\n",
    "- Parse the data into structured format\n",
    "- Upload the data to the TimescaleDB database\n",
    "- Avoid duplicate data entries\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Running TimescaleDB instance (configured in docker-compose.yml)\n",
    "- Access to directory containing MPP data files\n",
    "- Environment variables configured in .env file (for database connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8129a",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import required libraries and install any missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4840fec7-727b-4454-bed3-25d603203b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install psycopg2-binary sqlalchemy pandas tqdm pathlib python-dotenv ipynb-path\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a57a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb_path\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    notebook_file_path = ipynb_path.get()\n",
    "    print(f\"Successfully got notebook path: {notebook_file_path}\")\n",
    "    notebook_dir = os.path.dirname(notebook_file_path)\n",
    "    print(f\"Notebook directory: {notebook_dir}\")\n",
    "    os.chdir(notebook_dir)\n",
    "    print(f\"Successfully changed CWD to: {os.getcwd()}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"\\nFull traceback:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1a114d-c1f8-4b35-b2e1-16d3ccb9de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# Database libraries\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b57a6",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load configuration from environment variables or use defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d1a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "# Look for the .env file two directories up from the notebook location\n",
    "dotenv_path = Path(\"../../.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Database configuration from container environment variables with fallbacks\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('POSTGRES_HOST', 'timescaledb'),  # Use container service name\n",
    "    'port': int(os.getenv('POSTGRES_PORT', 5432)),\n",
    "    'database': os.getenv('POSTGRES_DB', 'perocube'),\n",
    "    'user': os.getenv('POSTGRES_USER', 'postgres'),\n",
    "    'password': os.getenv('POSTGRES_PASSWORD', 'postgres')\n",
    "}\n",
    "\n",
    "# Print database connection info (excluding password)\n",
    "print(f\"Database connection: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']} as {DB_CONFIG['user']}\")\n",
    "\n",
    "# Data directory configuration - using relative path from notebook location\n",
    "ROOT_DIRECTORY = str(Path(\"sample_data/datasets/PeroCube-sample-data\").resolve())\n",
    "\n",
    "# File matching pattern\n",
    "MPP_FILE_PATTERN = r\"output_board(\\d+)_channel(\\d+)\"\n",
    "\n",
    "# Batch size for database operations\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "# Data validation configuration\n",
    "VALIDATION_CONFIG = {\n",
    "    'enabled': False,  # Master switch for validation\n",
    "    'remove_nan': True,  # Always remove NaN values\n",
    "    'validate_ranges': False,  # Optional physical value validation\n",
    "    'ranges': {\n",
    "        'voltage': {'min': 0, 'max': 100},  # Voltage range in V\n",
    "        'current': {'min': -1, 'max': 100}, # Current range in mA\n",
    "        'power': {'min': 0, 'max': 1000}    # Power range in mW\n",
    "    }\n",
    "}\n",
    "\n",
    "def print_validation_config():\n",
    "    \"\"\"Print current validation configuration for user awareness\"\"\"\n",
    "    print(\"\\nData Validation Configuration:\")\n",
    "    print(f\"- Validation enabled: {VALIDATION_CONFIG['enabled']}\")\n",
    "    print(f\"- Remove NaN values: {VALIDATION_CONFIG['remove_nan']}\")\n",
    "    if VALIDATION_CONFIG['enabled'] and VALIDATION_CONFIG['validate_ranges']:\n",
    "        print(\"\\nPhysical value ranges:\")\n",
    "        for measure, limits in VALIDATION_CONFIG['ranges'].items():\n",
    "            print(f\"- {measure}: {limits['min']} to {limits['max']}\")\n",
    "    else:\n",
    "        print(\"\\nPhysical value validation is disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417f820",
   "metadata": {},
   "source": [
    "## 3. Utility Functions\n",
    "\n",
    "Helper functions for database connection and data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a0906e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_connection(config=DB_CONFIG):\n",
    "    \"\"\"\n",
    "    Create a SQLAlchemy database engine from configuration.\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary containing database connection parameters\n",
    "        \n",
    "    Returns:\n",
    "        SQLAlchemy engine instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection_string = f\"postgresql://{config['user']}:{config['password']}@{config['host']}:{config['port']}/{config['database']}\"\n",
    "        # Store connection string as attribute of engine for external access\n",
    "        engine = create_engine(connection_string)\n",
    "        engine.connection_string = connection_string  # This makes it accessible via engine.connection_string\n",
    "        \n",
    "        # Test the connection\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(\"SELECT 1\"))\n",
    "            logging.info(f\"Database connection successful: {config['host']}:{config['port']}/{config['database']}\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Database connection failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def validate_mpp_data(df, config=VALIDATION_CONFIG):\n",
    "    \"\"\"\n",
    "    Validate MPP data according to the specified configuration.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing MPP measurements\n",
    "        config: Dictionary containing validation configuration\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned and validated DataFrame, along with validation statistics\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df, {'initial_count': 0, 'final_count': 0, 'removed': {}}\n",
    "    \n",
    "    stats = {\n",
    "        'initial_count': len(df),\n",
    "        'final_count': None,\n",
    "        'removed': {\n",
    "            'nan_values': 0,\n",
    "            'voltage_range': 0,\n",
    "            'current_range': 0,\n",
    "            'power_range': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Always ensure timestamp is in UTC\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "    \n",
    "    # Remove NaN values if configured\n",
    "    if config['remove_nan']:\n",
    "        nan_count = df.isna().sum().sum()\n",
    "        df = df.dropna()\n",
    "        stats['removed']['nan_values'] = nan_count\n",
    "    \n",
    "    # Apply physical value validation if enabled\n",
    "    if config['enabled'] and config['validate_ranges']:\n",
    "        for measure, limits in config['ranges'].items():\n",
    "            if measure in df.columns:\n",
    "                invalid_count = len(df[~(df[measure].between(limits['min'], limits['max']))])\n",
    "                df = df[df[measure].between(limits['min'], limits['max'])]\n",
    "                stats['removed'][f'{measure}_range'] = invalid_count\n",
    "    \n",
    "    stats['final_count'] = len(df)\n",
    "    \n",
    "    # Log validation results\n",
    "    logging.info(\"Validation statistics:\")\n",
    "    logging.info(f\"Initial records: {stats['initial_count']}\")\n",
    "    if config['remove_nan']:\n",
    "        logging.info(f\"Removed NaN values: {stats['removed']['nan_values']}\")\n",
    "    if config['enabled'] and config['validate_ranges']:\n",
    "        for measure in config['ranges'].keys():\n",
    "            if stats['removed'][f'{measure}_range'] > 0:\n",
    "                logging.info(f\"Removed {measure} out of range: {stats['removed'][f'{measure}_range']}\")\n",
    "    logging.info(f\"Final records: {stats['final_count']}\")\n",
    "    \n",
    "    return df, stats\n",
    "\n",
    "def check_existing_data(engine, board, channel, timestamps):\n",
    "    \"\"\"\n",
    "    Check if data already exists in the database for given parameters.\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine\n",
    "        board: Board number\n",
    "        channel: Channel number\n",
    "        timestamps: List of timestamps to check\n",
    "        \n",
    "    Returns:\n",
    "        Boolean indicating if data exists\n",
    "    \"\"\"\n",
    "    if not timestamps:\n",
    "        return False\n",
    "        \n",
    "    # For efficiency, just check the min and max timestamps\n",
    "    min_timestamp = min(timestamps)\n",
    "    max_timestamp = max(timestamps)\n",
    "    \n",
    "    # Build a query to check for existing data\n",
    "    query = text(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM mpp_measurement\n",
    "        WHERE timestamp BETWEEN :min_timestamp AND :max_timestamp\n",
    "          AND tracking_channel_board = :board_id\n",
    "          AND tracking_channel_channel = :channel_id\n",
    "    \"\"\")\n",
    "    \n",
    "    # Execute the query\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query, {\n",
    "            \"min_timestamp\": min_timestamp,\n",
    "            \"max_timestamp\": max_timestamp,\n",
    "            \"board_id\": board,\n",
    "            \"channel_id\": channel\n",
    "        })\n",
    "        count = result.scalar()\n",
    "        \n",
    "    # If count > 0, some data exists\n",
    "    return count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "810bcc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_tracking_channels_exist(engine, channels):\n",
    "    \"\"\"\n",
    "    Ensure tracking channels exist in the database before inserting measurements.\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine\n",
    "        channels: List of tuples containing (board, channel) pairs to check/create\n",
    "        \n",
    "    Returns:\n",
    "        Set of (board, channel) tuples that were created\n",
    "    \"\"\"\n",
    "    try:\n",
    "        created = set()\n",
    "        with engine.connect() as conn:\n",
    "            for board, channel in channels:\n",
    "                # Check if the tracking channel exists\n",
    "                result = conn.execute(\n",
    "                    text(\"\"\"\n",
    "                    SELECT 1 \n",
    "                    FROM mpp_tracking_channel \n",
    "                    WHERE board = :board AND channel = :channel\n",
    "                    \"\"\"),\n",
    "                    {\"board\": board, \"channel\": channel}\n",
    "                )\n",
    "                \n",
    "                if not result.fetchone():\n",
    "                    # Create the tracking channel if it doesn't exist\n",
    "                    conn.execute(\n",
    "                        text(\"\"\"\n",
    "                        INSERT INTO mpp_tracking_channel (board, channel) \n",
    "                        VALUES (:board, :channel)\n",
    "                        \"\"\"),\n",
    "                        {\"board\": board, \"channel\": channel}\n",
    "                    )\n",
    "                    created.add((board, channel))\n",
    "                    logging.info(f\"Created tracking channel: board {board}, channel {channel}\")\n",
    "            \n",
    "            conn.commit()\n",
    "        return created\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error ensuring tracking channels exist: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62d1d0-c994-48e0-ac76-dd797484f5bc",
   "metadata": {},
   "source": [
    "## 4. MPP Data Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a027254a-35a5-4b2c-b066-b64905e68c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mpp_files(root_dir, engine, pattern=MPP_FILE_PATTERN, batch_size=BATCH_SIZE, validate=VALIDATION_CONFIG['enabled']):\n",
    "    \"\"\"\n",
    "    Crawls directories starting with 'data', finds files matching the pattern,\n",
    "    reads them into pandas DataFrames, and uploads them to the DB.\n",
    "\n",
    "    Args:\n",
    "        root_dir: The root directory to start the search from.\n",
    "        engine:   SQLAlchemy engine for database connection.\n",
    "        pattern:  Regex pattern to match MPP files.\n",
    "        batch_size: Number of rows to insert in one batch.\n",
    "        validate:   Whether to perform data validation.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistics about the processing.\n",
    "    \"\"\"\n",
    "    # Statistics to return\n",
    "    stats = {\n",
    "        'files_processed': 0,\n",
    "        'files_skipped': 0,\n",
    "        'files_error': 0,\n",
    "        'rows_inserted': 0,\n",
    "        'start_time': datetime.now(timezone.utc),\n",
    "        'total_files': 0,\n",
    "        'channels_created': 0\n",
    "    }\n",
    "\n",
    "    # Convert to Path object for better path handling\n",
    "    root_path = Path(root_dir)\n",
    "    if not root_path.exists():\n",
    "        logging.error(f\"Root directory does not exist: {root_dir}\")\n",
    "        return stats\n",
    "\n",
    "    # Compile the regex pattern for efficiency\n",
    "    pattern_compiled = re.compile(pattern)\n",
    "\n",
    "    # First, collect all matching filepaths and required channels\n",
    "    matching_files = []\n",
    "    required_channels = set()\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        path_parts = Path(dirpath).parts\n",
    "        if any(part.startswith(\"data\") for part in path_parts):\n",
    "            for filename in filenames:\n",
    "                filepath = Path(dirpath) / filename\n",
    "                match = pattern_compiled.search(filename)\n",
    "                if match:\n",
    "                    board = int(match.group(1))\n",
    "                    channel = int(match.group(2))\n",
    "                    required_channels.add((board, channel))\n",
    "                    matching_files.append((filepath, board, channel))\n",
    "\n",
    "    stats['total_files'] = len(matching_files)\n",
    "    logging.info(f\"Found {len(matching_files)} MPP data files to process\")\n",
    "\n",
    "    # Ensure all required tracking channels exist\n",
    "    logging.info(f\"Ensuring {len(required_channels)} tracking channels exist...\")\n",
    "    try:\n",
    "        created_channels = ensure_tracking_channels_exist(engine, required_channels)\n",
    "        stats['channels_created'] = len(created_channels)\n",
    "        if created_channels:\n",
    "            logging.info(f\"Created {len(created_channels)} new tracking channels\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to ensure tracking channels exist: {str(e)}\")\n",
    "        return stats\n",
    "\n",
    "    # Now, process each file\n",
    "    with tqdm(total=len(matching_files), desc=\"Processing MPP Files\") as pbar:\n",
    "        for filepath, board, channel in matching_files:\n",
    "            try:\n",
    "                logging.info(f\"Processing MPP file: {filepath} (Board: {board}, Channel: {channel})\")\n",
    "\n",
    "                # Read the file into a pandas DataFrame\n",
    "                df = pd.read_csv(filepath, sep='\\t',\n",
    "                              names=['timestamp', 'power', 'current', 'voltage'])\n",
    "                                \n",
    "                if df.empty:\n",
    "                    logging.warning(f\"Empty file: {filepath}\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Ensure timestamp is in UTC format\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "\n",
    "                # Add board and channel information to the DataFrame\n",
    "                df['tracking_channel_board'] = board\n",
    "                df['tracking_channel_channel'] = channel\n",
    "\n",
    "                # Validate data if enabled\n",
    "                if validate:\n",
    "                    df, validation_stats = validate_mpp_data(df)\n",
    "                    if df.empty:\n",
    "                        logging.warning(f\"All data filtered during validation: {filepath}\")\n",
    "                        stats['files_skipped'] += 1\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                # Check for existing data\n",
    "                timestamps = df['timestamp'].tolist()\n",
    "                data_exists = check_existing_data(engine, board, channel, timestamps)\n",
    "\n",
    "                if data_exists:\n",
    "                    logging.info(f\"Data already exists for {filepath}. Skipping file.\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                else:\n",
    "                    # Upload data in batches for large files\n",
    "                    total_rows = len(df)\n",
    "                    for i in range(0, total_rows, batch_size):\n",
    "                        batch_df = df.iloc[i:i+batch_size]\n",
    "                        batch_df.to_sql('mpp_measurement', engine, if_exists='append', index=False)\n",
    "                        \n",
    "                    stats['rows_inserted'] += total_rows\n",
    "                    stats['files_processed'] += 1\n",
    "                    logging.info(f\"Successfully uploaded {total_rows} rows from {filepath}\")\n",
    "\n",
    "                # Clean up\n",
    "                del df\n",
    "                pbar.update(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {filepath}: {str(e)}\")\n",
    "                stats['files_error'] += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Calculate duration\n",
    "    stats['end_time'] = datetime.now(timezone.utc)\n",
    "    stats['duration_seconds'] = (stats['end_time'] - stats['start_time']).total_seconds()\n",
    "    \n",
    "    logging.info(f\"Processing complete. Processed {stats['files_processed']} files, \"\n",
    "                 f\"skipped {stats['files_skipped']} files, \"\n",
    "                 f\"errors in {stats['files_error']} files. \"\n",
    "                 f\"Created {stats['channels_created']} tracking channels. \"\n",
    "                 f\"Inserted {stats['rows_inserted']} data points in {stats['duration_seconds']:.2f} seconds.\")\n",
    "                 \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027efb89",
   "metadata": {},
   "source": [
    "## 5. Execute the Data Upload Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e847a3b-a477-4960-b3a3-ad77ef6e13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database connection\n",
    "try:\n",
    "    engine = create_db_connection()\n",
    "    logging.info(\"Database connection established successfully\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to connect to database: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29e4cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine.connection_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08210fb",
   "metadata": {},
   "source": [
    "## Configure Data Validation\n",
    "\n",
    "Review and adjust the validation settings before processing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "757241dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review current validation configuration\n",
    "print_validation_config()\n",
    "\n",
    "# Uncomment and modify these lines to change validation settings\n",
    "# VALIDATION_CONFIG['enabled'] = True\n",
    "# VALIDATION_CONFIG['validate_ranges'] = True\n",
    "# VALIDATION_CONFIG['ranges']['voltage']['max'] = 150  # Adjust range if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9df07af7-7e2f-4c1f-b3b5-2c4654a5095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data processing with the configured root directory\n",
    "print(f\"Starting MPP data processing from directory: {ROOT_DIRECTORY}\")\n",
    "stats = process_mpp_files(ROOT_DIRECTORY, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c93cf",
   "metadata": {},
   "source": [
    "## 6. Results Summary\n",
    "\n",
    "After processing the MPP data files, here's a summary of what was accomplished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1903bb13-401b-46f8-aa03-0a5127af9d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_duration(seconds):\n",
    "    \"\"\"Format duration in a human-readable format\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = seconds % 60\n",
    "    if hours > 0:\n",
    "        return f\"{hours}h {minutes}m {secs:.1f}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}m {secs:.1f}s\"\n",
    "    else:\n",
    "        return f\"{secs:.1f}s\"\n",
    "\n",
    "def format_number(n):\n",
    "    \"\"\"Format number with thousand separators\"\"\"\n",
    "    return f\"{n:,}\"\n",
    "\n",
    "# Display processing statistics\n",
    "if 'stats' in locals():\n",
    "    print(\"📊 File Processing\")\n",
    "    print(\"━━━━━━━━━━━━━━━\")\n",
    "    print(f\"📁 Total files found:           {format_number(stats.get('total_files', 0)):>10}\")\n",
    "    print(f\"✅ Successfully processed:      {format_number(stats.get('files_processed', 0)):>10}\")\n",
    "    print(f\"⏭️  Skipped (existing/empty):    {format_number(stats.get('files_skipped', 0)):>10}\")\n",
    "    print(f\"❌ Errors during processing:    {format_number(stats.get('files_error', 0)):>10}\")\n",
    "    \n",
    "    print(\"\\n📈 Data Statistics\")\n",
    "    print(\"━━━━━━━━━━━━━━━\")\n",
    "    print(f\"📝 Data points inserted:        {format_number(stats.get('rows_inserted', 0)):>10}\")\n",
    "    \n",
    "    if 'duration_seconds' in stats:\n",
    "        duration = format_duration(stats['duration_seconds'])\n",
    "        print(\"\\n⚡ Performance Metrics\")\n",
    "        print(\"━━━━━━━━━━━━━━━━━━\")\n",
    "        print(f\"⏱️  Total processing time:      {duration:>10}\")\n",
    "        \n",
    "        if stats.get('rows_inserted', 0) > 0 and stats.get('duration_seconds', 0) > 0:\n",
    "            throughput = stats['rows_inserted'] / stats['duration_seconds']\n",
    "            print(f\"🚀 Processing speed:           {format_number(int(throughput)):>10} rows/sec\")\n",
    "\n",
    "    # Database verification\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(\"SELECT COUNT(*) FROM mpp_measurement\"))\n",
    "            total_count = result.scalar()\n",
    "            \n",
    "            print(\"\\n🗄️  Database Status\")\n",
    "            print(\"━━━━━━━━━━━━━━━━\")\n",
    "            print(f\"💾 Total records in database:  {format_number(total_count):>10}\")\n",
    "            \n",
    "            if stats.get('channels_created', 0) > 0:\n",
    "                print(f\"🔌 New channels created:       {format_number(stats.get('channels_created', 0)):>10}\")\n",
    "    except Exception as e:\n",
    "        print(\"\\n⚠️  Could not verify database status:\")\n",
    "        print(f\"   {str(e)}\")\n",
    "else:\n",
    "    print(\"❌ No statistics available - processing may have failed\")\n",
    "    print(\"   Please check the logs above for errors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
