{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283d56a9",
   "metadata": {},
   "source": [
    "# MPP Data Upload Notebook\n",
    "\n",
    "This notebook processes and uploads Maximum Power Point (MPP) tracking data from text files to the TimescaleDB database.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "- Scan directories for MPP data files (output_board{X}_channel{Y}.txt files)\n",
    "- Parse the data into structured format\n",
    "- Upload the data to the TimescaleDB database\n",
    "- Avoid duplicate data entries\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Running TimescaleDB instance (configured in docker-compose.yml)\n",
    "- Access to directory containing MPP data files\n",
    "- Environment variables configured in .env file (for database connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8129a",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import required libraries and install any missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a114d-c1f8-4b35-b2e1-16d3ccb9de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# Database libraries\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840fec7-727b-4454-bed3-25d603203b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install psycopg2-binary sqlalchemy pandas tqdm pathlib python-dotenv\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b57a6",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load configuration from environment variables or use defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "# Look for the .env file two directories up from the notebook location\n",
    "dotenv_path = Path(\"../../.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Database configuration from environment variables with fallbacks\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'timescaledb'),\n",
    "    'port': int(os.getenv('DB_PORT', 5432)),\n",
    "    'database': os.getenv('DB_NAME', 'perocube'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'postgres')\n",
    "}\n",
    "\n",
    "# Print database connection info (excluding password)\n",
    "print(f\"Database connection: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']} as {DB_CONFIG['user']}\")\n",
    "\n",
    "# Data directory configuration\n",
    "ROOT_DIRECTORY = os.getenv('DEFAULT_DATA_DIR', \"../../sample_data/datasets/PeroCube-sample-data\")\n",
    "\n",
    "# File matching pattern\n",
    "MPP_FILE_PATTERN = r\"output_board(\\d+)_channel(\\d+)\"\n",
    "\n",
    "# Batch size for database operations\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "# Flag to enable/disable data validation\n",
    "VALIDATE_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417f820",
   "metadata": {},
   "source": [
    "## 3. Utility Functions\n",
    "\n",
    "Helper functions for database connection and data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0906e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_connection(config=DB_CONFIG):\n",
    "    \"\"\"\n",
    "    Create a SQLAlchemy database engine from configuration.\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary containing database connection parameters\n",
    "        \n",
    "    Returns:\n",
    "        SQLAlchemy engine instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection_string = f\"postgresql://{config['user']}:{config['password']}@{config['host']}:{config['port']}/{config['database']}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        # Test the connection\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(\"SELECT 1\"))\n",
    "            logging.info(f\"Database connection successful: {config['host']}:{config['port']}/{config['database']}\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Database connection failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def validate_mpp_data(df):\n",
    "    \"\"\"\n",
    "    Validate MPP data for common issues and clean as needed.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing MPP measurements\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned and validated DataFrame\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Ensure timestamp is in UTC\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "    \n",
    "    # Filter out physically impossible values\n",
    "    if 'voltage' in df.columns:\n",
    "        df = df[(df['voltage'] >= 0) & (df['voltage'] < 100)]  # Assuming voltage range\n",
    "    \n",
    "    if 'current' in df.columns:\n",
    "        df = df[(df['current'] >= -1) & (df['current'] < 100)]  # Assuming current range\n",
    "    \n",
    "    if 'power' in df.columns:\n",
    "        df = df[(df['power'] >= 0) & (df['power'] < 1000)]  # Assuming power range\n",
    "    \n",
    "    # Log validation results\n",
    "    filtered_count = len(df)\n",
    "    if filtered_count < original_count:\n",
    "        logging.info(f\"Filtered out {original_count - filtered_count} invalid records\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def check_existing_data(engine, board, channel, timestamps):\n",
    "    \"\"\"\n",
    "    Check if data already exists in the database for given parameters.\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine\n",
    "        board: Board number\n",
    "        channel: Channel number\n",
    "        timestamps: List of timestamps to check\n",
    "        \n",
    "    Returns:\n",
    "        Boolean indicating if data exists\n",
    "    \"\"\"\n",
    "    if not timestamps:\n",
    "        return False\n",
    "        \n",
    "    # For efficiency, just check the min and max timestamps\n",
    "    min_timestamp = min(timestamps)\n",
    "    max_timestamp = max(timestamps)\n",
    "    \n",
    "    # Build a query to check for existing data\n",
    "    query = text(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM mpp_measurement\n",
    "        WHERE timestamp BETWEEN :min_timestamp AND :max_timestamp\n",
    "          AND tracking_channel_board = :board_id\n",
    "          AND tracking_channel_channel = :channel_id\n",
    "    \"\"\")\n",
    "    \n",
    "    # Execute the query\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query, {\n",
    "            \"min_timestamp\": min_timestamp,\n",
    "            \"max_timestamp\": max_timestamp,\n",
    "            \"board_id\": board,\n",
    "            \"channel_id\": channel\n",
    "        })\n",
    "        count = result.scalar()\n",
    "        \n",
    "    # If count > 0, some data exists\n",
    "    return count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62d1d0-c994-48e0-ac76-dd797484f5bc",
   "metadata": {},
   "source": [
    "## 4. MPP Data Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027254a-35a5-4b2c-b066-b64905e68c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mpp_files(root_dir, engine, pattern=MPP_FILE_PATTERN, batch_size=BATCH_SIZE, validate=VALIDATE_DATA):\n",
    "    \"\"\"\n",
    "    Crawls directories starting with 'data', finds files matching the pattern,\n",
    "    reads them into pandas DataFrames, and uploads them to the DB.\n",
    "\n",
    "    Args:\n",
    "        root_dir: The root directory to start the search from.\n",
    "        engine:   SQLAlchemy engine for database connection.\n",
    "        pattern:  Regex pattern to match MPP files.\n",
    "        batch_size: Number of rows to insert in one batch.\n",
    "        validate:   Whether to perform data validation.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistics about the processing.\n",
    "    \"\"\"\n",
    "    # Statistics to return\n",
    "    stats = {\n",
    "        'files_processed': 0,\n",
    "        'files_skipped': 0,\n",
    "        'files_error': 0,\n",
    "        'rows_inserted': 0,\n",
    "        'start_time': datetime.now(timezone.utc)\n",
    "        'total_files': 0\n",
    "    }\n",
    "\n",
    "    # Convert to Path object for better path handling\n",
    "    root_path = Path(root_dir)\n",
    "    if not root_path.exists():\n",
    "        logging.error(f\"Root directory does not exist: {root_dir}\")\n",
    "        return stats\n",
    "\n",
    "    # Compile the regex pattern for efficiency\n",
    "    pattern_compiled = re.compile(pattern)\n",
    "\n",
    "    # First, collect all matching filepaths\n",
    "    matching_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        path_parts = Path(dirpath).parts\n",
    "        if any(part.startswith(\"data\") for part in path_parts):\n",
    "            for filename in filenames:\n",
    "                filepath = Path(dirpath) / filename\n",
    "                if pattern_compiled.search(filename):\n",
    "                    matching_files.append(filepath)\n",
    "\n",
    "    stats['total_files'] = len(matching_files)\n",
    "    logging.info(f\"Found {len(matching_files)} MPP data files to process\")\n",
    "\n",
    "    # Now, process the collected filepaths\n",
    "    with tqdm(total=len(matching_files), desc=\"Processing MPP Files\") as pbar:\n",
    "        for filepath in matching_files:\n",
    "            try:\n",
    "                # Extract filename for pattern matching\n",
    "                filename = filepath.name\n",
    "                match = pattern_compiled.search(filename)\n",
    "\n",
    "                if not match:\n",
    "                    logging.warning(f\"Skipping file without proper format: {filepath}\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Extract board and channel numbers\n",
    "                board = int(match.group(1))\n",
    "                channel = int(match.group(2))\n",
    "\n",
    "                # Log processing information\n",
    "                logging.info(f\"Processing MPP file: {filepath} (Board: {board}, Channel: {channel})\")\n",
    "\n",
    "                # Read the file into a pandas DataFrame\n",
    "                df = pd.read_csv(filepath, sep='\\t',\n",
    "                                names=['timestamp', 'power', 'current', 'voltage'])\n",
    "                                \n",
    "                if df.empty:\n",
    "                    logging.warning(f\"Empty file: {filepath}\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Ensure timestamp is in UTC format\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "\n",
    "                # Add board and channel information to the DataFrame\n",
    "                df['tracking_channel_board'] = board\n",
    "                df['tracking_channel_channel'] = channel\n",
    "\n",
    "                # Validate data if enabled\n",
    "                if validate:\n",
    "                    df = validate_mpp_data(df)\n",
    "                    if df.empty:\n",
    "                        logging.warning(f\"All data filtered during validation: {filepath}\")\n",
    "                        stats['files_skipped'] += 1\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                # Check for existing data\n",
    "                timestamps = df['timestamp'].tolist()\n",
    "                data_exists = check_existing_data(engine, board, channel, timestamps)\n",
    "\n",
    "                if data_exists:\n",
    "                    logging.info(f\"Data already exists for {filepath}. Skipping file.\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                else:\n",
    "                    # Upload data in batches for large files\n",
    "                    total_rows = len(df)\n",
    "                    for i in range(0, total_rows, batch_size):\n",
    "                        batch_df = df.iloc[i:i+batch_size]\n",
    "                        batch_df.to_sql('mpp_measurement', engine, if_exists='append', index=False)\n",
    "                        \n",
    "                    stats['rows_inserted'] += total_rows\n",
    "                    stats['files_processed'] += 1\n",
    "                    logging.info(f\"Successfully uploaded {total_rows} rows from {filepath}\")\n",
    "\n",
    "                # Clean up\n",
    "                del df\n",
    "                pbar.update(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {filepath}: {str(e)}\")\n",
    "                stats['files_error'] += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Calculate duration\n",
    "    stats['end_time'] = datetime.now(timezone.utc)\n",
    "    stats['duration_seconds'] = (stats['end_time'] - stats['start_time']).total_seconds()\n",
    "    \n",
    "    logging.info(f\"Processing complete. Processed {stats['files_processed']} files, \"\n",
    "                 f\"skipped {stats['files_skipped']} files, \"\n",
    "                 f\"errors in {stats['files_error']} files. \"\n",
    "                 f\"Inserted {stats['rows_inserted']} data points in {stats['duration_seconds']:.2f} seconds.\")\n",
    "                 \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027efb89",
   "metadata": {},
   "source": [
    "## 5. Execute the Data Upload Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e847a3b-a477-4960-b3a3-ad77ef6e13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database connection\n",
    "try:\n",
    "    engine = create_db_connection()\n",
    "    logging.info(\"Database connection established successfully\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to connect to database: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df07af7-7e2f-4c1f-b3b5-2c4654a5095c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c471ede92f2d4ee2979052a21d92f7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing MPP Files:   0%|          | 0/1897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the data processing with the configured root directory\n",
    "print(f\"Starting MPP data processing from directory: {ROOT_DIRECTORY}\")\n",
    "stats = process_mpp_files(ROOT_DIRECTORY, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c93cf",
   "metadata": {},
   "source": [
    "## 6. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903bb13-401b-46f8-aa03-0a5127af9d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display processing statistics\n",
    "if 'stats' in locals():\n",
    "    print(\"\\nProcessing Summary:\")\n",
    "    print(f\"- Total files found: {stats.get('total_files', 0)}\")\n",
    "    print(f\"- Files successfully processed: {stats.get('files_processed', 0)}\")\n",
    "    print(f\"- Files skipped (empty or existing data): {stats.get('files_skipped', 0)}\")\n",
    "    print(f\"- Files with errors: {stats.get('files_error', 0)}\")\n",
    "    print(f\"- Total data points inserted: {stats.get('rows_inserted', 0)}\")\n",
    "    if 'duration_seconds' in stats:\n",
    "        print(f\"- Processing time: {stats['duration_seconds']:.2f} seconds\")\n",
    "        if stats.get('rows_inserted', 0) > 0 and stats.get('duration_seconds', 0) > 0:\n",
    "            throughput = stats['rows_inserted'] / stats['duration_seconds']\n",
    "            print(f\"- Throughput: {throughput:.2f} rows/second\")\n",
    "else:\n",
    "    print(\"No statistics available. Processing may have failed.\")\n",
    "\n",
    "# Verify database counts\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT COUNT(*) FROM mpp_measurement\"))\n",
    "        total_count = result.scalar()\n",
    "        print(f\"\\nTotal records in mpp_measurement table: {total_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not query database: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
