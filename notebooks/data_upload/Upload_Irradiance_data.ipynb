{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c37b897",
   "metadata": {},
   "source": [
    "# Irradiance Data Upload Notebook\n",
    "\n",
    "This notebook processes and uploads irradiance measurement data from text files to the TimescaleDB database.\n",
    "\n",
    "## Purpose\n",
    "- Scan directories for irradiance data files ({sensor_identifier}_channel_{channel}.txt files)\n",
    "- Parse the data into structured format\n",
    "- Upload the data to the TimescaleDB database\n",
    "- Avoid duplicate data entries\n",
    "\n",
    "## Prerequisites\n",
    "- Running TimescaleDB instance (configured in docker-compose.yml)\n",
    "- Access to directory containing irradiance data files\n",
    "- Environment variables configured in .env file (for database connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158ac0a",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import required libraries and install any missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# Database libraries\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install psycopg2-binary sqlalchemy pandas tqdm pathlib python-dotenv\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4fb1e",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load configuration from environment variables and set up constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "# Look for the .env file two directories up from the notebook location\n",
    "dotenv_path = Path(\"../../.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Database configuration from container environment variables with fallbacks\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('POSTGRES_HOST', 'timescaledb'),  # Use container service name\n",
    "    'port': int(os.getenv('POSTGRES_PORT', 5432)),\n",
    "    'database': os.getenv('POSTGRES_DB', 'perocube'),\n",
    "    'user': os.getenv('POSTGRES_USER', 'postgres'),\n",
    "    'password': os.getenv('POSTGRES_PASSWORD', 'postgres')\n",
    "}\n",
    "\n",
    "# Print database connection info (excluding password)\n",
    "print(f\"Database connection: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']} as {DB_CONFIG['user']}\")\n",
    "\n",
    "# Data directory configuration - using relative path from notebook location\n",
    "ROOT_DIRECTORY = str(Path(\"../../sample_data/datasets/PeroCube-sample-data\").resolve())\n",
    "\n",
    "# File matching pattern for irradiance data\n",
    "IRRADIANCE_FILE_PATTERN = r\"(.+)_channel_(\\d+)\\.txt$\"\n",
    "\n",
    "# Batch size for database operations\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "# UUID namespace for irradiance sensors\n",
    "SENSOR_UUID_NAMESPACE = uuid.UUID('12345678-1234-5678-1234-567812345678')\n",
    "\n",
    "# Data validation configuration\n",
    "VALIDATION_CONFIG = {\n",
    "    'enabled': True,  # Master switch for validation\n",
    "    'remove_nan': True,  # Always remove NaN values from timestamp column\n",
    "}\n",
    "\n",
    "def print_validation_config():\n",
    "    \"\"\"Print current validation configuration for user awareness\"\"\"\n",
    "    print(\"\\nData Validation Configuration:\")\n",
    "    print(f\"- Validation enabled: {VALIDATION_CONFIG['enabled']}\")\n",
    "    print(f\"- Remove NaN values from timestamp: {VALIDATION_CONFIG['remove_nan']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b040c1",
   "metadata": {},
   "source": [
    "## 3. Utility Functions\n",
    "\n",
    "Define helper functions for database connection, data validation, and sensor management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_connection(config=DB_CONFIG):\n",
    "    \"\"\"\n",
    "    Create a SQLAlchemy database engine from configuration.\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary containing database connection parameters\n",
    "        \n",
    "    Returns:\n",
    "        SQLAlchemy engine instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection_string = f\"postgresql://{config['user']}:{config['password']}@{config['host']}:{config['port']}/{config['database']}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        \n",
    "        # Test the connection\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(\"SELECT 1\"))\n",
    "            logging.info(f\"Database connection successful: {config['host']}:{config['port']}/{config['database']}\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Database connection failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def generate_sensor_id(sensor_identifier, channel):\n",
    "    \"\"\"\n",
    "    Generate a deterministic UUID for an irradiance sensor based on its identifier and channel.\n",
    "    \n",
    "    Args:\n",
    "        sensor_identifier (str): Complete sensor identifier (e.g., 'PT-104')\n",
    "        channel (str): Channel number\n",
    "        \n",
    "    Returns:\n",
    "        UUID: Deterministic UUID5 for the sensor\n",
    "    \"\"\"\n",
    "    # Create a unique name string that incorporates both the sensor identifier and channel\n",
    "    name = f\"{sensor_identifier}_{channel}\"\n",
    "    return uuid.uuid5(SENSOR_UUID_NAMESPACE, name)\n",
    "\n",
    "def get_or_create_sensor(engine, sensor_identifier, channel):\n",
    "    \"\"\"\n",
    "    Get existing sensor or create a new one if it doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine instance\n",
    "        sensor_identifier (str): Complete sensor identifier (e.g., 'PT-104')\n",
    "        channel (str): Channel number\n",
    "        \n",
    "    Returns:\n",
    "        UUID: sensor_id of the existing or newly created sensor\n",
    "    \"\"\"\n",
    "    sensor_id = generate_sensor_id(sensor_identifier, channel)\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Check if sensor exists\n",
    "            result = conn.execute(\n",
    "                text(\"\"\"\n",
    "                SELECT irradiance_sensor_id \n",
    "                FROM irradiance_sensor \n",
    "                WHERE sensor_identifier = :identifier \n",
    "                AND channel = :channel\n",
    "                \"\"\"),\n",
    "                {\"identifier\": sensor_identifier, \"channel\": channel}\n",
    "            )\n",
    "            \n",
    "            if not result.fetchone():\n",
    "                # Create new sensor if it doesn't exist\n",
    "                conn.execute(\n",
    "                    text(\"\"\"\n",
    "                    INSERT INTO irradiance_sensor \n",
    "                    (irradiance_sensor_id, sensor_identifier, channel, date_installed) \n",
    "                    VALUES (:id, :identifier, :channel, CURRENT_DATE)\n",
    "                    \"\"\"),\n",
    "                    {\n",
    "                        \"id\": sensor_id,\n",
    "                        \"identifier\": sensor_identifier,\n",
    "                        \"channel\": channel\n",
    "                    }\n",
    "                )\n",
    "                conn.commit()\n",
    "                logging.info(f\"Created new sensor: {sensor_identifier} channel {channel}\")\n",
    "            else:\n",
    "                logging.info(f\"Found existing sensor: {sensor_identifier} channel {channel}\")\n",
    "                \n",
    "        return sensor_id\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in get_or_create_sensor: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def validate_irradiance_data(df):\n",
    "    \"\"\"\n",
    "    Validate irradiance measurement data according to configuration.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing irradiance measurements\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned and validated DataFrame, along with validation statistics\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df, {'initial_count': 0, 'final_count': 0, 'removed': {}}\n",
    "    \n",
    "    stats = {\n",
    "        'initial_count': len(df),\n",
    "        'final_count': None,\n",
    "        'removed': {\n",
    "            'nan_values': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Always ensure timestamp is in UTC\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "    \n",
    "    # Remove NaN values from timestamp column as required by TimescaleDB\n",
    "    if VALIDATION_CONFIG['remove_nan']:\n",
    "        nan_count = df['timestamp'].isna().sum()\n",
    "        df = df.dropna(subset=['timestamp'])\n",
    "        stats['removed']['nan_values'] = nan_count\n",
    "    \n",
    "    stats['final_count'] = len(df)\n",
    "    \n",
    "    # Log validation results\n",
    "    logging.info(\"Validation statistics:\")\n",
    "    logging.info(f\"Initial records: {stats['initial_count']}\")\n",
    "    if VALIDATION_CONFIG['remove_nan']:\n",
    "        logging.info(f\"Removed timestamp NaN values: {stats['removed']['nan_values']}\")\n",
    "    logging.info(f\"Final records: {stats['final_count']}\")\n",
    "    \n",
    "    return df, stats\n",
    "\n",
    "def check_existing_data(engine, sensor_identifier, channel, timestamps):\n",
    "    \"\"\"\n",
    "    Check if data already exists in the database for given parameters.\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine\n",
    "        sensor_identifier: Sensor identifier\n",
    "        channel: Channel number\n",
    "        timestamps: List of timestamps to check\n",
    "        \n",
    "    Returns:\n",
    "        Boolean indicating if data exists\n",
    "    \"\"\"\n",
    "    if not timestamps:\n",
    "        return False\n",
    "        \n",
    "    # For efficiency, just check the min and max timestamps\n",
    "    min_timestamp = min(timestamps)\n",
    "    max_timestamp = max(timestamps)\n",
    "    \n",
    "    sensor_id = generate_sensor_id(sensor_identifier, channel)\n",
    "    \n",
    "    # Build a query to check for existing data\n",
    "    query = text(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM irradiance_measurement\n",
    "        WHERE timestamp BETWEEN :min_timestamp AND :max_timestamp\n",
    "          AND irradiance_sensor_id = :sensor_id\n",
    "    \"\"\")\n",
    "    \n",
    "    # Execute the query\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query, {\n",
    "            \"min_timestamp\": min_timestamp,\n",
    "            \"max_timestamp\": max_timestamp,\n",
    "            \"sensor_id\": sensor_id\n",
    "        })\n",
    "        count = result.scalar()\n",
    "        \n",
    "    # If count > 0, some data exists\n",
    "    return count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cf158",
   "metadata": {},
   "source": [
    "## 4. Data Processing Function\n",
    "\n",
    "Main function to process and upload irradiance data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_irradiance_files(root_dir, engine, pattern=IRRADIANCE_FILE_PATTERN, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Process irradiance data files and upload measurements to the database.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Root directory to search for files\n",
    "        engine: SQLAlchemy engine instance\n",
    "        pattern: Regex pattern to match files\n",
    "        batch_size: Number of records to process in one batch\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    # Statistics to track progress\n",
    "    stats = {\n",
    "        'files_processed': 0,\n",
    "        'files_skipped': 0,\n",
    "        'files_error': 0,\n",
    "        'rows_inserted': 0,\n",
    "        'start_time': datetime.now(timezone.utc),\n",
    "        'total_files': 0\n",
    "    }\n",
    "    \n",
    "    # Convert to Path object\n",
    "    root_path = Path(root_dir)\n",
    "    if not root_path.exists():\n",
    "        logging.error(f\"Root directory does not exist: {root_dir}\")\n",
    "        return stats\n",
    "    \n",
    "    # Compile regex pattern\n",
    "    pattern_compiled = re.compile(pattern)\n",
    "    \n",
    "    # Find all matching files\n",
    "    matching_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        path_parts = Path(dirpath).parts\n",
    "        if any(part.startswith(\"data\") for part in path_parts):\n",
    "            for filename in filenames:\n",
    "                filepath = Path(dirpath) / filename\n",
    "                match = pattern_compiled.search(filename)\n",
    "                if match:\n",
    "                    sensor_identifier = match.group(1)\n",
    "                    channel = int(match.group(2))\n",
    "                    matching_files.append((filepath, sensor_identifier, channel))\n",
    "    \n",
    "    stats['total_files'] = len(matching_files)\n",
    "    logging.info(f\"Found {len(matching_files)} irradiance data files to process\")\n",
    "    \n",
    "    # Process each file\n",
    "    with tqdm(total=len(matching_files), desc=\"Processing Files\") as pbar:\n",
    "        for filepath, sensor_identifier, channel in matching_files:\n",
    "            try:\n",
    "                logging.info(f\"Processing file: {filepath}\")\n",
    "                logging.info(f\"Sensor: {sensor_identifier}, Channel: {channel}\")\n",
    "                \n",
    "                # Read the data file\n",
    "                df = pd.read_csv(filepath, sep='\\t',\n",
    "                               names=['timestamp', 'raw_reading', 'irradiance'])\n",
    "                \n",
    "                if df.empty:\n",
    "                    logging.warning(f\"Empty file: {filepath}\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Validate data\n",
    "                df, validation_stats = validate_irradiance_data(df)\n",
    "                if df.empty:\n",
    "                    logging.warning(f\"No valid data after validation: {filepath}\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Check for existing data\n",
    "                if check_existing_data(engine, sensor_identifier, channel, df['timestamp'].tolist()):\n",
    "                    logging.info(f\"Data already exists for {filepath}. Skipping file.\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Get or create sensor\n",
    "                sensor_id = get_or_create_sensor(engine, sensor_identifier, channel)\n",
    "                \n",
    "                # Add sensor_id to DataFrame\n",
    "                df['irradiance_sensor_id'] = sensor_id\n",
    "                \n",
    "                # Upload data in batches\n",
    "                total_rows = len(df)\n",
    "                for i in range(0, total_rows, batch_size):\n",
    "                    batch_df = df.iloc[i:i+batch_size]\n",
    "                    batch_df.to_sql('irradiance_measurement', engine, \n",
    "                                  if_exists='append', index=False)\n",
    "                \n",
    "                stats['rows_inserted'] += total_rows\n",
    "                stats['files_processed'] += 1\n",
    "                logging.info(f\"Successfully uploaded {total_rows} rows from {filepath}\")\n",
    "                \n",
    "                # Clean up\n",
    "                del df\n",
    "                pbar.update(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {filepath}: {str(e)}\")\n",
    "                stats['files_error'] += 1\n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Calculate duration\n",
    "    stats['end_time'] = datetime.now(timezone.utc)\n",
    "    stats['duration_seconds'] = (stats['end_time'] - stats['start_time']).total_seconds()\n",
    "    \n",
    "    logging.info(f\"Processing complete. \"\n",
    "                 f\"Processed {stats['files_processed']} files, \"\n",
    "                 f\"skipped {stats['files_skipped']} files, \"\n",
    "                 f\"errors in {stats['files_error']} files. \"\n",
    "                 f\"Inserted {stats['rows_inserted']} data points \"\n",
    "                 f\"in {stats['duration_seconds']:.2f} seconds.\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f2c7b",
   "metadata": {},
   "source": [
    "## 5. Execute the Data Upload Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database connection\n",
    "try:\n",
    "    engine = create_db_connection()\n",
    "    logging.info(\"Database connection established successfully\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to connect to database: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review current validation configuration\n",
    "print_validation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937dde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data processing with the configured root directory\n",
    "print(f\"Starting irradiance data processing from directory: {ROOT_DIRECTORY}\")\n",
    "stats = process_irradiance_files(ROOT_DIRECTORY, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc8d69",
   "metadata": {},
   "source": [
    "## 6. Results Summary\n",
    "\n",
    "After processing the irradiance data files, here's a summary of what was accomplished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_duration(seconds):\n",
    "    \"\"\"Format duration in a human-readable format\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = seconds % 60\n",
    "    if hours > 0:\n",
    "        return f\"{hours}h {minutes}m {secs:.1f}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}m {secs:.1f}s\"\n",
    "    else:\n",
    "        return f\"{secs:.1f}s\"\n",
    "\n",
    "def format_number(n):\n",
    "    \"\"\"Format number with thousand separators\"\"\"\n",
    "    return f\"{n:,}\"\n",
    "\n",
    "# Display processing statistics\n",
    "if 'stats' in locals():\n",
    "    print(\"üìä File Processing Summary\")\n",
    "    print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "    print(f\"üìÅ Total files found:           {format_number(stats.get('total_files', 0)):>10}\")\n",
    "    print(f\"‚úÖ Successfully processed:      {format_number(stats.get('files_processed', 0)):>10}\")\n",
    "    print(f\"‚è≠Ô∏è  Skipped (existing/empty):    {format_number(stats.get('files_skipped', 0)):>10}\")\n",
    "    print(f\"‚ùå Errors during processing:    {format_number(stats.get('files_error', 0)):>10}\")\n",
    "    \n",
    "    print(\"\\nüìà Data Statistics\")\n",
    "    print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "    print(f\"üìù Data points inserted:        {format_number(stats.get('rows_inserted', 0)):>10}\")\n",
    "    \n",
    "    if 'duration_seconds' in stats:\n",
    "        duration = format_duration(stats['duration_seconds'])\n",
    "        print(\"\\n‚ö° Performance Metrics\")\n",
    "        print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "        print(f\"‚è±Ô∏è  Total processing time:      {duration:>10}\")\n",
    "        \n",
    "        if stats.get('rows_inserted', 0) > 0 and stats.get('duration_seconds', 0) > 0:\n",
    "            throughput = stats['rows_inserted'] / stats['duration_seconds']\n",
    "            print(f\"üöÄ Processing speed:           {format_number(int(throughput)):>10} rows/sec\")\n",
    "    \n",
    "    # Database verification\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(\"SELECT COUNT(*) FROM irradiance_measurement\"))\n",
    "            total_count = result.scalar()\n",
    "            \n",
    "            print(\"\\nüóÑÔ∏è  Database Status\")\n",
    "            print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "            print(f\"üíæ Total records in database:  {format_number(total_count):>10}\")\n",
    "            \n",
    "            result = conn.execute(text(\"SELECT COUNT(*) FROM irradiance_sensor\"))\n",
    "            sensor_count = result.scalar()\n",
    "            print(f\"üîå Total sensors in database:  {format_number(sensor_count):>10}\")\n",
    "    except Exception as e:\n",
    "        print(\"\\n‚ö†Ô∏è  Could not verify database status:\")\n",
    "        print(f\"   {str(e)}\")\n",
    "else:\n",
    "    print(\"‚ùå No statistics available - processing may have failed\")\n",
    "    print(\"   Please check the logs above for errors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
