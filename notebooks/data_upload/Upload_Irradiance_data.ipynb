{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c37b897",
   "metadata": {},
   "source": [
    "# Irradiance Data Upload Notebook\n",
    "\n",
    "This notebook processes and uploads irradiance measurement data from text files to the TimescaleDB database.\n",
    "\n",
    "## Purpose\n",
    "- Scan directories for irradiance data files ({sensor_identifier}_channel_{channel}.txt files)\n",
    "- Parse the data into structured format\n",
    "- Upload the data to the TimescaleDB database\n",
    "- Avoid duplicate data entries\n",
    "\n",
    "## Prerequisites\n",
    "- Running TimescaleDB instance (configured in docker-compose.yml)\n",
    "- Access to directory containing irradiance data files\n",
    "- Environment variables configured in .env file (for database connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158ac0a",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import required libraries and install any missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9fd3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# Database libraries\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d5b244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (2.9.10)\n",
      "Requirement already satisfied: sqlalchemy in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (2.0.40)\n",
      "Requirement already satisfied: pandas in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: pathlib in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: python-dotenv in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (from sqlalchemy) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (from sqlalchemy) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/qkg/Documents/1_PROJECTS/Perocube Data Monitoring System/perocube-data-monitoring-system/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install psycopg2-binary sqlalchemy pandas tqdm pathlib python-dotenv\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4fb1e",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load configuration from environment variables and set up constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a54a261f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection: localhost:5432/perocube as postgres\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "# Look for the .env file two directories up from the notebook location\n",
    "dotenv_path = Path(\"../../.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Database configuration from environment variables with fallbacks\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'port': int(os.getenv('DB_PORT', 5432)),\n",
    "    'database': os.getenv('DB_NAME', 'perocube'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'postgres')\n",
    "}\n",
    "\n",
    "# Print database connection info (excluding password)\n",
    "print(f\"Database connection: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']} as {DB_CONFIG['user']}\")\n",
    "\n",
    "# Data directory configuration\n",
    "ROOT_DIRECTORY = os.getenv('DEFAULT_DATA_DIR', \"../../sample_data/datasets/PeroCube-sample-data\")\n",
    "\n",
    "# File matching pattern for irradiance data\n",
    "IRRADIANCE_FILE_PATTERN = r\"(.+)_channel_(\\d+)\\.txt$\"\n",
    "\n",
    "# Batch size for database operations\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "# UUID namespace for irradiance sensors\n",
    "SENSOR_UUID_NAMESPACE = uuid.UUID('12345678-1234-5678-1234-567812345678')\n",
    "\n",
    "# Data validation configuration\n",
    "VALIDATION_CONFIG = {\n",
    "    'enabled': True,  # Master switch for validation\n",
    "    'remove_nan': True,  # Always remove NaN values from timestamp column\n",
    "}\n",
    "\n",
    "def print_validation_config():\n",
    "    \"\"\"Print current validation configuration for user awareness\"\"\"\n",
    "    print(\"\\nData Validation Configuration:\")\n",
    "    print(f\"- Validation enabled: {VALIDATION_CONFIG['enabled']}\")\n",
    "    print(f\"- Remove NaN values from timestamp: {VALIDATION_CONFIG['remove_nan']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b040c1",
   "metadata": {},
   "source": [
    "## 3. Utility Functions\n",
    "\n",
    "Define helper functions for database connection, data validation, and sensor management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c8c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_connection(config=DB_CONFIG):\n",
    "    \"\"\"\n",
    "    Create a SQLAlchemy database engine from configuration.\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary containing database connection parameters\n",
    "        \n",
    "    Returns:\n",
    "        SQLAlchemy engine instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection_string = f\"postgresql://{config['user']}:{config['password']}@{config['host']}:{config['port']}/{config['database']}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        \n",
    "        # Test the connection\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(\"SELECT 1\"))\n",
    "            logging.info(f\"Database connection successful: {config['host']}:{config['port']}/{config['database']}\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Database connection failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def generate_sensor_id(sensor_identifier, channel):\n",
    "    \"\"\"\n",
    "    Generate a deterministic UUID for an irradiance sensor based on its identifier and channel.\n",
    "    \n",
    "    Args:\n",
    "        sensor_identifier (str): Complete sensor identifier (e.g., 'PT-104')\n",
    "        channel (str): Channel number\n",
    "        \n",
    "    Returns:\n",
    "        UUID: Deterministic UUID5 for the sensor\n",
    "    \"\"\"\n",
    "    # Create a unique name string that incorporates both the sensor identifier and channel\n",
    "    name = f\"{sensor_identifier}_{channel}\"\n",
    "    return uuid.uuid5(SENSOR_UUID_NAMESPACE, name)\n",
    "\n",
    "def get_or_create_sensor(engine, sensor_identifier, channel):\n",
    "    \"\"\"\n",
    "    Get existing sensor or create a new one if it doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine instance\n",
    "        sensor_identifier (str): Complete sensor identifier (e.g., 'PT-104')\n",
    "        channel (str): Channel number\n",
    "        \n",
    "    Returns:\n",
    "        UUID: sensor_id of the existing or newly created sensor\n",
    "    \"\"\"\n",
    "    sensor_id = generate_sensor_id(sensor_identifier, channel)\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Check if sensor exists\n",
    "            result = conn.execute(\n",
    "                text(\"\"\"\n",
    "                SELECT irradiance_sensor_id \n",
    "                FROM irradiance_sensor \n",
    "                WHERE sensor_identifier = :identifier \n",
    "                AND channel = :channel\n",
    "                \"\"\"),\n",
    "                {\"identifier\": sensor_identifier, \"channel\": channel}\n",
    "            )\n",
    "            \n",
    "            if not result.fetchone():\n",
    "                # Create new sensor if it doesn't exist\n",
    "                conn.execute(\n",
    "                    text(\"\"\"\n",
    "                    INSERT INTO irradiance_sensor \n",
    "                    (irradiance_sensor_id, sensor_identifier, channel, date_installed) \n",
    "                    VALUES (:id, :identifier, :channel, CURRENT_DATE)\n",
    "                    \"\"\"),\n",
    "                    {\n",
    "                        \"id\": sensor_id,\n",
    "                        \"identifier\": sensor_identifier,\n",
    "                        \"channel\": channel\n",
    "                    }\n",
    "                )\n",
    "                conn.commit()\n",
    "                logging.info(f\"Created new sensor: {sensor_identifier} channel {channel}\")\n",
    "            else:\n",
    "                logging.info(f\"Found existing sensor: {sensor_identifier} channel {channel}\")\n",
    "                \n",
    "        return sensor_id\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in get_or_create_sensor: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def validate_irradiance_data(df):\n",
    "    \"\"\"\n",
    "    Validate irradiance measurement data according to configuration.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing irradiance measurements\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned and validated DataFrame, along with validation statistics\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df, {'initial_count': 0, 'final_count': 0, 'removed': {}}\n",
    "    \n",
    "    stats = {\n",
    "        'initial_count': len(df),\n",
    "        'final_count': None,\n",
    "        'removed': {\n",
    "            'nan_values': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Always ensure timestamp is in UTC\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "    \n",
    "    # Remove NaN values from timestamp column as required by TimescaleDB\n",
    "    if VALIDATION_CONFIG['remove_nan']:\n",
    "        nan_count = df['timestamp'].isna().sum()\n",
    "        df = df.dropna(subset=['timestamp'])\n",
    "        stats['removed']['nan_values'] = nan_count\n",
    "    \n",
    "    stats['final_count'] = len(df)\n",
    "    \n",
    "    # Log validation results\n",
    "    logging.info(\"Validation statistics:\")\n",
    "    logging.info(f\"Initial records: {stats['initial_count']}\")\n",
    "    if VALIDATION_CONFIG['remove_nan']:\n",
    "        logging.info(f\"Removed timestamp NaN values: {stats['removed']['nan_values']}\")\n",
    "    logging.info(f\"Final records: {stats['final_count']}\")\n",
    "    \n",
    "    return df, stats\n",
    "\n",
    "def check_existing_data(engine, sensor_identifier, channel, timestamps):\n",
    "    \"\"\"\n",
    "    Check if data already exists in the database for given parameters.\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine\n",
    "        sensor_identifier: Sensor identifier\n",
    "        channel: Channel number\n",
    "        timestamps: List of timestamps to check\n",
    "        \n",
    "    Returns:\n",
    "        Boolean indicating if data exists\n",
    "    \"\"\"\n",
    "    if not timestamps:\n",
    "        return False\n",
    "        \n",
    "    # For efficiency, just check the min and max timestamps\n",
    "    min_timestamp = min(timestamps)\n",
    "    max_timestamp = max(timestamps)\n",
    "    \n",
    "    sensor_id = generate_sensor_id(sensor_identifier, channel)\n",
    "    \n",
    "    # Build a query to check for existing data\n",
    "    query = text(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM irradiance_measurement\n",
    "        WHERE timestamp BETWEEN :min_timestamp AND :max_timestamp\n",
    "          AND irradiance_sensor_id = :sensor_id\n",
    "    \"\"\")\n",
    "    \n",
    "    # Execute the query\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query, {\n",
    "            \"min_timestamp\": min_timestamp,\n",
    "            \"max_timestamp\": max_timestamp,\n",
    "            \"sensor_id\": sensor_id\n",
    "        })\n",
    "        count = result.scalar()\n",
    "        \n",
    "    # If count > 0, some data exists\n",
    "    return count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cf158",
   "metadata": {},
   "source": [
    "## 4. Data Processing Function\n",
    "\n",
    "Main function to process and upload irradiance data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81ab94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_irradiance_files(root_dir, engine, pattern=IRRADIANCE_FILE_PATTERN, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Process irradiance data files and upload measurements to the database.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Root directory to search for files\n",
    "        engine: SQLAlchemy engine instance\n",
    "        pattern: Regex pattern to match files\n",
    "        batch_size: Number of records to process in one batch\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    # Statistics to track progress\n",
    "    stats = {\n",
    "        'files_processed': 0,\n",
    "        'files_skipped': 0,\n",
    "        'files_error': 0,\n",
    "        'rows_inserted': 0,\n",
    "        'start_time': datetime.now(timezone.utc),\n",
    "        'total_files': 0\n",
    "    }\n",
    "    \n",
    "    # Convert to Path object\n",
    "    root_path = Path(root_dir)\n",
    "    if not root_path.exists():\n",
    "        logging.error(f\"Root directory does not exist: {root_dir}\")\n",
    "        return stats\n",
    "    \n",
    "    # Compile regex pattern\n",
    "    pattern_compiled = re.compile(pattern)\n",
    "    \n",
    "    # Find all matching files\n",
    "    matching_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        path_parts = Path(dirpath).parts\n",
    "        if any(part.startswith(\"data\") for part in path_parts):\n",
    "            for filename in filenames:\n",
    "                filepath = Path(dirpath) / filename\n",
    "                match = pattern_compiled.search(filename)\n",
    "                if match:\n",
    "                    sensor_identifier = match.group(1)\n",
    "                    channel = int(match.group(2))\n",
    "                    matching_files.append((filepath, sensor_identifier, channel))\n",
    "    \n",
    "    stats['total_files'] = len(matching_files)\n",
    "    logging.info(f\"Found {len(matching_files)} irradiance data files to process\")\n",
    "    \n",
    "    # Process each file\n",
    "    with tqdm(total=len(matching_files), desc=\"Processing Files\") as pbar:\n",
    "        for filepath, sensor_identifier, channel in matching_files:\n",
    "            try:\n",
    "                logging.info(f\"Processing file: {filepath}\")\n",
    "                logging.info(f\"Sensor: {sensor_identifier}, Channel: {channel}\")\n",
    "                \n",
    "                # Read the data file\n",
    "                df = pd.read_csv(filepath, sep='\\t',\n",
    "                               names=['timestamp', 'raw_reading', 'irradiance'])\n",
    "                \n",
    "                if df.empty:\n",
    "                    logging.warning(f\"Empty file: {filepath}\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Validate data\n",
    "                df, validation_stats = validate_irradiance_data(df)\n",
    "                if df.empty:\n",
    "                    logging.warning(f\"No valid data after validation: {filepath}\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Check for existing data\n",
    "                if check_existing_data(engine, sensor_identifier, channel, df['timestamp'].tolist()):\n",
    "                    logging.info(f\"Data already exists for {filepath}. Skipping file.\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Get or create sensor\n",
    "                sensor_id = get_or_create_sensor(engine, sensor_identifier, channel)\n",
    "                \n",
    "                # Add sensor_id to DataFrame\n",
    "                df['irradiance_sensor_id'] = sensor_id\n",
    "                \n",
    "                # Upload data in batches\n",
    "                total_rows = len(df)\n",
    "                for i in range(0, total_rows, batch_size):\n",
    "                    batch_df = df.iloc[i:i+batch_size]\n",
    "                    batch_df.to_sql('irradiance_measurement', engine, \n",
    "                                  if_exists='append', index=False)\n",
    "                \n",
    "                stats['rows_inserted'] += total_rows\n",
    "                stats['files_processed'] += 1\n",
    "                logging.info(f\"Successfully uploaded {total_rows} rows from {filepath}\")\n",
    "                \n",
    "                # Clean up\n",
    "                del df\n",
    "                pbar.update(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {filepath}: {str(e)}\")\n",
    "                stats['files_error'] += 1\n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Calculate duration\n",
    "    stats['end_time'] = datetime.now(timezone.utc)\n",
    "    stats['duration_seconds'] = (stats['end_time'] - stats['start_time']).total_seconds()\n",
    "    \n",
    "    logging.info(f\"Processing complete. \"\n",
    "                 f\"Processed {stats['files_processed']} files, \"\n",
    "                 f\"skipped {stats['files_skipped']} files, \"\n",
    "                 f\"errors in {stats['files_error']} files. \"\n",
    "                 f\"Inserted {stats['rows_inserted']} data points \"\n",
    "                 f\"in {stats['duration_seconds']:.2f} seconds.\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f2c7b",
   "metadata": {},
   "source": [
    "## 5. Execute the Data Upload Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65fc867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 17:59:06,376 - INFO - Database connection successful: localhost:5432/perocube\n",
      "2025-05-15 17:59:06,377 - INFO - Database connection established successfully\n"
     ]
    }
   ],
   "source": [
    "# Create database connection\n",
    "try:\n",
    "    engine = create_db_connection()\n",
    "    logging.info(\"Database connection established successfully\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to connect to database: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9014e943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Validation Configuration:\n",
      "- Validation enabled: True\n",
      "- Remove NaN values from timestamp: True\n"
     ]
    }
   ],
   "source": [
    "# Review current validation configuration\n",
    "print_validation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "937dde3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 17:59:06,489 - INFO - Found 34 irradiance data files to process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting irradiance data processing from directory: ../../sample_data/datasets/PeroCube-sample-data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666969556b1941afad4d46a2f68912af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 17:59:06,495 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240319/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:06,495 - INFO - Sensor: PT-104, Channel: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 17:59:06,507 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:06,507 - INFO - Initial records: 10714\n",
      "2025-05-15 17:59:06,508 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:06,508 - INFO - Final records: 10714\n",
      "2025-05-15 17:59:06,526 - INFO - Created new sensor: PT-104 channel 1\n",
      "2025-05-15 17:59:07,003 - INFO - Successfully uploaded 10714 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240319/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:07,005 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240319/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:07,005 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 17:59:07,015 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:07,015 - INFO - Initial records: 10714\n",
      "2025-05-15 17:59:07,016 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:07,016 - INFO - Final records: 10714\n",
      "2025-05-15 17:59:07,030 - INFO - Created new sensor: PT-104 channel 3\n",
      "2025-05-15 17:59:07,370 - INFO - Successfully uploaded 10714 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240319/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:07,371 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240222/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:07,371 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 17:59:07,384 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:07,384 - INFO - Initial records: 7552\n",
      "2025-05-15 17:59:07,385 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:07,385 - INFO - Final records: 7552\n",
      "2025-05-15 17:59:07,395 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 17:59:07,710 - INFO - Successfully uploaded 7552 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240222/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:07,711 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240222/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:07,712 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 17:59:07,719 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:07,719 - INFO - Initial records: 7552\n",
      "2025-05-15 17:59:07,720 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:07,720 - INFO - Final records: 7552\n",
      "2025-05-15 17:59:07,731 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 17:59:07,982 - INFO - Successfully uploaded 7552 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240222/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:07,983 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240220/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:07,983 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 17:59:08,075 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:08,076 - INFO - Initial records: 179220\n",
      "2025-05-15 17:59:08,076 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:08,076 - INFO - Final records: 179220\n",
      "2025-05-15 17:59:08,231 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 17:59:15,026 - INFO - Successfully uploaded 179220 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240220/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:15,027 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240220/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:15,028 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 17:59:15,123 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:15,124 - INFO - Initial records: 179220\n",
      "2025-05-15 17:59:15,124 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:15,125 - INFO - Final records: 179220\n",
      "2025-05-15 17:59:15,218 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 17:59:21,285 - INFO - Successfully uploaded 179220 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240220/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:21,286 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240514/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:21,287 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 17:59:21,341 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:21,342 - INFO - Initial records: 61372\n",
      "2025-05-15 17:59:21,343 - INFO - Removed timestamp NaN values: 1\n",
      "2025-05-15 17:59:21,344 - INFO - Final records: 61371\n",
      "2025-05-15 17:59:21,383 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 17:59:23,596 - INFO - Successfully uploaded 61371 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240514/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:23,597 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240514/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:23,598 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 17:59:23,636 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:23,636 - INFO - Initial records: 61372\n",
      "2025-05-15 17:59:23,636 - INFO - Removed timestamp NaN values: 1\n",
      "2025-05-15 17:59:23,637 - INFO - Final records: 61371\n",
      "2025-05-15 17:59:23,672 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 17:59:25,898 - INFO - Successfully uploaded 61371 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240514/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:25,899 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240408/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:25,900 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 17:59:25,917 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:25,917 - INFO - Initial records: 26988\n",
      "2025-05-15 17:59:25,918 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:25,918 - INFO - Final records: 26988\n",
      "2025-05-15 17:59:25,939 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 17:59:26,970 - INFO - Successfully uploaded 26988 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240408/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:26,971 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240408/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:26,971 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 17:59:26,986 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:26,987 - INFO - Initial records: 26984\n",
      "2025-05-15 17:59:26,987 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:26,988 - INFO - Final records: 26984\n",
      "2025-05-15 17:59:27,006 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 17:59:27,945 - INFO - Successfully uploaded 26984 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240408/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:27,947 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240605/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:27,947 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 17:59:27,960 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:27,960 - INFO - Initial records: 18568\n",
      "2025-05-15 17:59:27,961 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:27,961 - INFO - Final records: 18568\n",
      "2025-05-15 17:59:27,974 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 17:59:28,613 - INFO - Successfully uploaded 18568 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240605/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:28,614 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240605/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:28,614 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 17:59:28,629 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:28,630 - INFO - Initial records: 18568\n",
      "2025-05-15 17:59:28,630 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:28,630 - INFO - Final records: 18568\n",
      "2025-05-15 17:59:28,643 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 17:59:29,341 - INFO - Successfully uploaded 18568 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240605/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:29,342 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20230920/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:29,342 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 17:59:29,448 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:29,449 - INFO - Initial records: 195120\n",
      "2025-05-15 17:59:29,450 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:29,450 - INFO - Final records: 195120\n",
      "2025-05-15 17:59:29,574 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 17:59:36,701 - INFO - Successfully uploaded 195120 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20230920/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:36,702 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20230920/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:36,703 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 17:59:36,810 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:36,811 - INFO - Initial records: 195120\n",
      "2025-05-15 17:59:36,812 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:36,812 - INFO - Final records: 195120\n",
      "2025-05-15 17:59:36,921 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 17:59:43,889 - INFO - Successfully uploaded 195120 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20230920/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:43,890 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240527/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:43,891 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 17:59:43,908 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:43,909 - INFO - Initial records: 27174\n",
      "2025-05-15 17:59:43,909 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:43,909 - INFO - Final records: 27174\n",
      "2025-05-15 17:59:43,932 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 17:59:44,841 - INFO - Successfully uploaded 27174 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240527/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:44,841 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240527/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:44,842 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 17:59:44,856 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:44,856 - INFO - Initial records: 27174\n",
      "2025-05-15 17:59:44,857 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:44,857 - INFO - Final records: 27174\n",
      "2025-05-15 17:59:44,944 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 17:59:45,818 - INFO - Successfully uploaded 27174 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240527/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:45,819 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20231207/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:45,820 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 17:59:45,855 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:45,855 - INFO - Initial records: 67149\n",
      "2025-05-15 17:59:45,855 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:45,855 - INFO - Final records: 67149\n",
      "2025-05-15 17:59:45,909 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 17:59:51,315 - INFO - Successfully uploaded 67149 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20231207/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:51,316 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20231207/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:51,316 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 17:59:51,356 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:51,356 - INFO - Initial records: 67149\n",
      "2025-05-15 17:59:51,357 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:51,357 - INFO - Final records: 67149\n",
      "2025-05-15 17:59:51,393 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 17:59:53,807 - INFO - Successfully uploaded 67149 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20231207/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:53,809 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240415/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:53,809 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 17:59:53,820 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:53,821 - INFO - Initial records: 14893\n",
      "2025-05-15 17:59:53,821 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:53,821 - INFO - Final records: 14893\n",
      "2025-05-15 17:59:53,836 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 17:59:54,345 - INFO - Successfully uploaded 14893 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240415/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:54,346 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240415/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:54,347 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 17:59:54,356 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:54,357 - INFO - Initial records: 14893\n",
      "2025-05-15 17:59:54,358 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:54,358 - INFO - Final records: 14893\n",
      "2025-05-15 17:59:54,370 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 17:59:54,958 - INFO - Successfully uploaded 14893 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240415/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:54,959 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240610/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:54,959 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 17:59:54,966 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:54,967 - INFO - Initial records: 10731\n",
      "2025-05-15 17:59:54,967 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:54,967 - INFO - Final records: 10731\n",
      "2025-05-15 17:59:54,980 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 17:59:55,346 - INFO - Successfully uploaded 10731 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240610/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:55,347 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240610/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:55,347 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 17:59:55,354 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:55,355 - INFO - Initial records: 10731\n",
      "2025-05-15 17:59:55,355 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:55,356 - INFO - Final records: 10731\n",
      "2025-05-15 17:59:55,367 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 17:59:55,743 - INFO - Successfully uploaded 10731 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240610/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:55,744 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240709/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:55,744 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 17:59:55,784 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:55,785 - INFO - Initial records: 60397\n",
      "2025-05-15 17:59:55,785 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:55,785 - INFO - Final records: 60397\n",
      "2025-05-15 17:59:55,826 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 17:59:58,108 - INFO - Successfully uploaded 60397 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240709/data/PT-104_channel_01.txt\n",
      "2025-05-15 17:59:58,109 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240709/data/PT-104_channel_03.txt\n",
      "2025-05-15 17:59:58,109 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 17:59:58,148 - INFO - Validation statistics:\n",
      "2025-05-15 17:59:58,148 - INFO - Initial records: 60397\n",
      "2025-05-15 17:59:58,149 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 17:59:58,149 - INFO - Final records: 60397\n",
      "2025-05-15 17:59:58,186 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 18:00:00,431 - INFO - Successfully uploaded 60397 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240709/data/PT-104_channel_03.txt\n",
      "2025-05-15 18:00:00,432 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240326/data/PT-104_channel_01.txt\n",
      "2025-05-15 18:00:00,432 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 18:00:00,447 - INFO - Validation statistics:\n",
      "2025-05-15 18:00:00,448 - INFO - Initial records: 14572\n",
      "2025-05-15 18:00:00,448 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 18:00:00,448 - INFO - Final records: 14572\n",
      "2025-05-15 18:00:00,460 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 18:00:01,022 - INFO - Successfully uploaded 14572 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240326/data/PT-104_channel_01.txt\n",
      "2025-05-15 18:00:01,023 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240326/data/PT-104_channel_03.txt\n",
      "2025-05-15 18:00:01,023 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 18:00:01,035 - INFO - Validation statistics:\n",
      "2025-05-15 18:00:01,036 - INFO - Initial records: 14572\n",
      "2025-05-15 18:00:01,036 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 18:00:01,036 - INFO - Final records: 14572\n",
      "2025-05-15 18:00:01,055 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 18:00:01,570 - INFO - Successfully uploaded 14572 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240326/data/PT-104_channel_03.txt\n",
      "2025-05-15 18:00:01,571 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240314/data/PT-104_channel_01.txt\n",
      "2025-05-15 18:00:01,571 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 18:00:01,591 - INFO - Validation statistics:\n",
      "2025-05-15 18:00:01,591 - INFO - Initial records: 28770\n",
      "2025-05-15 18:00:01,591 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 18:00:01,591 - INFO - Final records: 28770\n",
      "2025-05-15 18:00:01,614 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 18:00:02,716 - INFO - Successfully uploaded 28770 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240314/data/PT-104_channel_01.txt\n",
      "2025-05-15 18:00:02,717 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240314/data/PT-104_channel_03.txt\n",
      "2025-05-15 18:00:02,717 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 18:00:02,737 - INFO - Validation statistics:\n",
      "2025-05-15 18:00:02,738 - INFO - Initial records: 26748\n",
      "2025-05-15 18:00:02,738 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 18:00:02,738 - INFO - Final records: 26748\n",
      "2025-05-15 18:00:02,756 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 18:00:03,718 - INFO - Successfully uploaded 26748 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240314/data/PT-104_channel_03.txt\n",
      "2025-05-15 18:00:03,719 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20231012/data/PT-104_channel_01.txt\n",
      "2025-05-15 18:00:03,720 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 18:00:03,770 - INFO - Validation statistics:\n",
      "2025-05-15 18:00:03,771 - INFO - Initial records: 92067\n",
      "2025-05-15 18:00:03,771 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 18:00:03,772 - INFO - Final records: 92067\n",
      "2025-05-15 18:00:03,832 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 18:00:07,421 - INFO - Successfully uploaded 92067 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20231012/data/PT-104_channel_01.txt\n",
      "2025-05-15 18:00:07,422 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20231012/data/PT-104_channel_03.txt\n",
      "2025-05-15 18:00:07,422 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 18:00:07,468 - INFO - Validation statistics:\n",
      "2025-05-15 18:00:07,468 - INFO - Initial records: 92067\n",
      "2025-05-15 18:00:07,468 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 18:00:07,468 - INFO - Final records: 92067\n",
      "2025-05-15 18:00:07,519 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 18:00:10,837 - INFO - Successfully uploaded 92067 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20231012/data/PT-104_channel_03.txt\n",
      "2025-05-15 18:00:10,838 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20231121/data/PT-104_channel_01.txt\n",
      "2025-05-15 18:00:10,839 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 18:00:10,935 - INFO - Validation statistics:\n",
      "2025-05-15 18:00:10,935 - INFO - Initial records: 166615\n",
      "2025-05-15 18:00:10,936 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 18:00:10,936 - INFO - Final records: 166615\n",
      "2025-05-15 18:00:11,024 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 18:00:22,348 - INFO - Successfully uploaded 166615 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20231121/data/PT-104_channel_01.txt\n",
      "2025-05-15 18:00:22,350 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20231121/data/PT-104_channel_03.txt\n",
      "2025-05-15 18:00:22,350 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 18:00:22,468 - INFO - Validation statistics:\n",
      "2025-05-15 18:00:22,469 - INFO - Initial records: 166615\n",
      "2025-05-15 18:00:22,469 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 18:00:22,469 - INFO - Final records: 166615\n",
      "2025-05-15 18:00:22,583 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 18:00:44,621 - INFO - Successfully uploaded 166615 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20231121/data/PT-104_channel_03.txt\n",
      "2025-05-15 18:00:44,623 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240108/data/PT-104_channel_01.txt\n",
      "2025-05-15 18:00:44,623 - INFO - Sensor: PT-104, Channel: 1\n",
      "2025-05-15 18:00:44,690 - INFO - Validation statistics:\n",
      "2025-05-15 18:00:44,691 - INFO - Initial records: 134178\n",
      "2025-05-15 18:00:44,691 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 18:00:44,691 - INFO - Final records: 134178\n",
      "2025-05-15 18:00:44,798 - INFO - Found existing sensor: PT-104 channel 1\n",
      "2025-05-15 18:00:49,738 - INFO - Successfully uploaded 134178 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240108/data/PT-104_channel_01.txt\n",
      "2025-05-15 18:00:49,740 - INFO - Processing file: ../../sample_data/datasets/PeroCube-sample-data/data_20240108/data/PT-104_channel_03.txt\n",
      "2025-05-15 18:00:49,741 - INFO - Sensor: PT-104, Channel: 3\n",
      "2025-05-15 18:00:49,816 - INFO - Validation statistics:\n",
      "2025-05-15 18:00:49,817 - INFO - Initial records: 134178\n",
      "2025-05-15 18:00:49,817 - INFO - Removed timestamp NaN values: 0\n",
      "2025-05-15 18:00:49,818 - INFO - Final records: 134178\n",
      "2025-05-15 18:00:49,902 - INFO - Found existing sensor: PT-104 channel 3\n",
      "2025-05-15 18:00:54,637 - INFO - Successfully uploaded 134178 rows from ../../sample_data/datasets/PeroCube-sample-data/data_20240108/data/PT-104_channel_03.txt\n",
      "2025-05-15 18:00:54,639 - INFO - Processing complete. Processed 34 files, skipped 0 files, errors in 0 files. Inserted 2230132 data points in 108.16 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Run the data processing with the configured root directory\n",
    "print(f\"Starting irradiance data processing from directory: {ROOT_DIRECTORY}\")\n",
    "stats = process_irradiance_files(ROOT_DIRECTORY, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc8d69",
   "metadata": {},
   "source": [
    "## 6. Results Summary\n",
    "\n",
    "After processing the irradiance data files, here's a summary of what was accomplished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b6a8037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " File Processing Summary\n",
      "\n",
      " Total files found:                   34\n",
      " Successfully processed:              34\n",
      "  Skipped (existing/empty):             0\n",
      " Errors during processing:             0\n",
      "\n",
      " Data Statistics\n",
      "\n",
      " Data points inserted:         2,230,132\n",
      "\n",
      " Performance Metrics\n",
      "\n",
      "  Total processing time:        1m 48.2s\n",
      " Processing speed:               20,618 rows/sec\n",
      "\n",
      "  Database Status\n",
      "\n",
      " Total records in database:   2,230,132\n",
      " Total sensors in database:           2\n"
     ]
    }
   ],
   "source": [
    "def format_duration(seconds):\n",
    "    \"\"\"Format duration in a human-readable format\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = seconds % 60\n",
    "    if hours > 0:\n",
    "        return f\"{hours}h {minutes}m {secs:.1f}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}m {secs:.1f}s\"\n",
    "    else:\n",
    "        return f\"{secs:.1f}s\"\n",
    "\n",
    "def format_number(n):\n",
    "    \"\"\"Format number with thousand separators\"\"\"\n",
    "    return f\"{n:,}\"\n",
    "\n",
    "# Display processing statistics\n",
    "if 'stats' in locals():\n",
    "    print(\" File Processing Summary\")\n",
    "    print(\"\")\n",
    "    print(f\" Total files found:           {format_number(stats.get('total_files', 0)):>10}\")\n",
    "    print(f\" Successfully processed:      {format_number(stats.get('files_processed', 0)):>10}\")\n",
    "    print(f\"  Skipped (existing/empty):    {format_number(stats.get('files_skipped', 0)):>10}\")\n",
    "    print(f\" Errors during processing:    {format_number(stats.get('files_error', 0)):>10}\")\n",
    "    \n",
    "    print(\"\\n Data Statistics\")\n",
    "    print(\"\")\n",
    "    print(f\" Data points inserted:        {format_number(stats.get('rows_inserted', 0)):>10}\")\n",
    "    \n",
    "    if 'duration_seconds' in stats:\n",
    "        duration = format_duration(stats['duration_seconds'])\n",
    "        print(\"\\n Performance Metrics\")\n",
    "        print(\"\")\n",
    "        print(f\"  Total processing time:      {duration:>10}\")\n",
    "        \n",
    "        if stats.get('rows_inserted', 0) > 0 and stats.get('duration_seconds', 0) > 0:\n",
    "            throughput = stats['rows_inserted'] / stats['duration_seconds']\n",
    "            print(f\" Processing speed:           {format_number(int(throughput)):>10} rows/sec\")\n",
    "    \n",
    "    # Database verification\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(\"SELECT COUNT(*) FROM irradiance_measurement\"))\n",
    "            total_count = result.scalar()\n",
    "            \n",
    "            print(\"\\n  Database Status\")\n",
    "            print(\"\")\n",
    "            print(f\" Total records in database:  {format_number(total_count):>10}\")\n",
    "            \n",
    "            result = conn.execute(text(\"SELECT COUNT(*) FROM irradiance_sensor\"))\n",
    "            sensor_count = result.scalar()\n",
    "            print(f\" Total sensors in database:  {format_number(sensor_count):>10}\")\n",
    "    except Exception as e:\n",
    "        print(\"\\n  Could not verify database status:\")\n",
    "        print(f\"   {str(e)}\")\n",
    "else:\n",
    "    print(\" No statistics available - processing may have failed\")\n",
    "    print(\"   Please check the logs above for errors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
