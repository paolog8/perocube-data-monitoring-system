{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2870f38a",
   "metadata": {},
   "source": [
    "# Temperature & Irradiance Data Upload Notebook\n",
    "\n",
    "This notebook processes and uploads temperature and irradiance sensor data from text files to the TimescaleDB database.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "- Scan directories for temperature and irradiance data files\n",
    "- Register sensor metadata in the database\n",
    "- Upload measurement data to the TimescaleDB database\n",
    "- Avoid duplicate data entries\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Running TimescaleDB instance (configured in docker-compose.yml)\n",
    "- Access to directory containing temperature and irradiance data files\n",
    "- Environment variables configured in .env file (for database connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87661cb7",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import required libraries and install any missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6f2e1-0e2b-4ea5-acc4-b84fe5cf633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.11/site-packages (2.9.10)\n"
     ]
    }
   ],
   "source": [
    "# Core data processing libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# Database libraries\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ffac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install psycopg2-binary sqlalchemy pandas tqdm pathlib python-dotenv\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f945766",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load configuration from environment variables or use defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2059eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "# Look for the .env file two directories up from the notebook location\n",
    "dotenv_path = Path(\"../../.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Database configuration from environment variables with fallbacks\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'timescaledb'),\n",
    "    'port': int(os.getenv('DB_PORT', 5432)),\n",
    "    'database': os.getenv('DB_NAME', 'perocube'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'postgres')\n",
    "}\n",
    "\n",
    "# Print database connection info (excluding password)\n",
    "print(f\"Database connection: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']} as {DB_CONFIG['user']}\")\n",
    "\n",
    "# Data directory configuration\n",
    "ROOT_DIRECTORY = os.getenv('DEFAULT_DATA_DIR', \"../../sample_data/datasets/PeroCube-sample-data\")\n",
    "\n",
    "# File matching patterns\n",
    "IRRADIANCE_FILE_PATTERN = r\"PT-(\\d+)_channel_(\\d+)\"\n",
    "TEMPERATURE_FILE_PATTERN = r\"m7004_ID_(\\w+)\"\n",
    "\n",
    "# Batch size for database operations\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "# Flag to enable/disable data validation\n",
    "VALIDATE_DATA = True\n",
    "\n",
    "# Table names\n",
    "IRRADIANCE_SENSOR_TABLE = 'irradiance_sensor'\n",
    "IRRADIANCE_MEASUREMENT_TABLE = 'irradiance_measurement'\n",
    "TEMPERATURE_SENSOR_TABLE = 'temperature_sensor'\n",
    "TEMPERATURE_MEASUREMENT_TABLE = 'temperature_measurement'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ea590-f67b-4461-8722-93908204baed",
   "metadata": {},
   "source": [
    "## 3. Utility Functions\n",
    "\n",
    "Helper functions for database connection and data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d20f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_connection(config=DB_CONFIG):\n",
    "    \"\"\"\n",
    "    Create a SQLAlchemy database engine from configuration.\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary containing database connection parameters\n",
    "        \n",
    "    Returns:\n",
    "        SQLAlchemy engine instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection_string = f\"postgresql://{config['user']}:{config['password']}@{config['host']}:{config['port']}/{config['database']}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        # Test the connection\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(\"SELECT 1\"))\n",
    "            logging.info(f\"Database connection successful: {config['host']}:{config['port']}/{config['database']}\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Database connection failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def validate_irradiance_data(df):\n",
    "    \"\"\"\n",
    "    Validate irradiance data for common issues and clean as needed.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing irradiance measurements\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned and validated DataFrame\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Ensure timestamp is in UTC\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "    \n",
    "    # Filter out physically impossible values\n",
    "    if 'irradiance' in df.columns:\n",
    "        # Typical irradiance range: 0 to 1500 W/m²\n",
    "        df = df[(df['irradiance'] >= 0) & (df['irradiance'] < 1500)]\n",
    "    \n",
    "    # Log validation results\n",
    "    filtered_count = len(df)\n",
    "    if filtered_count < original_count:\n",
    "        logging.info(f\"Filtered out {original_count - filtered_count} invalid irradiance records\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def validate_temperature_data(df):\n",
    "    \"\"\"\n",
    "    Validate temperature data for common issues and clean as needed.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing temperature measurements\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned and validated DataFrame\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Ensure timestamp is in UTC\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "    \n",
    "    # Filter out physically impossible values for outdoor measurements\n",
    "    if 'temperature' in df.columns:\n",
    "        # Reasonable temperature range for outdoor monitoring: -50°C to 100°C\n",
    "        df = df[(df['temperature'] >= -50) & (df['temperature'] <= 100)]\n",
    "    \n",
    "    # Log validation results\n",
    "    filtered_count = len(df)\n",
    "    if filtered_count < original_count:\n",
    "        logging.info(f\"Filtered out {original_count - filtered_count} invalid temperature records\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def check_existing_data(engine, table, sensor_id_col, sensor_id, timestamp, id_field='id'):\n",
    "    \"\"\"\n",
    "    Check if data already exists in the database for given parameters.\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine\n",
    "        table: Table name\n",
    "        sensor_id_col: Column name for sensor ID\n",
    "        sensor_id: Sensor ID value\n",
    "        timestamp: Timestamp to check\n",
    "        id_field: Primary key field name\n",
    "        \n",
    "    Returns:\n",
    "        Boolean indicating if data exists\n",
    "    \"\"\"\n",
    "    if not timestamp:\n",
    "        return False\n",
    "        \n",
    "    # Build a query to check for existing data\n",
    "    query = text(f\"\"\"\n",
    "        SELECT {id_field}\n",
    "        FROM {table}\n",
    "        WHERE timestamp = :timestamp\n",
    "          AND {sensor_id_col} = :sensor_id\n",
    "        LIMIT 1\n",
    "    \"\"\")\n",
    "    \n",
    "    # Execute the query\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query, {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"sensor_id\": sensor_id\n",
    "        })\n",
    "        row = result.fetchone()\n",
    "        \n",
    "    # If row is not None, data exists\n",
    "    return row is not None\n",
    "\n",
    "def get_sensor_id(engine, sensor_table, name_col, name_val, channel_col=None, channel_val=None):\n",
    "    \"\"\"\n",
    "    Get sensor ID from the database or create if it doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine\n",
    "        sensor_table: Sensor table name\n",
    "        name_col: Column name for sensor name\n",
    "        name_val: Sensor name value\n",
    "        channel_col: Column name for channel (optional)\n",
    "        channel_val: Channel value (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Sensor ID\n",
    "    \"\"\"\n",
    "    # Build the query based on whether channel is provided\n",
    "    if channel_col and channel_val is not None:\n",
    "        query = text(f\"\"\"\n",
    "            SELECT {sensor_table}_id FROM {sensor_table} \n",
    "            WHERE {name_col} = :name_val AND {channel_col} = :channel_val\n",
    "        \"\"\")\n",
    "        params = {\"name_val\": name_val, \"channel_val\": channel_val}\n",
    "    else:\n",
    "        query = text(f\"\"\"\n",
    "            SELECT {sensor_table}_id FROM {sensor_table} \n",
    "            WHERE {name_col} = :name_val\n",
    "        \"\"\")\n",
    "        params = {\"name_val\": name_val}\n",
    "    \n",
    "    # Execute the query\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query, params)\n",
    "        row = result.fetchone()\n",
    "        \n",
    "    # Return the ID if found\n",
    "    if row:\n",
    "        return row[0]\n",
    "    else:\n",
    "        logging.warning(f\"Sensor not found: {name_val} in {sensor_table}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45911c6b",
   "metadata": {},
   "source": [
    "## 4. Sensor Registration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5554de48-89aa-4058-82de-8d53637f46d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to staging_irradiance_sensor table.\n"
     ]
    }
   ],
   "source": [
    "def register_irradiance_sensors(root_dir, engine, pattern=IRRADIANCE_FILE_PATTERN):\n",
    "    \"\"\"\n",
    "    Scan directories for irradiance sensor files and register unique sensors in the database.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: The root directory to start the search from\n",
    "        engine: SQLAlchemy engine for database connection\n",
    "        pattern: Regex pattern to match irradiance files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistics about registered sensors\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'sensors_found': 0,\n",
    "        'sensors_registered': 0,\n",
    "        'sensors_existing': 0\n",
    "    }\n",
    "    \n",
    "    # Convert to Path object for better path handling\n",
    "    root_path = Path(root_dir)\n",
    "    if not root_path.exists():\n",
    "        logging.error(f\"Root directory does not exist: {root_dir}\")\n",
    "        return stats\n",
    "    \n",
    "    # Compile the regex pattern for efficiency\n",
    "    pattern_compiled = re.compile(pattern)\n",
    "    \n",
    "    # Create a DataFrame to store sensor information\n",
    "    irradiance_sensors = pd.DataFrame(columns=['name', 'channel', 'date_installed', 'location', 'installation_angle'])\n",
    "    existing_sensors = set()  # Track unique sensor-channel combinations\n",
    "\n",
    "    # Scan directories for sensor files\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        path_parts = Path(dirpath).parts\n",
    "        if any(part.startswith(\"data\") for part in path_parts):\n",
    "            for filename in filenames:\n",
    "                match = pattern_compiled.search(filename)\n",
    "                if match:\n",
    "                    # Extract sensor name and channel\n",
    "                    sensor_name = \"PT-\" + match.group(1)\n",
    "                    channel = int(match.group(2))  # Ensure channel is an integer\n",
    "                    sensor_key = (sensor_name, channel)  # Unique key\n",
    "                    \n",
    "                    if sensor_key not in existing_sensors:\n",
    "                        # Create a new sensor entry\n",
    "                        sensor_data = {\n",
    "                            'name': sensor_name,\n",
    "                            'channel': channel,\n",
    "                            'date_installed': None,  # Would need additional data source\n",
    "                            'location': None,        # Would need additional data source\n",
    "                            'installation_angle': None  # Would need additional data source\n",
    "                        }\n",
    "                        \n",
    "                        # Add to DataFrame\n",
    "                        irradiance_sensors = pd.concat([irradiance_sensors, pd.DataFrame([sensor_data])], ignore_index=True)\n",
    "                        existing_sensors.add(sensor_key)\n",
    "                        stats['sensors_found'] += 1\n",
    "\n",
    "    # Check which sensors already exist in the database\n",
    "    for _, sensor in irradiance_sensors.iterrows():\n",
    "        query = text(f\"\"\"\n",
    "            SELECT {IRRADIANCE_SENSOR_TABLE}_id FROM {IRRADIANCE_SENSOR_TABLE} \n",
    "            WHERE name = :name AND channel = :channel\n",
    "        \"\"\")\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(query, {\"name\": sensor['name'], \"channel\": sensor['channel']})\n",
    "            exists = result.fetchone() is not None\n",
    "            \n",
    "        if exists:\n",
    "            stats['sensors_existing'] += 1\n",
    "        else:\n",
    "            # Insert new sensor\n",
    "            try:\n",
    "                # Convert NaN to None\n",
    "                sensor_dict = {k: (None if pd.isna(v) else v) for k, v in sensor.items()}\n",
    "                \n",
    "                insert_query = text(f\"\"\"\n",
    "                    INSERT INTO {IRRADIANCE_SENSOR_TABLE} (name, channel, date_installed, location, installation_angle)\n",
    "                    VALUES (:name, :channel, :date_installed, :location, :installation_angle)\n",
    "                    RETURNING {IRRADIANCE_SENSOR_TABLE}_id\n",
    "                \"\"\")\n",
    "                \n",
    "                with engine.connect() as conn:\n",
    "                    result = conn.execute(insert_query, sensor_dict)\n",
    "                    conn.commit()\n",
    "                    stats['sensors_registered'] += 1\n",
    "                    logging.info(f\"Registered irradiance sensor: {sensor['name']} channel {sensor['channel']}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to register sensor {sensor['name']} channel {sensor['channel']}: {str(e)}\")\n",
    "    \n",
    "    logging.info(f\"Found {stats['sensors_found']} unique irradiance sensors\")\n",
    "    logging.info(f\"Registered {stats['sensors_registered']} new irradiance sensors\")\n",
    "    logging.info(f\"Found {stats['sensors_existing']} existing irradiance sensors\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def register_temperature_sensors(root_dir, engine, pattern=TEMPERATURE_FILE_PATTERN):\n",
    "    \"\"\"\n",
    "    Scan directories for temperature sensor files and register unique sensors in the database.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: The root directory to start the search from\n",
    "        engine: SQLAlchemy engine for database connection\n",
    "        pattern: Regex pattern to match temperature files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistics about registered sensors\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'sensors_found': 0,\n",
    "        'sensors_registered': 0,\n",
    "        'sensors_existing': 0\n",
    "    }\n",
    "    \n",
    "    # Convert to Path object for better path handling\n",
    "    root_path = Path(root_dir)\n",
    "    if not root_path.exists():\n",
    "        logging.error(f\"Root directory does not exist: {root_dir}\")\n",
    "        return stats\n",
    "    \n",
    "    # Compile the regex pattern for efficiency\n",
    "    pattern_compiled = re.compile(pattern)\n",
    "    \n",
    "    # Create a DataFrame to store sensor information\n",
    "    temperature_sensors = pd.DataFrame(columns=['device_id', 'date_installed', 'location'])\n",
    "    existing_sensors = set()  # Track unique sensor IDs\n",
    "\n",
    "    # Scan directories for sensor files\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        path_parts = Path(dirpath).parts\n",
    "        if any(part.startswith(\"data\") for part in path_parts):\n",
    "            for filename in filenames:\n",
    "                match = pattern_compiled.search(filename)\n",
    "                if match:\n",
    "                    # Extract sensor device ID\n",
    "                    device_id = match.group(1)\n",
    "                    \n",
    "                    if device_id not in existing_sensors:\n",
    "                        # Create a new sensor entry\n",
    "                        sensor_data = {\n",
    "                            'device_id': device_id,\n",
    "                            'date_installed': None,  # Would need additional data source\n",
    "                            'location': None         # Would need additional data source\n",
    "                        }\n",
    "                        \n",
    "                        # Add to DataFrame\n",
    "                        temperature_sensors = pd.concat([temperature_sensors, pd.DataFrame([sensor_data])], ignore_index=True)\n",
    "                        existing_sensors.add(device_id)\n",
    "                        stats['sensors_found'] += 1\n",
    "\n",
    "    # Check which sensors already exist in the database\n",
    "    for _, sensor in temperature_sensors.iterrows():\n",
    "        query = text(f\"\"\"\n",
    "            SELECT {TEMPERATURE_SENSOR_TABLE}_id FROM {TEMPERATURE_SENSOR_TABLE} \n",
    "            WHERE device_id = :device_id\n",
    "        \"\"\")\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(query, {\"device_id\": sensor['device_id']})\n",
    "            exists = result.fetchone() is not None\n",
    "            \n",
    "        if exists:\n",
    "            stats['sensors_existing'] += 1\n",
    "        else:\n",
    "            # Insert new sensor\n",
    "            try:\n",
    "                # Convert NaN to None\n",
    "                sensor_dict = {k: (None if pd.isna(v) else v) for k, v in sensor.items()}\n",
    "                \n",
    "                insert_query = text(f\"\"\"\n",
    "                    INSERT INTO {TEMPERATURE_SENSOR_TABLE} (device_id, date_installed, location)\n",
    "                    VALUES (:device_id, :date_installed, :location)\n",
    "                    RETURNING {TEMPERATURE_SENSOR_TABLE}_id\n",
    "                \"\"\")\n",
    "                \n",
    "                with engine.connect() as conn:\n",
    "                    result = conn.execute(insert_query, sensor_dict)\n",
    "                    conn.commit()\n",
    "                    stats['sensors_registered'] += 1\n",
    "                    logging.info(f\"Registered temperature sensor: {sensor['device_id']}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to register temperature sensor {sensor['device_id']}: {str(e)}\")\n",
    "    \n",
    "    logging.info(f\"Found {stats['sensors_found']} unique temperature sensors\")\n",
    "    logging.info(f\"Registered {stats['sensors_registered']} new temperature sensors\")\n",
    "    logging.info(f\"Found {stats['sensors_existing']} existing temperature sensors\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7658b804",
   "metadata": {},
   "source": [
    "## 5. Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0494d4-6a4f-4874-924e-d6ec1da9be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_irradiance_files(root_dir, engine, pattern=IRRADIANCE_FILE_PATTERN, batch_size=BATCH_SIZE, validate=VALIDATE_DATA):\n",
    "    \"\"\"\n",
    "    Process irradiance data files and upload to database.\n",
    "\n",
    "    Args:\n",
    "        root_dir: The root directory to start the search from\n",
    "        engine: SQLAlchemy engine for database connection\n",
    "        pattern: Regex pattern to match irradiance files\n",
    "        batch_size: Number of rows to insert in one batch\n",
    "        validate: Whether to perform data validation\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistics about the processing\n",
    "    \"\"\"\n",
    "    # Statistics to return\n",
    "    stats = {\n",
    "        'files_processed': 0,\n",
    "        'files_skipped': 0,\n",
    "        'files_error': 0,\n",
    "        'rows_inserted': 0,\n",
    "        'start_time': datetime.now(timezone.utc),\n",
    "        'total_files': 0\n",
    "    }\n",
    "\n",
    "    # Convert to Path object for better path handling\n",
    "    root_path = Path(root_dir)\n",
    "    if not root_path.exists():\n",
    "        logging.error(f\"Root directory does not exist: {root_dir}\")\n",
    "        return stats\n",
    "\n",
    "    # Compile the regex pattern for efficiency\n",
    "    pattern_compiled = re.compile(pattern)\n",
    "\n",
    "    # First, collect all matching filepaths\n",
    "    matching_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        path_parts = Path(dirpath).parts\n",
    "        if any(part.startswith(\"data\") for part in path_parts):\n",
    "            for filename in filenames:\n",
    "                filepath = Path(dirpath) / filename\n",
    "                if pattern_compiled.search(filename):\n",
    "                    matching_files.append(filepath)\n",
    "\n",
    "    stats['total_files'] = len(matching_files)\n",
    "    logging.info(f\"Found {len(matching_files)} irradiance data files to process\")\n",
    "\n",
    "    # Process files with a progress bar\n",
    "    with tqdm(total=len(matching_files), desc=\"Processing Irradiance Files\") as pbar:\n",
    "        for filepath in matching_files:\n",
    "            try:\n",
    "                # Extract filename for pattern matching\n",
    "                filename = filepath.name\n",
    "                match = pattern_compiled.search(filename)\n",
    "\n",
    "                if not match:\n",
    "                    logging.warning(f\"Skipping file without proper format: {filepath}\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Extract sensor name and channel\n",
    "                sensor_name = \"PT-\" + match.group(1)\n",
    "                channel = int(match.group(2))\n",
    "\n",
    "                # Log processing information\n",
    "                logging.info(f\"Processing irradiance file: {filepath} (Sensor: {sensor_name}, Channel: {channel})\")\n",
    "\n",
    "                # Get the sensor ID from the database\n",
    "                sensor_id = get_sensor_id(\n",
    "                    engine,\n",
    "                    IRRADIANCE_SENSOR_TABLE,\n",
    "                    'name', sensor_name,\n",
    "                    'channel', channel\n",
    "                )\n",
    "\n",
    "                if not sensor_id:\n",
    "                    logging.warning(f\"Sensor not found: {sensor_name} (Channel: {channel}). Skipping file.\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Read the file into a pandas DataFrame\n",
    "                df = pd.read_csv(\n",
    "                    filepath,\n",
    "                    sep='\\t',\n",
    "                    names=['timestamp', 'raw_reading', 'irradiance']\n",
    "                )\n",
    "\n",
    "                if df.empty:\n",
    "                    logging.warning(f\"Empty file: {filepath}\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Ensure timestamp is in UTC format\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "\n",
    "                # Add sensor ID to the DataFrame\n",
    "                df['irradiance_sensor_id'] = sensor_id\n",
    "\n",
    "                # Validate data if enabled\n",
    "                if validate:\n",
    "                    df = validate_irradiance_data(df)\n",
    "                    if df.empty:\n",
    "                        logging.warning(f\"All data filtered during validation: {filepath}\")\n",
    "                        stats['files_skipped'] += 1\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                # Check if last data point exists to avoid duplicates\n",
    "                last_timestamp = df['timestamp'].iloc[-1]\n",
    "                data_exists = check_existing_data(\n",
    "                    engine,\n",
    "                    IRRADIANCE_MEASUREMENT_TABLE,\n",
    "                    'irradiance_sensor_id',\n",
    "                    sensor_id,\n",
    "                    last_timestamp\n",
    "                )\n",
    "\n",
    "                if data_exists:\n",
    "                    logging.info(f\"Data already exists for {filepath}. Skipping file.\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                else:\n",
    "                    # Upload data in batches for large files\n",
    "                    total_rows = len(df)\n",
    "                    for i in range(0, total_rows, batch_size):\n",
    "                        batch_df = df.iloc[i:i+batch_size]\n",
    "                        batch_df.to_sql(\n",
    "                            IRRADIANCE_MEASUREMENT_TABLE,\n",
    "                            engine,\n",
    "                            if_exists='append',\n",
    "                            index=False\n",
    "                        )\n",
    "\n",
    "                    stats['rows_inserted'] += total_rows\n",
    "                    stats['files_processed'] += 1\n",
    "                    logging.info(f\"Successfully uploaded {total_rows} rows from {filepath}\")\n",
    "\n",
    "                # Clean up\n",
    "                del df\n",
    "                pbar.update(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {filepath}: {str(e)}\")\n",
    "                stats['files_error'] += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Calculate duration\n",
    "    stats['end_time'] = datetime.now(timezone.utc)\n",
    "    stats['duration_seconds'] = (stats['end_time'] - stats['start_time']).total_seconds()\n",
    "\n",
    "    logging.info(f\"Processing complete. Processed {stats['files_processed']} files, \"\n",
    "                 f\"skipped {stats['files_skipped']} files, \"\n",
    "                 f\"errors in {stats['files_error']} files. \"\n",
    "                 f\"Inserted {stats['rows_inserted']} data points in {stats['duration_seconds']:.2f} seconds.\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "def process_temperature_files(root_dir, engine, pattern=TEMPERATURE_FILE_PATTERN, batch_size=BATCH_SIZE, validate=VALIDATE_DATA):\n",
    "    \"\"\"\n",
    "    Process temperature data files and upload to database.\n",
    "\n",
    "    Args:\n",
    "        root_dir: The root directory to start the search from\n",
    "        engine: SQLAlchemy engine for database connection\n",
    "        pattern: Regex pattern to match temperature files\n",
    "        batch_size: Number of rows to insert in one batch\n",
    "        validate: Whether to perform data validation\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistics about the processing\n",
    "    \"\"\"\n",
    "    # Statistics to return\n",
    "    stats = {\n",
    "        'files_processed': 0,\n",
    "        'files_skipped': 0,\n",
    "        'files_error': 0,\n",
    "        'rows_inserted': 0,\n",
    "        'start_time': datetime.now(timezone.utc),\n",
    "        'total_files': 0\n",
    "    }\n",
    "\n",
    "    # Convert to Path object for better path handling\n",
    "    root_path = Path(root_dir)\n",
    "    if not root_path.exists():\n",
    "        logging.error(f\"Root directory does not exist: {root_dir}\")\n",
    "        return stats\n",
    "\n",
    "    # Compile the regex pattern for efficiency\n",
    "    pattern_compiled = re.compile(pattern)\n",
    "\n",
    "    # First, collect all matching filepaths\n",
    "    matching_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        path_parts = Path(dirpath).parts\n",
    "        if any(part.startswith(\"data\") for part in path_parts):\n",
    "            for filename in filenames:\n",
    "                filepath = Path(dirpath) / filename\n",
    "                if pattern_compiled.search(filename):\n",
    "                    matching_files.append(filepath)\n",
    "\n",
    "    stats['total_files'] = len(matching_files)\n",
    "    logging.info(f\"Found {len(matching_files)} temperature data files to process\")\n",
    "\n",
    "    # Process files with a progress bar\n",
    "    with tqdm(total=len(matching_files), desc=\"Processing Temperature Files\") as pbar:\n",
    "        for filepath in matching_files:\n",
    "            try:\n",
    "                # Extract filename for pattern matching\n",
    "                filename = filepath.name\n",
    "                match = pattern_compiled.search(filename)\n",
    "\n",
    "                if not match:\n",
    "                    logging.warning(f\"Skipping file without proper format: {filepath}\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Extract device ID\n",
    "                device_id = match.group(1)\n",
    "\n",
    "                # Log processing information\n",
    "                logging.info(f\"Processing temperature file: {filepath} (Device ID: {device_id})\")\n",
    "\n",
    "                # Get the sensor ID from the database\n",
    "                sensor_id = get_sensor_id(\n",
    "                    engine,\n",
    "                    TEMPERATURE_SENSOR_TABLE,\n",
    "                    'device_id', device_id\n",
    "                )\n",
    "\n",
    "                if not sensor_id:\n",
    "                    logging.warning(f\"Temperature sensor not found: {device_id}. Skipping file.\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Read the file into a pandas DataFrame\n",
    "                # Assuming format with timestamp and temperature columns\n",
    "                df = pd.read_csv(\n",
    "                    filepath,\n",
    "                    sep='\\t',\n",
    "                    names=['timestamp', 'temperature']\n",
    "                )\n",
    "\n",
    "                if df.empty:\n",
    "                    logging.warning(f\"Empty file: {filepath}\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Ensure timestamp is in UTC format\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "\n",
    "                # Add sensor ID to the DataFrame\n",
    "                df['temperature_sensor_id'] = sensor_id\n",
    "\n",
    "                # Validate data if enabled\n",
    "                if validate:\n",
    "                    df = validate_temperature_data(df)\n",
    "                    if df.empty:\n",
    "                        logging.warning(f\"All data filtered during validation: {filepath}\")\n",
    "                        stats['files_skipped'] += 1\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                # Check if last data point exists to avoid duplicates\n",
    "                last_timestamp = df['timestamp'].iloc[-1]\n",
    "                data_exists = check_existing_data(\n",
    "                    engine,\n",
    "                    TEMPERATURE_MEASUREMENT_TABLE,\n",
    "                    'temperature_sensor_id',\n",
    "                    sensor_id,\n",
    "                    last_timestamp\n",
    "                )\n",
    "\n",
    "                if data_exists:\n",
    "                    logging.info(f\"Data already exists for {filepath}. Skipping file.\")\n",
    "                    stats['files_skipped'] += 1\n",
    "                else:\n",
    "                    # Upload data in batches for large files\n",
    "                    total_rows = len(df)\n",
    "                    for i in range(0, total_rows, batch_size):\n",
    "                        batch_df = df.iloc[i:i+batch_size]\n",
    "                        batch_df.to_sql(\n",
    "                            TEMPERATURE_MEASUREMENT_TABLE,\n",
    "                            engine,\n",
    "                            if_exists='append',\n",
    "                            index=False\n",
    "                        )\n",
    "\n",
    "                    stats['rows_inserted'] += total_rows\n",
    "                    stats['files_processed'] += 1\n",
    "                    logging.info(f\"Successfully uploaded {total_rows} rows from {filepath}\")\n",
    "\n",
    "                # Clean up\n",
    "                del df\n",
    "                pbar.update(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {filepath}: {str(e)}\")\n",
    "                stats['files_error'] += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Calculate duration\n",
    "    stats['end_time'] = datetime.now(timezone.utc)\n",
    "    stats['duration_seconds'] = (stats['end_time'] - stats['start_time']).total_seconds()\n",
    "\n",
    "    logging.info(f\"Processing complete. Processed {stats['files_processed']} files, \"\n",
    "                 f\"skipped {stats['files_skipped']} files, \"\n",
    "                 f\"errors in {stats['files_error']} files. \"\n",
    "                 f\"Inserted {stats['rows_inserted']} data points in {stats['duration_seconds']:.2f} seconds.\")\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8940b",
   "metadata": {},
   "source": [
    "## 6. Execute the Data Upload Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03115d31-ab12-4d9e-81ba-216619bdf396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cc94da625d4fab8cc9d24f50e4db35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Irradiance Files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing ./data_20240514/data/PT-104_channel_01.txt: (psycopg2.errors.NotNullViolation) null value in column \"timestamp\" of relation \"irradiance_measurement\" violates not-null constraint\n",
      "DETAIL:  Failing row contains (null, 4428827, 92.46, 90527cac-fcb9-41b0-8c3f-375ef83093b3).\n",
      "\n",
      "[SQL: INSERT INTO irradiance_measurement (timestamp, raw_reading, irradiance, irradiance_sensor_id) VALUES (%(timestamp__0)s, %(raw_reading__0)s, %(irradiance__0)s, %(irradiance_sensor_id__0)s), (%(timestamp__1)s, %(raw_reading__1)s, %(irradiance__1)s, %(i ... 95309 characters truncated ... 8)s), (%(timestamp__999)s, %(raw_reading__999)s, %(irradiance__999)s, %(irradiance_sensor_id__999)s)]\n",
      "[parameters: {'irradiance_sensor_id__0': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__0': datetime.datetime(2024, 4, 24, 20, 57, 59, tzinfo=datetime.timezone.utc), 'raw_reading__0': -107844, 'irradiance__0': -2.251, 'irradiance_sensor_id__1': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__1': datetime.datetime(2024, 4, 24, 20, 58, 29, tzinfo=datetime.timezone.utc), 'raw_reading__1': -107211, 'irradiance__1': -2.238, 'irradiance_sensor_id__2': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__2': datetime.datetime(2024, 4, 24, 20, 58, 59, tzinfo=datetime.timezone.utc), 'raw_reading__2': -107030, 'irradiance__2': -2.234, 'irradiance_sensor_id__3': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__3': datetime.datetime(2024, 4, 24, 20, 59, 29, tzinfo=datetime.timezone.utc), 'raw_reading__3': -107398, 'irradiance__3': -2.242, 'irradiance_sensor_id__4': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__4': datetime.datetime(2024, 4, 24, 20, 59, 59, tzinfo=datetime.timezone.utc), 'raw_reading__4': -106782, 'irradiance__4': -2.229, 'irradiance_sensor_id__5': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__5': datetime.datetime(2024, 4, 24, 21, 0, 29, tzinfo=datetime.timezone.utc), 'raw_reading__5': -106646, 'irradiance__5': -2.226, 'irradiance_sensor_id__6': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__6': datetime.datetime(2024, 4, 24, 21, 0, 59, tzinfo=datetime.timezone.utc), 'raw_reading__6': -106953, 'irradiance__6': -2.233, 'irradiance_sensor_id__7': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__7': datetime.datetime(2024, 4, 24, 21, 1, 29, tzinfo=datetime.timezone.utc), 'raw_reading__7': -106963, 'irradiance__7': -2.233, 'irradiance_sensor_id__8': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__8': datetime.datetime(2024, 4, 24, 21, 1, 59, tzinfo=datetime.timezone.utc), 'raw_reading__8': -106668, 'irradiance__8': -2.227, 'irradiance_sensor_id__9': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__9': datetime.datetime(2024, 4, 24, 21, 2, 29, tzinfo=datetime.timezone.utc), 'raw_reading__9': -106670, 'irradiance__9': -2.227, 'irradiance_sensor_id__10': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__10': datetime.datetime(2024, 4, 24, 21, 2, 59, tzinfo=datetime.timezone.utc), 'raw_reading__10': -106651, 'irradiance__10': -2.227, 'irradiance_sensor_id__11': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__11': datetime.datetime(2024, 4, 24, 21, 3, 29, tzinfo=datetime.timezone.utc), 'raw_reading__11': -107093, 'irradiance__11': -2.236, 'irradiance_sensor_id__12': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__12': datetime.datetime(2024, 4, 24, 21, 3, 59, tzinfo=datetime.timezone.utc) ... 3900 parameters truncated ... 'raw_reading__987': 10315279, 'irradiance__987': 215.35, 'irradiance_sensor_id__988': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__988': datetime.datetime(2024, 4, 25, 12, 34, 40, tzinfo=datetime.timezone.utc), 'raw_reading__988': 9368756, 'irradiance__988': 195.59, 'irradiance_sensor_id__989': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__989': datetime.datetime(2024, 4, 25, 12, 35, 10, tzinfo=datetime.timezone.utc), 'raw_reading__989': 8630145, 'irradiance__989': 180.17, 'irradiance_sensor_id__990': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__990': datetime.datetime(2024, 4, 25, 12, 35, 40, tzinfo=datetime.timezone.utc), 'raw_reading__990': 8141310, 'irradiance__990': 169.965, 'irradiance_sensor_id__991': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__991': datetime.datetime(2024, 4, 25, 12, 36, 10, tzinfo=datetime.timezone.utc), 'raw_reading__991': 7847787, 'irradiance__991': 163.837, 'irradiance_sensor_id__992': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__992': datetime.datetime(2024, 4, 25, 12, 36, 40, tzinfo=datetime.timezone.utc), 'raw_reading__992': 7751061, 'irradiance__992': 161.818, 'irradiance_sensor_id__993': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__993': datetime.datetime(2024, 4, 25, 12, 37, 10, tzinfo=datetime.timezone.utc), 'raw_reading__993': 7820316, 'irradiance__993': 163.263, 'irradiance_sensor_id__994': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__994': datetime.datetime(2024, 4, 25, 12, 37, 40, tzinfo=datetime.timezone.utc), 'raw_reading__994': 7997260, 'irradiance__994': 166.957, 'irradiance_sensor_id__995': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__995': datetime.datetime(2024, 4, 25, 12, 38, 10, tzinfo=datetime.timezone.utc), 'raw_reading__995': 8307783, 'irradiance__995': 173.44, 'irradiance_sensor_id__996': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__996': datetime.datetime(2024, 4, 25, 12, 38, 40, tzinfo=datetime.timezone.utc), 'raw_reading__996': 8723989, 'irradiance__996': 182.129, 'irradiance_sensor_id__997': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__997': datetime.datetime(2024, 4, 25, 12, 39, 10, tzinfo=datetime.timezone.utc), 'raw_reading__997': 9199563, 'irradiance__997': 192.058, 'irradiance_sensor_id__998': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__998': datetime.datetime(2024, 4, 25, 12, 39, 40, tzinfo=datetime.timezone.utc), 'raw_reading__998': 9730471, 'irradiance__998': 203.141, 'irradiance_sensor_id__999': UUID('90527cac-fcb9-41b0-8c3f-375ef83093b3'), 'timestamp__999': datetime.datetime(2024, 4, 25, 12, 40, 10, tzinfo=datetime.timezone.utc), 'raw_reading__999': 10330480, 'irradiance__999': 215.668}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/gkpj)\n",
      "Error processing ./data_20240514/data/PT-104_channel_03.txt: (psycopg2.errors.NotNullViolation) null value in column \"timestamp\" of relation \"irradiance_measurement\" violates not-null constraint\n",
      "DETAIL:  Failing row contains (null, 5100925, 106.491, f1ab9b1a-8a96-40aa-a954-33e80485975f).\n",
      "\n",
      "[SQL: INSERT INTO irradiance_measurement (timestamp, raw_reading, irradiance, irradiance_sensor_id) VALUES (%(timestamp__0)s, %(raw_reading__0)s, %(irradiance__0)s, %(irradiance_sensor_id__0)s), (%(timestamp__1)s, %(raw_reading__1)s, %(irradiance__1)s, %(i ... 95309 characters truncated ... 8)s), (%(timestamp__999)s, %(raw_reading__999)s, %(irradiance__999)s, %(irradiance_sensor_id__999)s)]\n",
      "[parameters: {'irradiance_sensor_id__0': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__0': datetime.datetime(2024, 4, 24, 20, 57, 59, tzinfo=datetime.timezone.utc), 'raw_reading__0': -105444, 'irradiance__0': -2.201, 'irradiance_sensor_id__1': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__1': datetime.datetime(2024, 4, 24, 20, 58, 29, tzinfo=datetime.timezone.utc), 'raw_reading__1': -105248, 'irradiance__1': -2.197, 'irradiance_sensor_id__2': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__2': datetime.datetime(2024, 4, 24, 20, 58, 59, tzinfo=datetime.timezone.utc), 'raw_reading__2': -105302, 'irradiance__2': -2.198, 'irradiance_sensor_id__3': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__3': datetime.datetime(2024, 4, 24, 20, 59, 29, tzinfo=datetime.timezone.utc), 'raw_reading__3': -104807, 'irradiance__3': -2.188, 'irradiance_sensor_id__4': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__4': datetime.datetime(2024, 4, 24, 20, 59, 59, tzinfo=datetime.timezone.utc), 'raw_reading__4': -105134, 'irradiance__4': -2.195, 'irradiance_sensor_id__5': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__5': datetime.datetime(2024, 4, 24, 21, 0, 29, tzinfo=datetime.timezone.utc), 'raw_reading__5': -104731, 'irradiance__5': -2.186, 'irradiance_sensor_id__6': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__6': datetime.datetime(2024, 4, 24, 21, 0, 59, tzinfo=datetime.timezone.utc), 'raw_reading__6': -104914, 'irradiance__6': -2.19, 'irradiance_sensor_id__7': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__7': datetime.datetime(2024, 4, 24, 21, 1, 29, tzinfo=datetime.timezone.utc), 'raw_reading__7': -104734, 'irradiance__7': -2.187, 'irradiance_sensor_id__8': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__8': datetime.datetime(2024, 4, 24, 21, 1, 59, tzinfo=datetime.timezone.utc), 'raw_reading__8': -104505, 'irradiance__8': -2.182, 'irradiance_sensor_id__9': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__9': datetime.datetime(2024, 4, 24, 21, 2, 29, tzinfo=datetime.timezone.utc), 'raw_reading__9': -104493, 'irradiance__9': -2.181, 'irradiance_sensor_id__10': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__10': datetime.datetime(2024, 4, 24, 21, 2, 59, tzinfo=datetime.timezone.utc), 'raw_reading__10': -104547, 'irradiance__10': -2.183, 'irradiance_sensor_id__11': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__11': datetime.datetime(2024, 4, 24, 21, 3, 29, tzinfo=datetime.timezone.utc), 'raw_reading__11': -104759, 'irradiance__11': -2.187, 'irradiance_sensor_id__12': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__12': datetime.datetime(2024, 4, 24, 21, 3, 59, tzinfo=datetime.timezone.utc) ... 3900 parameters truncated ... 'raw_reading__987': 12000458, 'irradiance__987': 250.531, 'irradiance_sensor_id__988': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__988': datetime.datetime(2024, 4, 25, 12, 34, 40, tzinfo=datetime.timezone.utc), 'raw_reading__988': 10855295, 'irradiance__988': 226.624, 'irradiance_sensor_id__989': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__989': datetime.datetime(2024, 4, 25, 12, 35, 10, tzinfo=datetime.timezone.utc), 'raw_reading__989': 10042162, 'irradiance__989': 209.648, 'irradiance_sensor_id__990': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__990': datetime.datetime(2024, 4, 25, 12, 35, 40, tzinfo=datetime.timezone.utc), 'raw_reading__990': 9478335, 'irradiance__990': 197.878, 'irradiance_sensor_id__991': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__991': datetime.datetime(2024, 4, 25, 12, 36, 10, tzinfo=datetime.timezone.utc), 'raw_reading__991': 9133382, 'irradiance__991': 190.676, 'irradiance_sensor_id__992': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__992': datetime.datetime(2024, 4, 25, 12, 36, 40, tzinfo=datetime.timezone.utc), 'raw_reading__992': 9036836, 'irradiance__992': 188.66, 'irradiance_sensor_id__993': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__993': datetime.datetime(2024, 4, 25, 12, 37, 10, tzinfo=datetime.timezone.utc), 'raw_reading__993': 9121878, 'irradiance__993': 190.436, 'irradiance_sensor_id__994': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__994': datetime.datetime(2024, 4, 25, 12, 37, 40, tzinfo=datetime.timezone.utc), 'raw_reading__994': 9347048, 'irradiance__994': 195.137, 'irradiance_sensor_id__995': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__995': datetime.datetime(2024, 4, 25, 12, 38, 10, tzinfo=datetime.timezone.utc), 'raw_reading__995': 9699592, 'irradiance__995': 202.497, 'irradiance_sensor_id__996': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__996': datetime.datetime(2024, 4, 25, 12, 38, 40, tzinfo=datetime.timezone.utc), 'raw_reading__996': 10189368, 'irradiance__996': 212.722, 'irradiance_sensor_id__997': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__997': datetime.datetime(2024, 4, 25, 12, 39, 10, tzinfo=datetime.timezone.utc), 'raw_reading__997': 10777688, 'irradiance__997': 225.004, 'irradiance_sensor_id__998': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__998': datetime.datetime(2024, 4, 25, 12, 39, 40, tzinfo=datetime.timezone.utc), 'raw_reading__998': 11409354, 'irradiance__998': 238.191, 'irradiance_sensor_id__999': UUID('f1ab9b1a-8a96-40aa-a954-33e80485975f'), 'timestamp__999': datetime.datetime(2024, 4, 25, 12, 40, 10, tzinfo=datetime.timezone.utc), 'raw_reading__999': 12091966, 'irradiance__999': 252.442}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/gkpj)\n",
      "Irradiance files processed and uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create database connection\n",
    "try:\n",
    "    engine = create_db_connection()\n",
    "    logging.info(\"Database connection established successfully\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to connect to database: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Register sensors in the database\n",
    "print(f\"Starting sensor registration from directory: {ROOT_DIRECTORY}\")\n",
    "print(\"\\n1. Registering irradiance sensors...\")\n",
    "irr_sensor_stats = register_irradiance_sensors(ROOT_DIRECTORY, engine)\n",
    "\n",
    "print(\"\\n2. Registering temperature sensors...\")\n",
    "temp_sensor_stats = register_temperature_sensors(ROOT_DIRECTORY, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd05b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Process and upload irradiance data\n",
    "print(\"\\n3. Processing irradiance data files...\")\n",
    "irr_stats = process_irradiance_files(ROOT_DIRECTORY, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Process and upload temperature data\n",
    "print(\"\\n4. Processing temperature data files...\")\n",
    "temp_stats = process_temperature_files(ROOT_DIRECTORY, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d1002",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e459c72-da19-4b28-bb87-1a7aac77d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display processing statistics\n",
    "print(\"\\n===== UPLOAD SUMMARY =====\\n\")\n",
    "\n",
    "# Sensor registration summary\n",
    "print(\"SENSOR REGISTRATION:\")\n",
    "print(f\"- Irradiance Sensors: {irr_sensor_stats.get('sensors_found', 0)} found, \"\n",
    "      f\"{irr_sensor_stats.get('sensors_registered', 0)} newly registered, \"\n",
    "      f\"{irr_sensor_stats.get('sensors_existing', 0)} already existing\")\n",
    "\n",
    "print(f\"- Temperature Sensors: {temp_sensor_stats.get('sensors_found', 0)} found, \"\n",
    "      f\"{temp_sensor_stats.get('sensors_registered', 0)} newly registered, \"\n",
    "      f\"{temp_sensor_stats.get('sensors_existing', 0)} already existing\")\n",
    "\n",
    "# Data processing summary\n",
    "print(\"\\nDATA PROCESSING:\")\n",
    "print(\"Irradiance Data:\")\n",
    "print(f\"- Files processed: {irr_stats.get('files_processed', 0)}\")\n",
    "print(f\"- Files skipped: {irr_stats.get('files_skipped', 0)}\")\n",
    "print(f\"- Files with errors: {irr_stats.get('files_error', 0)}\")\n",
    "print(f\"- Rows inserted: {irr_stats.get('rows_inserted', 0)}\")\n",
    "if 'duration_seconds' in irr_stats:\n",
    "    print(f\"- Processing time: {irr_stats['duration_seconds']:.2f} seconds\")\n",
    "\n",
    "print(\"\\nTemperature Data:\")\n",
    "print(f\"- Files processed: {temp_stats.get('files_processed', 0)}\")\n",
    "print(f\"- Files skipped: {temp_stats.get('files_skipped', 0)}\")\n",
    "print(f\"- Files with errors: {temp_stats.get('files_error', 0)}\")\n",
    "print(f\"- Rows inserted: {temp_stats.get('rows_inserted', 0)}\")\n",
    "if 'duration_seconds' in temp_stats:\n",
    "    print(f\"- Processing time: {temp_stats['duration_seconds']:.2f} seconds\")\n",
    "\n",
    "# Verify database counts\n",
    "try:\n",
    "    print(\"\\nDATABASE VERIFICATION:\")\n",
    "    with engine.connect() as conn:\n",
    "        # Get irradiance data counts\n",
    "        result = conn.execute(text(f\"SELECT COUNT(*) FROM {IRRADIANCE_MEASUREMENT_TABLE}\"))\n",
    "        irradiance_count = result.scalar()\n",
    "        print(f\"- Total irradiance measurements: {irradiance_count}\")\n",
    "        \n",
    "        # Get temperature data counts\n",
    "        result = conn.execute(text(f\"SELECT COUNT(*) FROM {TEMPERATURE_MEASUREMENT_TABLE}\"))\n",
    "        temperature_count = result.scalar()\n",
    "        print(f\"- Total temperature measurements: {temperature_count}\")\n",
    "        \n",
    "        # Get sensor counts\n",
    "        result = conn.execute(text(f\"SELECT COUNT(*) FROM {IRRADIANCE_SENSOR_TABLE}\"))\n",
    "        irradiance_sensor_count = result.scalar()\n",
    "        print(f\"- Total irradiance sensors: {irradiance_sensor_count}\")\n",
    "        \n",
    "        result = conn.execute(text(f\"SELECT COUNT(*) FROM {TEMPERATURE_SENSOR_TABLE}\"))\n",
    "        temperature_sensor_count = result.scalar()\n",
    "        print(f\"- Total temperature sensors: {temperature_sensor_count}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not query database: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ca5c51",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "After successfully uploading the temperature and irradiance data, you might want to:\n",
    "\n",
    "1. Analyze correlations between environmental conditions and solar cell performance\n",
    "2. Set up dashboards to visualize temperature and irradiance trends\n",
    "3. Create reports comparing different sensors and locations\n",
    "4. Implement automated data quality monitoring\n",
    "\n",
    "See the other notebooks in this project for additional data processing and analysis examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
