{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7e94ca",
   "metadata": {},
   "source": [
    "# Perocube Logbook Data Upload Notebook\n",
    "\n",
    "This notebook processes and uploads logbook data from the Perocube Excel file to the TimescaleDB database.\n",
    "\n",
    "## Purpose\n",
    "- Read logbook data from 'PeroCube_logbook_example.xlsx'\n",
    "- Parse and validate the data\n",
    "- Upload the data to the TimescaleDB database\n",
    "- Avoid duplicate data entries\n",
    "\n",
    "## Prerequisites\n",
    "- Running TimescaleDB instance (configured in docker-compose.yml)\n",
    "- Access to the Perocube logbook Excel file\n",
    "- Environment variables configured in .env file (for database connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e0476",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import required libraries and install any missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# Database libraries\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af362b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install psycopg2-binary sqlalchemy pandas tqdm pathlib python-dotenv openpyxl\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41d919",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load configuration from environment variables and set up constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for the .env file two directories up from the notebook location\n",
    "dotenv_path = Path(\"../../.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Database configuration from environment variables with fallbacks\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'port': int(os.getenv('DB_PORT', 5432)),\n",
    "    'database': os.getenv('DB_NAME', 'perocube'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'postgres')\n",
    "}\n",
    "\n",
    "# Print database connection info (excluding password)\n",
    "print(f\"Database connection: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']} as {DB_CONFIG['user']}\")\n",
    "\n",
    "# Data directory and file configuration\n",
    "ROOT_DIRECTORY = os.getenv('DEFAULT_DATA_DIR', \"../../sample_data/datasets/PeroCube-sample-data\")\n",
    "LOGBOOK_FILE = \"PeroCube_logbook_example.xlsx\"\n",
    "LOGBOOK_SHEET = \"Perocube history\"\n",
    "\n",
    "# Batch size for database operations\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd22ffb",
   "metadata": {},
   "source": [
    "## 3. Read and Process Logbook Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full path to the logbook file\n",
    "logbook_path = Path(ROOT_DIRECTORY) / LOGBOOK_FILE\n",
    "\n",
    "# Read the Excel sheet, skip first row and use second row as header\n",
    "try:\n",
    "    df = pd.read_excel(logbook_path, sheet_name=LOGBOOK_SHEET, header=1)\n",
    "    print(f\"Successfully read {len(df)} rows from {LOGBOOK_FILE}\")\n",
    "    \n",
    "    # Display the first few rows and data info\n",
    "    print(\"\\nColumn names:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    print(\"\\nFirst few rows of the data:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nDataset information:\")\n",
    "    display(df.info())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading Excel file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze current dataframe state\n",
    "print(\"Checking for unnamed columns:\")\n",
    "unnamed_cols = [col for col in df.columns if 'Unnamed' in str(col)]\n",
    "print(f\"Unnamed columns found: {unnamed_cols}\")\n",
    "\n",
    "print(\"\\nCurrent data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nTotal rows with all missing values:\")\n",
    "print(df.isna().all(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframe\n",
    "\n",
    "# 1. Remove unnamed columns\n",
    "df = df.loc[:, ~df.columns.str.contains('Unnamed')]\n",
    "\n",
    "# 2. Drop rows where all values are missing\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "# Print cleaning results\n",
    "print(\"Dataframe shape after cleaning:\")\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "# Display updated column list\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display updated missing values count\n",
    "print(\"\\nMissing values per column after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display first few rows of cleaned data\n",
    "print(\"\\nFirst few rows of cleaned data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e47291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove completely empty Comment 2 column\n",
    "df = df.drop('Comment 2', axis=1)\n",
    "\n",
    "# Print updated dataframe info\n",
    "print(\"Dataframe shape after removing Comment 2:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# Display updated column list\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display first few rows of cleaned data\n",
    "print(\"\\nFirst few rows of cleaned data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee62e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data types\n",
    "\n",
    "# 1. Convert date columns to datetime\n",
    "df['Date removed'] = pd.to_datetime(df['Date removed'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Date installed'] = pd.to_datetime(df['Date installed'], format='%d.%m.%Y', errors='coerce')\n",
    "\n",
    "# 2. Convert numeric columns\n",
    "numeric_columns = ['Board', 'Channel', 'Status', 'Area', 'Init.PCE']\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 3. Ensure string columns are properly formatted (strip whitespace)\n",
    "string_columns = ['Cell type', 'Cell name', 'Pixel', 'Encap.', 'Structure', \n",
    "                  'Producer', 'Owner', 'Project', 'Temp sensor', 'Comment 1']\n",
    "for col in string_columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    # Replace 'nan' strings with actual NaN\n",
    "    df[col] = df[col].replace('nan', pd.NA)\n",
    "\n",
    "# Display the updated data types\n",
    "print(\"Updated data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display sample of the data to verify conversions\n",
    "print(\"\\nSample of converted data:\")\n",
    "display(df.head(25))\n",
    "\n",
    "# Check for any conversion issues (invalid dates or numbers)\n",
    "print(\"\\nCount of NaN values after type conversion:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dcff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where both dates are null\n",
    "original_rows = len(df)\n",
    "df = df.dropna(subset=['Date removed', 'Date installed'], how='all')\n",
    "\n",
    "# Print results\n",
    "print(f\"Removed {original_rows - len(df)} rows where both dates were missing\")\n",
    "print(f\"Remaining rows: {len(df)}\")\n",
    "\n",
    "# Display missing values count after removal\n",
    "print(\"\\nMissing values per column after removing rows with missing dates:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Display sample of remaining data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rows where installation date is missing\n",
    "missing_install_date = df[df['Date installed'].isna()]\n",
    "\n",
    "print(f\"Found {len(missing_install_date)} rows with missing installation date:\\n\")\n",
    "display(missing_install_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing installation dates with January 1st, 2020\n",
    "default_install_date = pd.to_datetime('2020-01-01')\n",
    "df['Date installed'] = df['Date installed'].fillna(default_install_date)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Checking for any remaining missing installation dates:\")\n",
    "print(f\"Missing installation dates: {df['Date installed'].isna().sum()}\")\n",
    "\n",
    "# Display the rows that were updated\n",
    "print(\"\\nVerifying the rows that were updated:\")\n",
    "display(df[df['Date installed'] == default_install_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Board and Channel to integers (as per database schema)\n",
    "df['Board'] = df['Board'].astype('Int64')  # Using Int64 to handle NaN values\n",
    "df['Channel'] = df['Channel'].astype('Int64')  # Using Int64 to handle NaN values\n",
    "\n",
    "# Verify the conversion\n",
    "print(\"Updated data types for Board and Channel:\")\n",
    "print(df[['Board', 'Channel']].dtypes)\n",
    "\n",
    "# Display a sample to verify the conversion\n",
    "print(\"\\nSample of Board and Channel data:\")\n",
    "display(df[['Board', 'Channel']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee39dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align numeric types with database schema\n",
    "df['Area'] = df['Area'].astype('float64')\n",
    "df['Init.PCE'] = df['Init.PCE'].astype('float64')\n",
    "\n",
    "# Validate and truncate string columns to match VARCHAR(255)\n",
    "string_columns = ['Cell type', 'Cell name', 'Pixel', 'Encap.', 'Structure', \n",
    "                 'Producer', 'Owner', 'Project', 'Temp sensor', 'Comment 1']\n",
    "\n",
    "for col in string_columns:\n",
    "    # Check if any string is longer than 255 characters\n",
    "    mask = df[col].str.len() > 255\n",
    "    if mask.any():\n",
    "        print(f\"Warning: Found {mask.sum()} values in {col} longer than 255 characters. Truncating...\")\n",
    "        df.loc[mask, col] = df.loc[mask, col].str.slice(0, 255)\n",
    "\n",
    "# Display updated data types\n",
    "print(\"\\nUpdated data types after database alignment:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for any values that might be too long\n",
    "print(\"\\nMaximum string lengths:\")\n",
    "for col in string_columns:\n",
    "    max_len = df[col].str.len().max()\n",
    "    print(f\"{col}: {max_len}\")\n",
    "\n",
    "# Display sample of numeric columns\n",
    "print(\"\\nSample of numeric columns:\")\n",
    "display(df[['Area', 'Init.PCE']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7d277",
   "metadata": {},
   "source": [
    "## 4. Database Mapping Reference\n",
    "\n",
    "Based on the database schema in `tables.sql`, our logbook data maps to the following tables:\n",
    "\n",
    "### 1. scientist table\n",
    "```sql\n",
    "CREATE TABLE scientist (\n",
    "    scientist_id UUID PRIMARY KEY,\n",
    "    name VARCHAR(255) NOT NULL\n",
    ");\n",
    "```\n",
    "- Maps from: 'Owner', 'Producer' columns\n",
    "- Fields:\n",
    "  - scientist_id (UUID, generated)\n",
    "  - name (from 'Owner' and 'Producer')\n",
    "\n",
    "### 2. solar_cell_device table\n",
    "```sql\n",
    "CREATE TABLE solar_cell_device (\n",
    "    name VARCHAR(255) PRIMARY KEY,\n",
    "    nomad_id UUID UNIQUE,\n",
    "    technology VARCHAR(255),\n",
    "    area DOUBLE PRECISION,\n",
    "    initial_pce DOUBLE PRECISION,\n",
    "    date_produced TIMESTAMP WITH TIME ZONE,\n",
    "    form_factor VARCHAR(255),\n",
    "    encapsulation VARCHAR(255),\n",
    "    experiment_id UUID,\n",
    "    date_encapsulated TIMESTAMP WITH TIME ZONE,\n",
    "    owner_id UUID REFERENCES scientist(scientist_id),\n",
    "    producer_id UUID REFERENCES scientist(scientist_id)\n",
    ");\n",
    "```\n",
    "- Maps from multiple columns\n",
    "- Fields:\n",
    "  - name (from 'Cell name')\n",
    "  - nomad_id (UUID, generated)\n",
    "  - technology (from 'Cell type')\n",
    "  - area (from 'Area')\n",
    "  - initial_pce (from 'Init.PCE')\n",
    "  - date_produced (can use 'Date installed')\n",
    "  - form_factor (not in logbook)\n",
    "  - encapsulation (from 'Encap.')\n",
    "  - experiment_id (not in logbook)\n",
    "  - date_encapsulated (not in logbook)\n",
    "  - owner_id (link to scientist table)\n",
    "  - producer_id (link to scientist table)\n",
    "\n",
    "### 3. solar_cell_pixel table\n",
    "```sql\n",
    "CREATE TABLE solar_cell_pixel (\n",
    "    solar_cell_id VARCHAR(255) REFERENCES solar_cell_device(name),\n",
    "    pixel VARCHAR(255),\n",
    "    active_area DOUBLE PRECISION,\n",
    "    PRIMARY KEY (solar_cell_id, pixel)\n",
    ");\n",
    "```\n",
    "- Maps from pixel-specific data\n",
    "- Fields:\n",
    "  - solar_cell_id (foreign key to solar_cell_device.name)\n",
    "  - pixel (from 'Pixel' column)\n",
    "  - active_area (from 'Area')\n",
    "\n",
    "### 4. mpp_tracking_channel table\n",
    "```sql\n",
    "CREATE TABLE mpp_tracking_channel (\n",
    "    board INTEGER,\n",
    "    channel INTEGER,\n",
    "    PRIMARY KEY (board, channel)\n",
    ");\n",
    "```\n",
    "- Maps from channel data\n",
    "- Fields:\n",
    "  - board (from 'Board')\n",
    "  - channel (from 'Channel')\n",
    "\n",
    "### 5. measurement_connection_event table\n",
    "```sql\n",
    "CREATE TABLE measurement_connection_event (\n",
    "    solar_cell_id VARCHAR(255),\n",
    "    pixel VARCHAR(255),\n",
    "    tracking_channel_board INTEGER,\n",
    "    tracking_channel_channel INTEGER,\n",
    "    temperature_sensor_id VARCHAR(255),\n",
    "    connection_datetime TIMESTAMP WITH TIME ZONE NOT NULL,\n",
    "    FOREIGN KEY (solar_cell_id, pixel) REFERENCES solar_cell_pixel(solar_cell_id, pixel),\n",
    "    FOREIGN KEY (tracking_channel_board, tracking_channel_channel) REFERENCES mpp_tracking_channel(board, channel)\n",
    ");\n",
    "```\n",
    "- Maps connection events\n",
    "- Fields:\n",
    "  - solar_cell_id (links to solar_cell_device.name)\n",
    "  - pixel (from 'Pixel')\n",
    "  - tracking_channel_board (from 'Board')\n",
    "  - tracking_channel_channel (from 'Channel')\n",
    "  - temperature_sensor_id (from 'Temp sensor')\n",
    "  - connection_datetime (from 'Date installed')\n",
    "\n",
    "This mapping shows we'll need to:\n",
    "1. Generate UUIDs for new scientists\n",
    "2. Handle the relationships between tables\n",
    "3. Convert dates to proper timestamp format\n",
    "4. Validate data against database constraints\n",
    "5. Ensure referential integrity across all tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d021e",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Database Upload\n",
    "\n",
    "We'll prepare the data for each table in the database schema, starting with the scientist table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Get unique scientists from both Owner and Producer columns\n",
    "scientists = pd.concat([df['Owner'].dropna(), df['Producer'].dropna()]).unique()\n",
    "\n",
    "# Create scientist dataframe\n",
    "scientist_df = pd.DataFrame({\n",
    "    'scientist_id': [uuid.uuid4() for _ in range(len(scientists))],\n",
    "    'name': scientists\n",
    "})\n",
    "\n",
    "# Create a mapping dictionary for later use\n",
    "scientist_id_map = dict(zip(scientist_df['name'], scientist_df['scientist_id']))\n",
    "\n",
    "# Display the results\n",
    "print(f\"Found {len(scientist_df)} unique scientists\")\n",
    "print(\"\\nScientist table preview:\")\n",
    "display(scientist_df)\n",
    "\n",
    "print(\"\\nValidating unique constraints:\")\n",
    "print(f\"Duplicate names: {scientist_df['name'].duplicated().sum()}\")\n",
    "print(f\"Duplicate UUIDs: {scientist_df['scientist_id'].duplicated().sum()}\")\n",
    "\n",
    "# Store the mapping for later use\n",
    "print(\"\\nScientist ID mapping (first few entries):\")\n",
    "for name, id in list(scientist_id_map.items())[:5]:\n",
    "    print(f\"{name}: {id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00252df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up scientist data\n",
    "\n",
    "# Remove NA values\n",
    "scientist_df = scientist_df.dropna()\n",
    "\n",
    "# Clean up names\n",
    "# 1. Strip whitespace\n",
    "# 2. Replace multiple spaces with single space\n",
    "# 3. Remove any trailing commas or periods\n",
    "scientist_df['name'] = scientist_df['name'].str.strip()\n",
    "scientist_df['name'] = scientist_df['name'].str.replace(r'\\s+', ' ', regex=True)\n",
    "scientist_df['name'] = scientist_df['name'].str.replace(r'[,.]$', '', regex=True)\n",
    "\n",
    "# Update the mapping dictionary\n",
    "scientist_id_map = dict(zip(scientist_df['name'], scientist_df['scientist_id']))\n",
    "\n",
    "# Display cleaned results\n",
    "print(f\"After cleaning, found {len(scientist_df)} scientists\")\n",
    "print(\"\\nCleaned scientist table:\")\n",
    "display(scientist_df)\n",
    "\n",
    "# Verify no duplicates or NA values remain\n",
    "print(\"\\nValidating cleaned data:\")\n",
    "print(f\"Null values: {scientist_df['name'].isna().sum()}\")\n",
    "print(f\"Duplicate names: {scientist_df['name'].duplicated().sum()}\")\n",
    "print(f\"Duplicate UUIDs: {scientist_df['scientist_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional cleaning: Remove <NA> strings and ensure no invalid values remain\n",
    "print(\"Checking for '<NA>' values...\")\n",
    "\n",
    "# Check for '<NA>' strings\n",
    "na_mask = scientist_df['name'].isin(['<NA>', 'NA', '<na>', 'na'])\n",
    "if na_mask.any():\n",
    "    print(f\"Found {na_mask.sum()} '<NA>' values. Removing them...\")\n",
    "    scientist_df = scientist_df[~na_mask]\n",
    "\n",
    "# Update the mapping dictionary again\n",
    "scientist_id_map = dict(zip(scientist_df['name'], scientist_df['scientist_id']))\n",
    "\n",
    "# Validate final results\n",
    "print(\"\\nFinal validation:\")\n",
    "print(f\"Total scientists after removing <NA>: {len(scientist_df)}\")\n",
    "print(f\"Any remaining NA values: {scientist_df['name'].isna().any()}\")\n",
    "print(f\"Any remaining <NA> strings: {scientist_df['name'].isin(['<NA>', 'NA', '<na>', 'na']).any()}\")\n",
    "\n",
    "# Display final cleaned data\n",
    "print(\"\\nFinal cleaned scientist table:\")\n",
    "display(scientist_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fddf3ac",
   "metadata": {},
   "source": [
    "### Prepare Solar Cell Device Table\n",
    "\n",
    "The solar_cell_device table requires the following fields:\n",
    "- name (from 'Cell name', serves as primary key)\n",
    "- nomad_id (empty, will be filled later, unique identifier)\n",
    "- technology (from 'Cell type')\n",
    "- area (from 'Area')\n",
    "- initial_pce (from 'Init.PCE')\n",
    "- date_produced (empty, will be filled later)\n",
    "- encapsulation (from 'Encap.')\n",
    "- owner_id (link to scientist table)\n",
    "- producer_id (link to scientist table)\n",
    "\n",
    "We'll prepare this data by first creating a dataframe with the required columns, then clean and validate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get unique devices based on cell name to avoid duplicates\n",
    "unique_devices = df.groupby('Cell name').agg({\n",
    "    'Cell type': 'first',  # Take first occurrence of cell type\n",
    "    'Area': 'first',       # Take first area value\n",
    "    'Init.PCE': 'first',   # Take first PCE value\n",
    "    'Encap.': 'first',     # Take first encapsulation value\n",
    "    'Owner': 'first',      # Take first owner\n",
    "    'Producer': 'first'    # Take first producer\n",
    "}).reset_index()\n",
    "\n",
    "# Create initial solar cell device dataframe\n",
    "solar_cell_device_df = pd.DataFrame({\n",
    "    'nomad_id': pd.NA,  # Will be filled later\n",
    "    'name': unique_devices['Cell name'],  # Add device name from Cell name\n",
    "    'technology': unique_devices['Cell type'],\n",
    "    'area': unique_devices['Area'],\n",
    "    'initial_pce': unique_devices['Init.PCE'],\n",
    "    'date_produced': pd.NA,  # Will be filled later\n",
    "    'encapsulation': unique_devices['Encap.'],\n",
    "    'owner_id': unique_devices['Owner'].map(scientist_id_map),\n",
    "    'producer_id': unique_devices['Producer'].map(scientist_id_map)\n",
    "})\n",
    "\n",
    "# Create a mapping dictionary for cell names to use in subsequent tables\n",
    "cell_name_map = dict(zip(unique_devices['Cell name'], range(len(unique_devices))))\n",
    "\n",
    "# Display initial state\n",
    "print(f\"Found {len(solar_cell_device_df)} unique cell devices\")\n",
    "print(\"\\nInitial solar cell device table:\")\n",
    "display(solar_cell_device_df.head(25))\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(solar_cell_device_df.isnull().sum())\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nColumn data types:\")\n",
    "print(solar_cell_device_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1211bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate solar cell device dataframe against database requirements\n",
    "\n",
    "# 1. Add missing columns required by schema\n",
    "solar_cell_device_df['form_factor'] = pd.NA  # Will need to be filled\n",
    "solar_cell_device_df['experiment_id'] = pd.NA  # Will need to be filled\n",
    "solar_cell_device_df['date_encapsulated'] = pd.NA  # Will need to be filled\n",
    "\n",
    "# 2. Check for duplicate names (would violate PRIMARY KEY constraint)\n",
    "duplicates = solar_cell_device_df['name'].duplicated()\n",
    "if duplicates.any():\n",
    "    print(\"WARNING: Found duplicate device names (would violate PRIMARY KEY constraint):\")\n",
    "    print(solar_cell_device_df[duplicates]['name'])\n",
    "\n",
    "# 3. Check for invalid characters and length in name field\n",
    "print(\"\\nValidating name field:\")\n",
    "print(f\"Max name length: {solar_cell_device_df['name'].str.len().max()} (limit 255)\")\n",
    "\n",
    "# 4. Verify non-null owner_id and producer_id values exist in scientist table\n",
    "valid_scientist_ids = set(scientist_df['scientist_id'])\n",
    "\n",
    "# Check only non-null references\n",
    "invalid_owners = solar_cell_device_df[solar_cell_device_df['owner_id'].notna()]['owner_id'].isin(valid_scientist_ids) == False\n",
    "invalid_producers = solar_cell_device_df[solar_cell_device_df['producer_id'].notna()]['producer_id'].isin(valid_scientist_ids) == False\n",
    "\n",
    "if invalid_owners.any():\n",
    "    print(\"\\nWARNING: Found invalid owner_id references (excluding NULL values):\")\n",
    "    print(solar_cell_device_df[invalid_owners][['name', 'owner_id']])\n",
    "\n",
    "if invalid_producers.any():\n",
    "    print(\"\\nWARNING: Found invalid producer_id references (excluding NULL values):\")\n",
    "    print(solar_cell_device_df[invalid_producers][['name', 'producer_id']])\n",
    "\n",
    "# 5. Check for required non-null fields (only name is required)\n",
    "print(\"\\nChecking required field (name):\")\n",
    "null_count = solar_cell_device_df['name'].isna().sum()\n",
    "if null_count > 0:\n",
    "    print(f\"WARNING: Found {null_count} missing values in required field 'name'\")\n",
    "\n",
    "# Display updated dataframe structure\n",
    "print(\"\\nUpdated dataframe structure:\")\n",
    "print(solar_cell_device_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix any issues found in validation\n",
    "\n",
    "# 1. Handle duplicate device names if any\n",
    "if duplicates.any():\n",
    "    print(\"\\nHandling duplicate device names...\")\n",
    "    # Add suffix to duplicates\n",
    "    dup_mask = solar_cell_device_df['name'].duplicated(keep='first')\n",
    "    dup_names = solar_cell_device_df.loc[dup_mask, 'name']\n",
    "    for name in dup_names:\n",
    "        matches = solar_cell_device_df['name'] == name\n",
    "        # Add numeric suffix to duplicates (e.g., device_1, device_2)\n",
    "        for i, idx in enumerate(solar_cell_device_df[matches].index[1:], 1):\n",
    "            solar_cell_device_df.loc[idx, 'name'] = f\"{name}_{i}\"\n",
    "\n",
    "# 2. Handle missing values in required fields\n",
    "print(\"\\nHandling missing required values...\")\n",
    "# Only name is required (PRIMARY KEY)\n",
    "missing_names = solar_cell_device_df['name'].isna().sum()\n",
    "if missing_names > 0:\n",
    "    raise ValueError(f\"Found {missing_names} missing values in 'name' column. This is required and cannot be null.\")\n",
    "\n",
    "# 3. Handle invalid scientist references (only for non-null values)\n",
    "if invalid_owners.any() or invalid_producers.any():\n",
    "    print(\"\\nHandling invalid scientist references...\")\n",
    "    # Create a default scientist for invalid references\n",
    "    default_scientist_id = uuid.uuid4()\n",
    "    scientist_df = pd.concat([scientist_df, pd.DataFrame({\n",
    "        'scientist_id': [default_scientist_id],\n",
    "        'name': ['Unknown Scientist']\n",
    "    })], ignore_index=True)\n",
    "    \n",
    "    # Only update invalid non-null references\n",
    "    if invalid_owners.any():\n",
    "        solar_cell_device_df.loc[invalid_owners.index, 'owner_id'] = default_scientist_id\n",
    "    if invalid_producers.any():\n",
    "        solar_cell_device_df.loc[invalid_producers.index, 'producer_id'] = default_scientist_id\n",
    "\n",
    "# Verify final state\n",
    "print(\"\\nFinal validation:\")\n",
    "print(f\"Duplicate names: {solar_cell_device_df['name'].duplicated().any()}\")\n",
    "print(f\"Missing required values (name): {solar_cell_device_df['name'].isna().sum()}\")\n",
    "print(f\"Invalid non-null owner references: {(solar_cell_device_df[solar_cell_device_df['owner_id'].notna()]['owner_id'].isin(valid_scientist_ids) == False).sum()}\")\n",
    "print(f\"Invalid non-null producer references: {(solar_cell_device_df[solar_cell_device_df['producer_id'].notna()]['producer_id'].isin(valid_scientist_ids) == False).sum()}\")\n",
    "\n",
    "# Display the cleaned data\n",
    "print(\"\\nCleaned solar cell device table:\")\n",
    "display(solar_cell_device_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022f2ad",
   "metadata": {},
   "source": [
    "### Prepare Solar Cell Pixel Table\n",
    "\n",
    "The solar_cell_pixel table requires:\n",
    "- solar_cell_id (foreign key to solar_cell_device.name)\n",
    "- pixel (pixel number/identifier)\n",
    "- active_area (area measurement for specific pixel)\n",
    "\n",
    "Important considerations:\n",
    "- The combination of solar_cell_id and pixel forms the PRIMARY KEY\n",
    "- Different solar cells can have pixels with the same names (e.g., 'a', 'b', 'c')\n",
    "- We need to preserve all pixel entries while ensuring uniqueness of the composite key\n",
    "\n",
    "Let's extract and validate this data from our logbook entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create solar cell pixel dataframe from original logbook data\n",
    "solar_cell_pixel_df = df[['Cell name', 'Pixel', 'Area']].copy()\n",
    "\n",
    "# Clean pixel data\n",
    "solar_cell_pixel_df['Pixel'] = solar_cell_pixel_df['Pixel'].astype(str)\n",
    "solar_cell_pixel_df['Pixel'] = solar_cell_pixel_df['Pixel'].str.strip()\n",
    "\n",
    "# Remove rows where pixel is missing or invalid\n",
    "solar_cell_pixel_df = solar_cell_pixel_df.dropna(subset=['Pixel'])\n",
    "solar_cell_pixel_df = solar_cell_pixel_df[solar_cell_pixel_df['Pixel'].str.lower() != 'nan']\n",
    "\n",
    "# Ensure area is a float\n",
    "solar_cell_pixel_df['active_area'] = pd.to_numeric(solar_cell_pixel_df['Area'], errors='coerce')\n",
    "\n",
    "# Get unique combinations of cell and pixel\n",
    "# Note: We don't deduplicate pixels alone as they can repeat across different cells\n",
    "solar_cell_pixel_df = solar_cell_pixel_df.drop_duplicates(subset=['Cell name', 'Pixel'])\n",
    "\n",
    "# Display initial state\n",
    "print(f\"Found {len(solar_cell_pixel_df)} unique cell-pixel combinations\")\n",
    "print(f\"Number of unique cells: {solar_cell_pixel_df['Cell name'].nunique()}\")\n",
    "print(f\"Number of unique pixel names: {solar_cell_pixel_df['Pixel'].nunique()}\")\n",
    "\n",
    "# Show distribution of pixel names across cells\n",
    "pixel_counts = solar_cell_pixel_df.groupby('Pixel').size()\n",
    "print(\"\\nPixel name frequency (showing most common):\")\n",
    "display(pixel_counts.sort_values(ascending=False).head())\n",
    "\n",
    "# Display initial data\n",
    "print(\"\\nInitial solar cell pixel table:\")\n",
    "display(solar_cell_pixel_df.head(25))\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(solar_cell_pixel_df.isnull().sum())\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nColumn data types:\")\n",
    "print(solar_cell_pixel_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5580bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and clean pixel data\n",
    "\n",
    "# 1. Check for invalid pixel values\n",
    "print(\"Unique pixel values:\")\n",
    "print(solar_cell_pixel_df['Pixel'].unique())\n",
    "\n",
    "# 2. Verify all cell names exist in solar_cell_device_df\n",
    "invalid_cells = ~solar_cell_pixel_df['Cell name'].isin(solar_cell_device_df['name'])\n",
    "if invalid_cells.any():\n",
    "    print(f\"\\nWARNING: Found {invalid_cells.sum()} pixels with invalid cell references\")\n",
    "    print(\"Invalid cell names:\")\n",
    "    print(solar_cell_pixel_df[invalid_cells]['Cell name'].unique())\n",
    "    \n",
    "    # Remove invalid entries\n",
    "    solar_cell_pixel_df = solar_cell_pixel_df[~invalid_cells]\n",
    "\n",
    "# 3. Check for negative or zero areas\n",
    "invalid_areas = solar_cell_pixel_df['active_area'] <= 0\n",
    "if invalid_areas.any():\n",
    "    print(f\"\\nWARNING: Found {invalid_areas.sum()} pixels with invalid areas\")\n",
    "    print(\"Entries with invalid areas:\")\n",
    "    display(solar_cell_pixel_df[invalid_areas])\n",
    "    \n",
    "    # Set invalid areas to NULL\n",
    "    solar_cell_pixel_df.loc[invalid_areas, 'active_area'] = pd.NA\n",
    "\n",
    "# Display cleaned data\n",
    "print(\"\\nCleaned solar cell pixel table:\")\n",
    "display(solar_cell_pixel_df.head(25))\n",
    "\n",
    "# Final validation counts\n",
    "print(f\"\\nFinal validation:\")\n",
    "print(f\"Total unique pixels: {len(solar_cell_pixel_df)}\")\n",
    "print(f\"Unique cells: {solar_cell_pixel_df['Cell name'].nunique()}\")\n",
    "print(f\"Pixels without area: {solar_cell_pixel_df['active_area'].isna().sum()}\")\n",
    "print(f\"Invalid cell references: {(~solar_cell_pixel_df['Cell name'].isin(solar_cell_device_df['name'])).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aecb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final table structure for database upload\n",
    "solar_cell_pixel_upload_df = pd.DataFrame({\n",
    "    'solar_cell_id': solar_cell_pixel_df['Cell name'].astype('string'),  # VARCHAR(255)\n",
    "    'pixel': solar_cell_pixel_df['Pixel'].astype('string'),              # VARCHAR(255)\n",
    "    'active_area': solar_cell_pixel_df['active_area'].astype('float64')  # DOUBLE PRECISION\n",
    "})\n",
    "\n",
    "# Validate data types match database schema\n",
    "print(\"Data type validation:\")\n",
    "print(solar_cell_pixel_upload_df.dtypes)\n",
    "\n",
    "# Validate string lengths (VARCHAR(255) limit)\n",
    "max_solar_cell_id_len = solar_cell_pixel_upload_df['solar_cell_id'].str.len().max()\n",
    "max_pixel_len = solar_cell_pixel_upload_df['pixel'].str.len().max()\n",
    "print(f\"\\nVARCHAR length validation:\")\n",
    "print(f\"solar_cell_id max length: {max_solar_cell_id_len}/255\")\n",
    "print(f\"pixel max length: {max_pixel_len}/255\")\n",
    "\n",
    "# Ensure the combined solar_cell_id and pixel form a unique identifier\n",
    "duplicates = solar_cell_pixel_upload_df.duplicated(subset=['solar_cell_id', 'pixel'], keep=False)\n",
    "if duplicates.any():\n",
    "    print(\"\\nWARNING: Found duplicate pixel entries:\")\n",
    "    display(solar_cell_pixel_upload_df[duplicates].sort_values(['solar_cell_id', 'pixel']))\n",
    "    \n",
    "    # Keep the first occurrence of each pixel per cell\n",
    "    solar_cell_pixel_upload_df = solar_cell_pixel_upload_df.drop_duplicates(\n",
    "        subset=['solar_cell_id', 'pixel'],\n",
    "        keep='first'\n",
    "    )\n",
    "\n",
    "# Validate numeric ranges\n",
    "print(\"\\nNumeric range validation:\")\n",
    "print(\"active_area range:\")\n",
    "print(f\"min: {solar_cell_pixel_upload_df['active_area'].min()}\")\n",
    "print(f\"max: {solar_cell_pixel_upload_df['active_area'].max()}\")\n",
    "\n",
    "# Final validation\n",
    "print(\"\\nSchema validation:\")\n",
    "print(f\"Primary key uniqueness: {not solar_cell_pixel_upload_df.duplicated(subset=['solar_cell_id', 'pixel']).any()}\")\n",
    "print(f\"Foreign key validation: {solar_cell_pixel_upload_df['solar_cell_id'].isin(solar_cell_device_df['name']).all()}\")\n",
    "print(f\"Total rows: {len(solar_cell_pixel_upload_df)}\")\n",
    "\n",
    "# Null checks\n",
    "print(\"\\nNull value checks:\")\n",
    "print(solar_cell_pixel_upload_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796b28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00d20043",
   "metadata": {},
   "source": [
    "### Prepare MPP Tracking Channel Table\n",
    "\n",
    "The mpp_tracking_channel table requires:\n",
    "- board (INTEGER)\n",
    "- channel (INTEGER)\n",
    "\n",
    "Important considerations:\n",
    "- The combination of board and channel forms the PRIMARY KEY\n",
    "- Both fields are required (no NULL values allowed)\n",
    "- Values must be valid integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b04f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MPP tracking channel dataframe from original logbook data\n",
    "mpp_tracking_channel_df = df[['Board', 'Channel']].copy()\n",
    "\n",
    "# Drop any rows where either Board or Channel is missing\n",
    "mpp_tracking_channel_df = mpp_tracking_channel_df.dropna()\n",
    "\n",
    "# Convert to integers\n",
    "mpp_tracking_channel_df['board'] = mpp_tracking_channel_df['Board'].astype('int64')\n",
    "mpp_tracking_channel_df['channel'] = mpp_tracking_channel_df['Channel'].astype('int64')\n",
    "\n",
    "# Drop the original columns\n",
    "mpp_tracking_channel_df = mpp_tracking_channel_df[['board', 'channel']]\n",
    "\n",
    "# Remove duplicates\n",
    "mpp_tracking_channel_df = mpp_tracking_channel_df.drop_duplicates()\n",
    "\n",
    "# Display initial state\n",
    "print(f\"Found {len(mpp_tracking_channel_df)} unique board-channel combinations\")\n",
    "print(f\"Number of unique boards: {mpp_tracking_channel_df['board'].nunique()}\")\n",
    "print(f\"Number of unique channels: {mpp_tracking_channel_df['channel'].nunique()}\")\n",
    "\n",
    "# Show distribution of channels across boards\n",
    "channel_counts = mpp_tracking_channel_df.groupby('board')['channel'].nunique()\n",
    "print(\"\\nChannels per board:\")\n",
    "display(channel_counts)\n",
    "\n",
    "# Validate data\n",
    "print(\"\\nValidation:\")\n",
    "print(f\"Primary key uniqueness: {not mpp_tracking_channel_df.duplicated().any()}\")\n",
    "print(f\"Negative board values: {(mpp_tracking_channel_df['board'] < 0).sum()}\")\n",
    "print(f\"Negative channel values: {(mpp_tracking_channel_df['channel'] < 0).sum()}\")\n",
    "\n",
    "# Display the prepared data\n",
    "print(\"\\nPrepared MPP tracking channel table:\")\n",
    "display(mpp_tracking_channel_df.sort_values(['board', 'channel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0035ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
