{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7e94ca",
   "metadata": {},
   "source": [
    "# Perocube Logbook Data Upload Notebook\n",
    "\n",
    "This notebook processes and uploads logbook data from the Perocube Excel file to the TimescaleDB database.\n",
    "\n",
    "## Purpose\n",
    "- Read logbook data from 'PeroCube_logbook_example.xlsx'\n",
    "- Parse and validate the data\n",
    "- Upload the data to the TimescaleDB database\n",
    "- Avoid duplicate data entries\n",
    "\n",
    "## Prerequisites\n",
    "- Running TimescaleDB instance (configured in docker-compose.yml)\n",
    "- Access to the Perocube logbook Excel file\n",
    "- Environment variables configured in .env file (for database connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e0476",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import required libraries and install any missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# Database libraries\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af362b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install psycopg2-binary sqlalchemy pandas tqdm pathlib python-dotenv openpyxl\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41d919",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load configuration from environment variables and set up constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for the .env file two directories up from the notebook location\n",
    "dotenv_path = Path(\"../../.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Database configuration from environment variables with fallbacks\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'port': int(os.getenv('DB_PORT', 5432)),\n",
    "    'database': os.getenv('DB_NAME', 'perocube'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'postgres')\n",
    "}\n",
    "\n",
    "# Print database connection info (excluding password)\n",
    "print(f\"Database connection: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']} as {DB_CONFIG['user']}\")\n",
    "\n",
    "# Data directory and file configuration\n",
    "ROOT_DIRECTORY = os.getenv('DEFAULT_DATA_DIR', \"../../sample_data/datasets/PeroCube-sample-data\")\n",
    "LOGBOOK_FILE = \"PeroCube_logbook_example.xlsx\"\n",
    "LOGBOOK_SHEET = \"Perocube history\"\n",
    "\n",
    "# Batch size for database operations\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd22ffb",
   "metadata": {},
   "source": [
    "## 3. Read and Process Logbook Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full path to the logbook file\n",
    "logbook_path = Path(ROOT_DIRECTORY) / LOGBOOK_FILE\n",
    "\n",
    "# Read the Excel sheet, skip first row and use second row as header\n",
    "try:\n",
    "    df = pd.read_excel(logbook_path, sheet_name=LOGBOOK_SHEET, header=1)\n",
    "    print(f\"Successfully read {len(df)} rows from {LOGBOOK_FILE}\")\n",
    "    \n",
    "    # Display the first few rows and data info\n",
    "    print(\"\\nColumn names:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    print(\"\\nFirst few rows of the data:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nDataset information:\")\n",
    "    display(df.info())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading Excel file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6234a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for invalid date entries ('??') and missing cell names\n",
    "\n",
    "# 1. Check for invalid date entries\n",
    "invalid_dates = df['Date removed'].astype(str).str.contains(r'\\?')\n",
    "if invalid_dates.any():\n",
    "    print(f\"Found {invalid_dates.sum()} rows with invalid removal dates ('??')\")\n",
    "    print(\"\\nSample of rows with invalid dates:\")\n",
    "    display(df[invalid_dates][['Cell name', 'Pixel', 'Date removed', 'Date installed']])\n",
    "\n",
    "# 2. Check for missing cell names\n",
    "missing_cell_names = df['Cell name'].isna() | (df['Cell name'].astype(str).str.strip() == '')\n",
    "if missing_cell_names.any():\n",
    "    print(f\"\\nFound {missing_cell_names.sum()} rows with missing cell names\")\n",
    "    print(\"\\nSample of rows with missing cell names:\")\n",
    "    display(df[missing_cell_names][['Cell name', 'Pixel', 'Date removed', 'Date installed']])\n",
    "\n",
    "# Remove rows with either invalid dates or missing cell names\n",
    "df = df[~(invalid_dates | missing_cell_names)]\n",
    "print(f\"\\nRemaining rows after removal: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c36429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for invalid date entries ('??') before date conversion\n",
    "invalid_dates = df['Date removed'].astype(str).str.contains(r'\\?', regex=True)\n",
    "if invalid_dates.any():\n",
    "    print(f\"Found {invalid_dates.sum()} rows with invalid removal dates ('??')\")\n",
    "    print(\"\\nSample of rows to be removed:\")\n",
    "    display(df[invalid_dates][['Cell name', 'Pixel', 'Date removed', 'Date installed']])\n",
    "    \n",
    "    # Remove these rows\n",
    "    df = df[~invalid_dates]\n",
    "    print(f\"\\nRemaining rows after removal: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze current dataframe state\n",
    "print(\"Checking for unnamed columns:\")\n",
    "unnamed_cols = [col for col in df.columns if 'Unnamed' in str(col)]\n",
    "print(f\"Unnamed columns found: {unnamed_cols}\")\n",
    "\n",
    "print(\"\\nCurrent data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nTotal rows with all missing values:\")\n",
    "print(df.isna().all(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframe\n",
    "\n",
    "# 1. Remove unnamed columns\n",
    "df = df.loc[:, ~df.columns.str.contains('Unnamed')]\n",
    "\n",
    "# 2. Drop rows where all values are missing\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "# Print cleaning results\n",
    "print(\"Dataframe shape after cleaning:\")\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "# Display updated column list\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display updated missing values count\n",
    "print(\"\\nMissing values per column after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display first few rows of cleaned data\n",
    "print(\"\\nFirst few rows of cleaned data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e47291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove completely empty Comment 2 column\n",
    "df = df.drop('Comment 2', axis=1)\n",
    "\n",
    "# Print updated dataframe info\n",
    "print(\"Dataframe shape after removing Comment 2:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# Display updated column list\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display first few rows of cleaned data\n",
    "print(\"\\nFirst few rows of cleaned data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee62e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data types\n",
    "\n",
    "# 1. Convert date columns to datetime\n",
    "df['Date removed'] = pd.to_datetime(df['Date removed'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Date installed'] = pd.to_datetime(df['Date installed'], format='%d.%m.%Y', errors='coerce')\n",
    "\n",
    "# 2. Convert numeric columns\n",
    "numeric_columns = ['Board', 'Channel', 'Status', 'Area', 'Init.PCE']\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 3. Ensure string columns are properly formatted (strip whitespace)\n",
    "string_columns = ['Cell type', 'Cell name', 'Pixel', 'Encap.', 'Structure', \n",
    "                  'Producer', 'Owner', 'Project', 'Temp sensor', 'Comment 1']\n",
    "for col in string_columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    # Replace 'nan' strings with actual NaN\n",
    "    df[col] = df[col].replace('nan', pd.NA)\n",
    "\n",
    "# Display the updated data types\n",
    "print(\"Updated data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display sample of the data to verify conversions\n",
    "print(\"\\nSample of converted data:\")\n",
    "display(df.head(25))\n",
    "\n",
    "# Check for any conversion issues (invalid dates or numbers)\n",
    "print(\"\\nCount of NaN values after type conversion:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dcff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where both dates are null\n",
    "original_rows = len(df)\n",
    "df = df.dropna(subset=['Date removed', 'Date installed'], how='all')\n",
    "\n",
    "# Print results\n",
    "print(f\"Removed {original_rows - len(df)} rows where both dates were missing\")\n",
    "print(f\"Remaining rows: {len(df)}\")\n",
    "\n",
    "# Display missing values count after removal\n",
    "print(\"\\nMissing values per column after removing rows with missing dates:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Display sample of remaining data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rows where installation date is missing\n",
    "missing_install_date = df[df['Date installed'].isna()]\n",
    "\n",
    "print(f\"Found {len(missing_install_date)} rows with missing installation date:\\n\")\n",
    "display(missing_install_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing installation dates with January 1st, 2020\n",
    "default_install_date = pd.to_datetime('2020-01-01')\n",
    "df['Date installed'] = df['Date installed'].fillna(default_install_date)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Checking for any remaining missing installation dates:\")\n",
    "print(f\"Missing installation dates: {df['Date installed'].isna().sum()}\")\n",
    "\n",
    "# Display the rows that were updated\n",
    "print(\"\\nVerifying the rows that were updated:\")\n",
    "display(df[df['Date installed'] == default_install_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Board and Channel to integers (as per database schema)\n",
    "df['Board'] = df['Board'].astype('Int64')  # Using Int64 to handle NaN values\n",
    "df['Channel'] = df['Channel'].astype('Int64')  # Using Int64 to handle NaN values\n",
    "\n",
    "# Verify the conversion\n",
    "print(\"Updated data types for Board and Channel:\")\n",
    "print(df[['Board', 'Channel']].dtypes)\n",
    "\n",
    "# Display a sample to verify the conversion\n",
    "print(\"\\nSample of Board and Channel data:\")\n",
    "display(df[['Board', 'Channel']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee39dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align numeric types with database schema\n",
    "df['Area'] = df['Area'].astype('float64')\n",
    "df['Init.PCE'] = df['Init.PCE'].astype('float64')\n",
    "\n",
    "# Validate and truncate string columns to match VARCHAR(255)\n",
    "string_columns = ['Cell type', 'Cell name', 'Pixel', 'Encap.', 'Structure', \n",
    "                 'Producer', 'Owner', 'Project', 'Temp sensor', 'Comment 1']\n",
    "\n",
    "for col in string_columns:\n",
    "    # Check if any string is longer than 255 characters\n",
    "    mask = df[col].str.len() > 255\n",
    "    if mask.any():\n",
    "        print(f\"Warning: Found {mask.sum()} values in {col} longer than 255 characters. Truncating...\")\n",
    "        df.loc[mask, col] = df.loc[mask, col].str.slice(0, 255)\n",
    "\n",
    "# Display updated data types\n",
    "print(\"\\nUpdated data types after database alignment:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for any values that might be too long\n",
    "print(\"\\nMaximum string lengths:\")\n",
    "for col in string_columns:\n",
    "    max_len = df[col].str.len().max()\n",
    "    print(f\"{col}: {max_len}\")\n",
    "\n",
    "# Display sample of numeric columns\n",
    "print(\"\\nSample of numeric columns:\")\n",
    "display(df[['Area', 'Init.PCE']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7d277",
   "metadata": {},
   "source": [
    "## 4. Database Mapping Reference\n",
    "\n",
    "Based on the database schema in `tables.sql`, our logbook data maps to the following tables:\n",
    "\n",
    "### 1. scientist table\n",
    "```sql\n",
    "CREATE TABLE scientist (\n",
    "    scientist_id UUID PRIMARY KEY,\n",
    "    name VARCHAR(255) NOT NULL\n",
    ");\n",
    "```\n",
    "- Maps from: 'Owner', 'Producer' columns\n",
    "- Fields:\n",
    "  - scientist_id (UUID, generated)\n",
    "  - name (from 'Owner' and 'Producer')\n",
    "\n",
    "### 2. solar_cell_device table\n",
    "```sql\n",
    "CREATE TABLE solar_cell_device (\n",
    "    name VARCHAR(255) PRIMARY KEY,\n",
    "    nomad_id UUID UNIQUE,\n",
    "    technology VARCHAR(255),\n",
    "    area DOUBLE PRECISION,\n",
    "    initial_pce DOUBLE PRECISION,\n",
    "    date_produced TIMESTAMP WITH TIME ZONE,\n",
    "    form_factor VARCHAR(255),\n",
    "    encapsulation VARCHAR(255),\n",
    "    experiment_id UUID,\n",
    "    date_encapsulated TIMESTAMP WITH TIME ZONE,\n",
    "    owner_id UUID REFERENCES scientist(scientist_id),\n",
    "    producer_id UUID REFERENCES scientist(scientist_id)\n",
    ");\n",
    "```\n",
    "- Maps from multiple columns\n",
    "- Fields:\n",
    "  - name (from 'Cell name')\n",
    "  - nomad_id (UUID, generated)\n",
    "  - technology (from 'Cell type')\n",
    "  - area (from 'Area')\n",
    "  - initial_pce (from 'Init.PCE')\n",
    "  - date_produced (can use 'Date installed')\n",
    "  - form_factor (not in logbook)\n",
    "  - encapsulation (from 'Encap.')\n",
    "  - experiment_id (not in logbook)\n",
    "  - date_encapsulated (not in logbook)\n",
    "  - owner_id (link to scientist table)\n",
    "  - producer_id (link to scientist table)\n",
    "\n",
    "### 3. solar_cell_pixel table\n",
    "```sql\n",
    "CREATE TABLE solar_cell_pixel (\n",
    "    solar_cell_id VARCHAR(255) REFERENCES solar_cell_device(name),\n",
    "    pixel VARCHAR(255),\n",
    "    active_area DOUBLE PRECISION,\n",
    "    PRIMARY KEY (solar_cell_id, pixel)\n",
    ");\n",
    "```\n",
    "- Maps from pixel-specific data\n",
    "- Fields:\n",
    "  - solar_cell_id (foreign key to solar_cell_device.name)\n",
    "  - pixel (from 'Pixel' column)\n",
    "  - active_area (from 'Area')\n",
    "\n",
    "### 4. mpp_tracking_channel table\n",
    "```sql\n",
    "CREATE TABLE mpp_tracking_channel (\n",
    "    board INTEGER,\n",
    "    channel INTEGER,\n",
    "    PRIMARY KEY (board, channel)\n",
    ");\n",
    "```\n",
    "- Maps from channel data\n",
    "- Fields:\n",
    "  - board (from 'Board')\n",
    "  - channel (from 'Channel')\n",
    "\n",
    "### 5. measurement_connection_event table\n",
    "```sql\n",
    "CREATE TABLE measurement_connection_event (\n",
    "    solar_cell_id VARCHAR(255),\n",
    "    pixel VARCHAR(255),\n",
    "    tracking_channel_board INTEGER,\n",
    "    tracking_channel_channel INTEGER,\n",
    "    temperature_sensor_id VARCHAR(255),\n",
    "    connection_datetime TIMESTAMP WITH TIME ZONE NOT NULL,\n",
    "    FOREIGN KEY (solar_cell_id, pixel) REFERENCES solar_cell_pixel(solar_cell_id, pixel),\n",
    "    FOREIGN KEY (tracking_channel_board, tracking_channel_channel) REFERENCES mpp_tracking_channel(board, channel)\n",
    ");\n",
    "```\n",
    "- Maps connection events\n",
    "- Fields:\n",
    "  - solar_cell_id (links to solar_cell_device.name)\n",
    "  - pixel (from 'Pixel')\n",
    "  - tracking_channel_board (from 'Board')\n",
    "  - tracking_channel_channel (from 'Channel')\n",
    "  - temperature_sensor_id (from 'Temp sensor')\n",
    "  - connection_datetime (from 'Date installed')\n",
    "\n",
    "This mapping shows we'll need to:\n",
    "1. Generate UUIDs for new scientists\n",
    "2. Handle the relationships between tables\n",
    "3. Convert dates to proper timestamp format\n",
    "4. Validate data against database constraints\n",
    "5. Ensure referential integrity across all tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d021e",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Database Upload\n",
    "\n",
    "We'll prepare the data for each table in the database schema, starting with the scientist table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Get unique scientists from both Owner and Producer columns\n",
    "scientists = pd.concat([df['Owner'].dropna(), df['Producer'].dropna()]).unique()\n",
    "\n",
    "# Create scientist dataframe\n",
    "scientist_df = pd.DataFrame({\n",
    "    'scientist_id': [uuid.uuid4() for _ in range(len(scientists))],\n",
    "    'name': scientists\n",
    "})\n",
    "\n",
    "# Create a mapping dictionary for later use\n",
    "scientist_id_map = dict(zip(scientist_df['name'], scientist_df['scientist_id']))\n",
    "\n",
    "# Display the results\n",
    "print(f\"Found {len(scientist_df)} unique scientists\")\n",
    "print(\"\\nScientist table preview:\")\n",
    "display(scientist_df)\n",
    "\n",
    "print(\"\\nValidating unique constraints:\")\n",
    "print(f\"Duplicate names: {scientist_df['name'].duplicated().sum()}\")\n",
    "print(f\"Duplicate UUIDs: {scientist_df['scientist_id'].duplicated().sum()}\")\n",
    "\n",
    "# Store the mapping for later use\n",
    "print(\"\\nScientist ID mapping (first few entries):\")\n",
    "for name, id in list(scientist_id_map.items())[:5]:\n",
    "    print(f\"{name}: {id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00252df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up scientist data\n",
    "\n",
    "# Remove NA values\n",
    "scientist_df = scientist_df.dropna()\n",
    "\n",
    "# Clean up names\n",
    "# 1. Strip whitespace\n",
    "# 2. Replace multiple spaces with single space\n",
    "# 3. Remove any trailing commas or periods\n",
    "scientist_df['name'] = scientist_df['name'].str.strip()\n",
    "scientist_df['name'] = scientist_df['name'].str.replace(r'\\s+', ' ', regex=True)\n",
    "scientist_df['name'] = scientist_df['name'].str.replace(r'[,.]$', '', regex=True)\n",
    "\n",
    "# Update the mapping dictionary\n",
    "scientist_id_map = dict(zip(scientist_df['name'], scientist_df['scientist_id']))\n",
    "\n",
    "# Display cleaned results\n",
    "print(f\"After cleaning, found {len(scientist_df)} scientists\")\n",
    "print(\"\\nCleaned scientist table:\")\n",
    "display(scientist_df)\n",
    "\n",
    "# Verify no duplicates or NA values remain\n",
    "print(\"\\nValidating cleaned data:\")\n",
    "print(f\"Null values: {scientist_df['name'].isna().sum()}\")\n",
    "print(f\"Duplicate names: {scientist_df['name'].duplicated().sum()}\")\n",
    "print(f\"Duplicate UUIDs: {scientist_df['scientist_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional cleaning: Remove <NA> strings and ensure no invalid values remain\n",
    "print(\"Checking for '<NA>' values...\")\n",
    "\n",
    "# Check for '<NA>' strings\n",
    "na_mask = scientist_df['name'].isin(['<NA>', 'NA', '<na>', 'na'])\n",
    "if na_mask.any():\n",
    "    print(f\"Found {na_mask.sum()} '<NA>' values. Removing them...\")\n",
    "    scientist_df = scientist_df[~na_mask]\n",
    "\n",
    "# Update the mapping dictionary again\n",
    "scientist_id_map = dict(zip(scientist_df['name'], scientist_df['scientist_id']))\n",
    "\n",
    "# Validate final results\n",
    "print(\"\\nFinal validation:\")\n",
    "print(f\"Total scientists after removing <NA>: {len(scientist_df)}\")\n",
    "print(f\"Any remaining NA values: {scientist_df['name'].isna().any()}\")\n",
    "print(f\"Any remaining <NA> strings: {scientist_df['name'].isin(['<NA>', 'NA', '<na>', 'na']).any()}\")\n",
    "\n",
    "# Display final cleaned data\n",
    "print(\"\\nFinal cleaned scientist table:\")\n",
    "display(scientist_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fddf3ac",
   "metadata": {},
   "source": [
    "### Prepare Solar Cell Device Table\n",
    "\n",
    "The solar_cell_device table requires the following fields:\n",
    "- name (from 'Cell name', serves as primary key)\n",
    "- nomad_id (empty, will be filled later, unique identifier)\n",
    "- technology (from 'Cell type')\n",
    "- area (from 'Area')\n",
    "- initial_pce (from 'Init.PCE')\n",
    "- date_produced (empty, will be filled later)\n",
    "- encapsulation (from 'Encap.')\n",
    "- owner_id (link to scientist table)\n",
    "- producer_id (link to scientist table)\n",
    "\n",
    "We'll prepare this data by first creating a dataframe with the required columns, then clean and validate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get unique devices based on cell name to avoid duplicates\n",
    "unique_devices = df.groupby('Cell name').agg({\n",
    "    'Cell type': 'first',  # Take first occurrence of cell type\n",
    "    'Area': 'first',       # Take first area value\n",
    "    'Init.PCE': 'first',   # Take first PCE value\n",
    "    'Encap.': 'first',     # Take first encapsulation value\n",
    "    'Owner': 'first',      # Take first owner\n",
    "    'Producer': 'first'    # Take first producer\n",
    "}).reset_index()\n",
    "\n",
    "# Create initial solar cell device dataframe\n",
    "solar_cell_device_df = pd.DataFrame({\n",
    "    'nomad_id': pd.NA,  # Will be filled later\n",
    "    'name': unique_devices['Cell name'],  # Add device name from Cell name\n",
    "    'technology': unique_devices['Cell type'],\n",
    "    'area': unique_devices['Area'],\n",
    "    'initial_pce': unique_devices['Init.PCE'],\n",
    "    'date_produced': pd.NA,  # Will be filled later\n",
    "    'encapsulation': unique_devices['Encap.'],\n",
    "    'owner_id': unique_devices['Owner'].map(scientist_id_map),\n",
    "    'producer_id': unique_devices['Producer'].map(scientist_id_map)\n",
    "})\n",
    "\n",
    "# Create a mapping dictionary for cell names to use in subsequent tables\n",
    "cell_name_map = dict(zip(unique_devices['Cell name'], range(len(unique_devices))))\n",
    "\n",
    "# Display initial state\n",
    "print(f\"Found {len(solar_cell_device_df)} unique cell devices\")\n",
    "print(\"\\nInitial solar cell device table:\")\n",
    "display(solar_cell_device_df.head(25))\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(solar_cell_device_df.isnull().sum())\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nColumn data types:\")\n",
    "print(solar_cell_device_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1211bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate solar cell device dataframe against database requirements\n",
    "\n",
    "# 1. Add missing columns required by schema\n",
    "solar_cell_device_df['form_factor'] = pd.NA  # Will need to be filled\n",
    "solar_cell_device_df['experiment_id'] = pd.NA  # Will need to be filled\n",
    "solar_cell_device_df['date_encapsulated'] = pd.NA  # Will need to be filled\n",
    "\n",
    "# 2. Check for duplicate names (would violate PRIMARY KEY constraint)\n",
    "duplicates = solar_cell_device_df['name'].duplicated()\n",
    "if duplicates.any():\n",
    "    print(\"WARNING: Found duplicate device names (would violate PRIMARY KEY constraint):\")\n",
    "    print(solar_cell_device_df[duplicates]['name'])\n",
    "\n",
    "# 3. Check for invalid characters and length in name field\n",
    "print(\"\\nValidating name field:\")\n",
    "print(f\"Max name length: {solar_cell_device_df['name'].str.len().max()} (limit 255)\")\n",
    "\n",
    "# 4. Verify non-null owner_id and producer_id values exist in scientist table\n",
    "valid_scientist_ids = set(scientist_df['scientist_id'])\n",
    "\n",
    "# Check only non-null references\n",
    "invalid_owners = solar_cell_device_df[solar_cell_device_df['owner_id'].notna()]['owner_id'].isin(valid_scientist_ids) == False\n",
    "invalid_producers = solar_cell_device_df[solar_cell_device_df['producer_id'].notna()]['producer_id'].isin(valid_scientist_ids) == False\n",
    "\n",
    "if invalid_owners.any():\n",
    "    print(\"\\nWARNING: Found invalid owner_id references (excluding NULL values):\")\n",
    "    print(solar_cell_device_df[invalid_owners][['name', 'owner_id']])\n",
    "\n",
    "if invalid_producers.any():\n",
    "    print(\"\\nWARNING: Found invalid producer_id references (excluding NULL values):\")\n",
    "    print(solar_cell_device_df[invalid_producers][['name', 'producer_id']])\n",
    "\n",
    "# 5. Check for required non-null fields (only name is required)\n",
    "print(\"\\nChecking required field (name):\")\n",
    "null_count = solar_cell_device_df['name'].isna().sum()\n",
    "if null_count > 0:\n",
    "    print(f\"WARNING: Found {null_count} missing values in required field 'name'\")\n",
    "\n",
    "# Display updated dataframe structure\n",
    "print(\"\\nUpdated dataframe structure:\")\n",
    "print(solar_cell_device_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix any issues found in validation\n",
    "\n",
    "# 1. Handle duplicate device names if any\n",
    "if duplicates.any():\n",
    "    print(\"\\nHandling duplicate device names...\")\n",
    "    # Add suffix to duplicates\n",
    "    dup_mask = solar_cell_device_df['name'].duplicated(keep='first')\n",
    "    dup_names = solar_cell_device_df.loc[dup_mask, 'name']\n",
    "    for name in dup_names:\n",
    "        matches = solar_cell_device_df['name'] == name\n",
    "        # Add numeric suffix to duplicates (e.g., device_1, device_2)\n",
    "        for i, idx in enumerate(solar_cell_device_df[matches].index[1:], 1):\n",
    "            solar_cell_device_df.loc[idx, 'name'] = f\"{name}_{i}\"\n",
    "\n",
    "# 2. Handle missing values in required fields\n",
    "print(\"\\nHandling missing required values...\")\n",
    "# Only name is required (PRIMARY KEY)\n",
    "missing_names = solar_cell_device_df['name'].isna().sum()\n",
    "if missing_names > 0:\n",
    "    raise ValueError(f\"Found {missing_names} missing values in 'name' column. This is required and cannot be null.\")\n",
    "\n",
    "# 3. Handle invalid scientist references (only for non-null values)\n",
    "if invalid_owners.any() or invalid_producers.any():\n",
    "    print(\"\\nHandling invalid scientist references...\")\n",
    "    # Create a default scientist for invalid references\n",
    "    default_scientist_id = uuid.uuid4()\n",
    "    scientist_df = pd.concat([scientist_df, pd.DataFrame({\n",
    "        'scientist_id': [default_scientist_id],\n",
    "        'name': ['Unknown Scientist']\n",
    "    })], ignore_index=True)\n",
    "    \n",
    "    # Only update invalid non-null references\n",
    "    if invalid_owners.any():\n",
    "        solar_cell_device_df.loc[invalid_owners.index, 'owner_id'] = default_scientist_id\n",
    "    if invalid_producers.any():\n",
    "        solar_cell_device_df.loc[invalid_producers.index, 'producer_id'] = default_scientist_id\n",
    "\n",
    "# Verify final state\n",
    "print(\"\\nFinal validation:\")\n",
    "print(f\"Duplicate names: {solar_cell_device_df['name'].duplicated().any()}\")\n",
    "print(f\"Missing required values (name): {solar_cell_device_df['name'].isna().sum()}\")\n",
    "print(f\"Invalid non-null owner references: {(solar_cell_device_df[solar_cell_device_df['owner_id'].notna()]['owner_id'].isin(valid_scientist_ids) == False).sum()}\")\n",
    "print(f\"Invalid non-null producer references: {(solar_cell_device_df[solar_cell_device_df['producer_id'].notna()]['producer_id'].isin(valid_scientist_ids) == False).sum()}\")\n",
    "\n",
    "# Display the cleaned data\n",
    "print(\"\\nCleaned solar cell device table:\")\n",
    "display(solar_cell_device_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022f2ad",
   "metadata": {},
   "source": [
    "### Prepare Solar Cell Pixel Table\n",
    "\n",
    "The solar_cell_pixel table requires:\n",
    "- solar_cell_id (foreign key to solar_cell_device.name)\n",
    "- pixel (pixel number/identifier)\n",
    "- active_area (area measurement for specific pixel)\n",
    "\n",
    "Important considerations:\n",
    "- The combination of solar_cell_id and pixel forms the PRIMARY KEY\n",
    "- Different solar cells can have pixels with the same names (e.g., 'a', 'b', 'c')\n",
    "- We need to preserve all pixel entries while ensuring uniqueness of the composite key\n",
    "\n",
    "Let's extract and validate this data from our logbook entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create solar cell pixel dataframe from original logbook data\n",
    "solar_cell_pixel_df = df[['Cell name', 'Pixel', 'Area']].copy()\n",
    "\n",
    "# Clean pixel data\n",
    "solar_cell_pixel_df['Pixel'] = solar_cell_pixel_df['Pixel'].astype(str)\n",
    "solar_cell_pixel_df['Pixel'] = solar_cell_pixel_df['Pixel'].str.strip()\n",
    "\n",
    "# Remove rows where pixel is missing or invalid\n",
    "solar_cell_pixel_df = solar_cell_pixel_df.dropna(subset=['Pixel'])\n",
    "solar_cell_pixel_df = solar_cell_pixel_df[solar_cell_pixel_df['Pixel'].str.lower() != 'nan']\n",
    "\n",
    "# Ensure area is a float\n",
    "solar_cell_pixel_df['active_area'] = pd.to_numeric(solar_cell_pixel_df['Area'], errors='coerce')\n",
    "\n",
    "# Get unique combinations of cell and pixel\n",
    "# Note: We don't deduplicate pixels alone as they can repeat across different cells\n",
    "solar_cell_pixel_df = solar_cell_pixel_df.drop_duplicates(subset=['Cell name', 'Pixel'])\n",
    "\n",
    "# Display initial state\n",
    "print(f\"Found {len(solar_cell_pixel_df)} unique cell-pixel combinations\")\n",
    "print(f\"Number of unique cells: {solar_cell_pixel_df['Cell name'].nunique()}\")\n",
    "print(f\"Number of unique pixel names: {solar_cell_pixel_df['Pixel'].nunique()}\")\n",
    "\n",
    "# Show distribution of pixel names across cells\n",
    "pixel_counts = solar_cell_pixel_df.groupby('Pixel').size()\n",
    "print(\"\\nPixel name frequency (showing most common):\")\n",
    "display(pixel_counts.sort_values(ascending=False).head())\n",
    "\n",
    "# Display initial data\n",
    "print(\"\\nInitial solar cell pixel table:\")\n",
    "display(solar_cell_pixel_df.head(25))\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(solar_cell_pixel_df.isnull().sum())\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nColumn data types:\")\n",
    "print(solar_cell_pixel_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5580bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and clean pixel data\n",
    "\n",
    "# 1. Check for invalid pixel values\n",
    "print(\"Unique pixel values:\")\n",
    "print(solar_cell_pixel_df['Pixel'].unique())\n",
    "\n",
    "# 2. Verify all cell names exist in solar_cell_device_df\n",
    "invalid_cells = ~solar_cell_pixel_df['Cell name'].isin(solar_cell_device_df['name'])\n",
    "if invalid_cells.any():\n",
    "    print(f\"\\nWARNING: Found {invalid_cells.sum()} pixels with invalid cell references\")\n",
    "    print(\"Invalid cell names:\")\n",
    "    print(solar_cell_pixel_df[invalid_cells]['Cell name'].unique())\n",
    "    \n",
    "    # Remove invalid entries\n",
    "    solar_cell_pixel_df = solar_cell_pixel_df[~invalid_cells]\n",
    "else:\n",
    "    print(\"✓ All cell names in solar_cell_pixel_df are valid\")\n",
    "\n",
    "# 3. Check for negative or zero areas\n",
    "invalid_areas = solar_cell_pixel_df['active_area'] <= 0\n",
    "if invalid_areas.any():\n",
    "    print(f\"\\nWARNING: Found {invalid_areas.sum()} pixels with invalid areas\")\n",
    "    print(\"Entries with invalid areas:\")\n",
    "    display(solar_cell_pixel_df[invalid_areas])\n",
    "    \n",
    "    # Set invalid areas to NULL\n",
    "    solar_cell_pixel_df.loc[invalid_areas, 'active_area'] = pd.NA\n",
    "else:\n",
    "    print(\"✓ All active_area values are valid\")\n",
    "\n",
    "# Display cleaned data\n",
    "print(\"\\nCleaned solar cell pixel table:\")\n",
    "display(solar_cell_pixel_df.head(25))\n",
    "\n",
    "# Final validation counts\n",
    "print(f\"\\nFinal validation:\")\n",
    "print(f\"Total unique pixels: {len(solar_cell_pixel_df)}\")\n",
    "print(f\"Unique cells: {solar_cell_pixel_df['Cell name'].nunique()}\")\n",
    "print(f\"Pixels without area: {solar_cell_pixel_df['active_area'].isna().sum()}\")\n",
    "print(f\"Invalid cell references: {(~solar_cell_pixel_df['Cell name'].isin(solar_cell_device_df['name'])).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aecb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final table structure for database upload\n",
    "solar_cell_pixel_upload_df = pd.DataFrame({\n",
    "    'solar_cell_id': solar_cell_pixel_df['Cell name'].astype('string'),  # VARCHAR(255)\n",
    "    'pixel': solar_cell_pixel_df['Pixel'].astype('string'),              # VARCHAR(255)\n",
    "    'active_area': solar_cell_pixel_df['active_area'].astype('float64')  # DOUBLE PRECISION\n",
    "})\n",
    "\n",
    "# Validate data types match database schema\n",
    "print(\"Data type validation:\")\n",
    "print(solar_cell_pixel_upload_df.dtypes)\n",
    "\n",
    "# Validate string lengths (VARCHAR(255) limit)\n",
    "max_solar_cell_id_len = solar_cell_pixel_upload_df['solar_cell_id'].str.len().max()\n",
    "max_pixel_len = solar_cell_pixel_upload_df['pixel'].str.len().max()\n",
    "print(f\"\\nVARCHAR length validation:\")\n",
    "print(f\"solar_cell_id max length: {max_solar_cell_id_len}/255\")\n",
    "print(f\"pixel max length: {max_pixel_len}/255\")\n",
    "\n",
    "# Ensure the combined solar_cell_id and pixel form a unique identifier\n",
    "duplicates = solar_cell_pixel_upload_df.duplicated(subset=['solar_cell_id', 'pixel'], keep=False)\n",
    "if duplicates.any():\n",
    "    print(\"\\nWARNING: Found duplicate pixel entries:\")\n",
    "    display(solar_cell_pixel_upload_df[duplicates].sort_values(['solar_cell_id', 'pixel']))\n",
    "    \n",
    "    # Keep the first occurrence of each pixel per cell\n",
    "    solar_cell_pixel_upload_df = solar_cell_pixel_upload_df.drop_duplicates(\n",
    "        subset=['solar_cell_id', 'pixel'],\n",
    "        keep='first'\n",
    "    )\n",
    "else:\n",
    "    print(\"✓ No duplicate pixel entries found\")\n",
    "\n",
    "# Validate numeric ranges\n",
    "print(\"\\nNumeric range validation:\")\n",
    "print(\"active_area range:\")\n",
    "print(f\"min: {solar_cell_pixel_upload_df['active_area'].min()}\")\n",
    "print(f\"max: {solar_cell_pixel_upload_df['active_area'].max()}\")\n",
    "\n",
    "# Final validation\n",
    "print(\"\\nSchema validation:\")\n",
    "print(f\"Primary key uniqueness: {not solar_cell_pixel_upload_df.duplicated(subset=['solar_cell_id', 'pixel']).any()}\")\n",
    "print(f\"Foreign key validation: {solar_cell_pixel_upload_df['solar_cell_id'].isin(solar_cell_device_df['name']).all()}\")\n",
    "print(f\"Total rows: {len(solar_cell_pixel_upload_df)}\")\n",
    "\n",
    "# Null checks\n",
    "print(\"\\nNull value checks:\")\n",
    "print(solar_cell_pixel_upload_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d20043",
   "metadata": {},
   "source": [
    "### Prepare MPP Tracking Channel Table\n",
    "\n",
    "The mpp_tracking_channel table requires:\n",
    "- board (INTEGER)\n",
    "- channel (INTEGER)\n",
    "\n",
    "Important considerations:\n",
    "- The combination of board and channel forms the PRIMARY KEY\n",
    "- Both fields are required (no NULL values allowed)\n",
    "- Values must be valid integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b04f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MPP tracking channel dataframe from original logbook data\n",
    "mpp_tracking_channel_df = df[['Board', 'Channel']].copy()\n",
    "\n",
    "# Drop any rows where either Board or Channel is missing\n",
    "mpp_tracking_channel_df = mpp_tracking_channel_df.dropna()\n",
    "\n",
    "# Convert to integers\n",
    "mpp_tracking_channel_df['board'] = mpp_tracking_channel_df['Board'].astype('int64')\n",
    "mpp_tracking_channel_df['channel'] = mpp_tracking_channel_df['Channel'].astype('int64')\n",
    "\n",
    "# Drop the original columns\n",
    "mpp_tracking_channel_df = mpp_tracking_channel_df[['board', 'channel']]\n",
    "\n",
    "# Add the additional required fields with default values\n",
    "# These will need to be updated with actual values later\n",
    "mpp_tracking_channel_df['address'] = pd.NA  # Will need actual address mapping\n",
    "mpp_tracking_channel_df['com_port'] = pd.NA  # Will need actual COM port mapping\n",
    "mpp_tracking_channel_df['current_limit'] = pd.NA  # Default current limit in mA\n",
    "\n",
    "# Remove duplicates\n",
    "mpp_tracking_channel_df = mpp_tracking_channel_df.drop_duplicates(subset=['board', 'channel'])\n",
    "\n",
    "# Display initial state\n",
    "print(f\"Found {len(mpp_tracking_channel_df)} unique board-channel combinations\")\n",
    "print(f\"Number of unique boards: {mpp_tracking_channel_df['board'].nunique()}\")\n",
    "print(f\"Number of unique channels: {mpp_tracking_channel_df['channel'].nunique()}\")\n",
    "\n",
    "# Show distribution of channels across boards\n",
    "channel_counts = mpp_tracking_channel_df.groupby('board')['channel'].nunique()\n",
    "print(\"\\nChannels per board:\")\n",
    "display(channel_counts)\n",
    "\n",
    "# Validate data\n",
    "print(\"\\nValidation:\")\n",
    "print(f\"Primary key uniqueness: {not mpp_tracking_channel_df.duplicated(subset=['board', 'channel']).any()}\")\n",
    "print(f\"Negative board values: {(mpp_tracking_channel_df['board'] < 0).sum()}\")\n",
    "print(f\"Negative channel values: {(mpp_tracking_channel_df['channel'] < 0).sum()}\")\n",
    "\n",
    "# Display the prepared data\n",
    "print(\"\\nPrepared MPP tracking channel table:\")\n",
    "display(mpp_tracking_channel_df.sort_values(['board', 'channel']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee4b222",
   "metadata": {},
   "source": [
    "### Prepare Measurement Connection Event Table\n",
    "\n",
    "We'll process the cleaned logbook data (`df`) that was prepared in the earlier cells. This dataframe contains:\n",
    "- Cleaned and validated dates (Date installed, Date removed)\n",
    "- Integer-converted Board and Channel numbers\n",
    "- Cleaned string fields for Cell name, Pixel, etc.\n",
    "- Proper handling of missing values\n",
    "\n",
    "We'll split the preparation into several steps:\n",
    "1. Extract connection and disconnection events\n",
    "2. Process and validate dates\n",
    "3. Add required fields and convert data types\n",
    "4. Validate referential integrity\n",
    "5. Validate event sequence logic\n",
    "6. Final data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca54777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first verify the state of our input dataframe\n",
    "print(\"Input dataframe (cleaned logbook data) summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nSample of input data:\")\n",
    "display(df[['Cell name', 'Pixel', 'Board', 'Channel', 'Date installed', 'Date removed']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf249c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract Connection Events from cleaned logbook data\n",
    "logbook_connection_events = df[[\n",
    "    'Cell name',\n",
    "    'Pixel',\n",
    "    'Board',\n",
    "    'Channel',\n",
    "    'Temp sensor',\n",
    "    'Date installed'\n",
    "]].copy()\n",
    "\n",
    "# Add event_type column for connections\n",
    "logbook_connection_events['event_type'] = 'CONNECTED'\n",
    "\n",
    "print(\"Connection events extracted from logbook:\")\n",
    "print(f\"Total events: {len(logbook_connection_events)}\")\n",
    "print(\"\\nSample of connection events:\")\n",
    "display(logbook_connection_events.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46628d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract Disconnection Events from cleaned logbook data\n",
    "logbook_disconnection_events = df[[\n",
    "    'Cell name',\n",
    "    'Pixel',\n",
    "    'Board',\n",
    "    'Channel',\n",
    "    'Temp sensor',\n",
    "    'Date removed'\n",
    "]].copy()\n",
    "\n",
    "# Add event_type column for disconnections\n",
    "logbook_disconnection_events['event_type'] = 'DISCONNECTED'\n",
    "\n",
    "# Rename Date removed to match connection_datetime field\n",
    "logbook_disconnection_events = logbook_disconnection_events.rename(columns={'Date removed': 'Date installed'})\n",
    "\n",
    "# Remove rows where Date removed is null (currently connected cells)\n",
    "logbook_disconnection_events = logbook_disconnection_events.dropna(subset=['Date installed'])\n",
    "\n",
    "print(\"Disconnection events extracted from logbook:\")\n",
    "print(f\"Total events: {len(logbook_disconnection_events)}\")\n",
    "print(f\"Currently connected cells (no disconnection date): {len(df) - len(logbook_disconnection_events)}\")\n",
    "print(\"\\nSample of disconnection events:\")\n",
    "display(logbook_disconnection_events.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Combine and Process Events\n",
    "# Combine connection and disconnection events into a new measurement events dataframe\n",
    "measurement_connection_df = pd.concat([logbook_connection_events, logbook_disconnection_events], ignore_index=True)\n",
    "\n",
    "# Rename columns to match database schema\n",
    "measurement_connection_df.columns = [\n",
    "    'solar_cell_id',\n",
    "    'pixel',\n",
    "    'tracking_channel_board',\n",
    "    'tracking_channel_channel',\n",
    "    'temperature_sensor_id',\n",
    "    'connection_datetime',\n",
    "    'event_type'\n",
    "]\n",
    "\n",
    "# Add default fields\n",
    "measurement_connection_df['irradiance_sensor_id'] = pd.NA\n",
    "measurement_connection_df['mppt_mode'] = pd.NA\n",
    "measurement_connection_df['mppt_polarity'] = pd.NA\n",
    "\n",
    "print(\"Combined measurement connection events:\")\n",
    "print(f\"Total events: {len(measurement_connection_df)}\")\n",
    "print(f\"From which:\")\n",
    "print(f\"- Connection events: {len(logbook_connection_events)}\")\n",
    "print(f\"- Disconnection events: {len(logbook_disconnection_events)}\")\n",
    "print(\"\\nColumns in final dataframe:\")\n",
    "print(measurement_connection_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ad22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Data Type Conversions\n",
    "# First create database connection\n",
    "engine = create_engine(\n",
    "    f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "# Convert board and channel to integers\n",
    "measurement_connection_df['tracking_channel_board'] = measurement_connection_df['tracking_channel_board'].astype('int64')\n",
    "measurement_connection_df['tracking_channel_channel'] = measurement_connection_df['tracking_channel_channel'].astype('int64')\n",
    "\n",
    "# Print current state of datetime column\n",
    "print(\"\\nBefore timezone localization:\")\n",
    "print(\"connection_datetime type:\", measurement_connection_df['connection_datetime'].dtype)\n",
    "print(\"Sample value:\", measurement_connection_df['connection_datetime'].iloc[0])\n",
    "\n",
    "# Add timezone info (dates are already datetime objects)\n",
    "measurement_connection_df['connection_datetime'] = measurement_connection_df['connection_datetime'].dt.tz_localize('UTC')\n",
    "\n",
    "print(\"\\nAfter timezone localization:\")\n",
    "print(\"connection_datetime type:\", measurement_connection_df['connection_datetime'].dtype)\n",
    "# Now add timezone if needed\n",
    "if pd.api.types.is_datetime64_dtype(measurement_connection_df['connection_datetime']):\n",
    "    print(\"\\nAdding timezone info...\")\n",
    "    measurement_connection_df['connection_datetime'] = measurement_connection_df['connection_datetime'].dt.tz_localize('UTC')\n",
    "\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(\"connection_datetime type:\", measurement_connection_df['connection_datetime'].dtype)\n",
    "print(\"Sample value:\", measurement_connection_df['connection_datetime'].iloc[0])\n",
    "\n",
    "# Get temperature sensor IDs from the database\n",
    "def get_sensor_id_map(engine):\n",
    "    \"\"\"Get mapping of sensor identifiers to their UUIDs from the database.\"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        # Query all temperature sensors\n",
    "        result = conn.execute(text(\"\"\"\n",
    "            SELECT temperature_sensor_id, sensor_identifier \n",
    "            FROM temperature_sensor\n",
    "            WHERE sensor_identifier LIKE 'm7004_ID_%'\n",
    "        \"\"\"))\n",
    "        \n",
    "        # Create mapping from sensor ID part to UUID\n",
    "        # e.g., '37F6F9511A64FF28' -> UUID from database\n",
    "        return {\n",
    "            identifier.replace('m7004_ID_', ''): uuid \n",
    "            for uuid, identifier in result\n",
    "        }\n",
    "\n",
    "# Get mapping of sensor identifiers to their database UUIDs\n",
    "sensor_uuid_map = get_sensor_id_map(engine)\n",
    "\n",
    "print(f\"\\nFound {len(sensor_uuid_map)} temperature sensors in database\")\n",
    "\n",
    "# Check which sensors from the logbook exist in our mapping\n",
    "valid_sensor_mask = measurement_connection_df['temperature_sensor_id'].isin(sensor_uuid_map.keys())\n",
    "if valid_sensor_mask.any():\n",
    "    print(f\"Found {valid_sensor_mask.sum()} valid temperature sensor entries\")\n",
    "else:\n",
    "    print(\"No valid temperature sensor entries found\")\n",
    "\n",
    "# List any sensors that appear in the logbook but aren't in the database\n",
    "missing_sensors = set(measurement_connection_df['temperature_sensor_id'].dropna()) - set(sensor_uuid_map.keys())\n",
    "if missing_sensors:\n",
    "    print(f\"Note: {len(missing_sensors)} sensors from logbook are not in database (will be set to NULL)\")\n",
    "    if len(missing_sensors) <= 10:\n",
    "        print(\"Missing sensors:\", sorted(missing_sensors))\n",
    "\n",
    "# Map sensors to their UUIDs, setting non-matching ones to NULL\n",
    "measurement_connection_df['temperature_sensor_id'] = measurement_connection_df['temperature_sensor_id'].map(sensor_uuid_map)\n",
    "\n",
    "# Print mapping statistics\n",
    "print(f\"\\nTemperature sensor mapping summary:\")\n",
    "print(f\"Total entries: {len(measurement_connection_df)}\")\n",
    "print(f\"Found in database: {valid_sensor_mask.sum()}\")\n",
    "print(f\"Successfully mapped: {measurement_connection_df['temperature_sensor_id'].notna().sum()}\")\n",
    "print(f\"Set to NULL: {measurement_connection_df['temperature_sensor_id'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(measurement_connection_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054de34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Validate Referential Integrity\n",
    "print(\"Running referential integrity validation...\")\n",
    "\n",
    "# Check solar_cell_pixel references\n",
    "valid_pixels = solar_cell_pixel_upload_df.apply(\n",
    "    lambda row: f\"{row['solar_cell_id']}_{row['pixel']}\",\n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "test_pixels = measurement_connection_df.apply(\n",
    "    lambda row: f\"{row['solar_cell_id']}_{row['pixel']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "invalid_pixels = ~test_pixels.isin(valid_pixels)\n",
    "if invalid_pixels.any():\n",
    "    print(\"WARNING: Invalid solar cell pixel references found\")\n",
    "    print(\"Number of invalid references:\", invalid_pixels.sum())\n",
    "    display(measurement_connection_df[invalid_pixels].head())\n",
    "else:\n",
    "    print(\"✓ All solar cell pixel references are valid\")\n",
    "\n",
    "# Check mpp_tracking_channel references\n",
    "valid_channels = mpp_tracking_channel_df.apply(\n",
    "    lambda row: f\"{row['board']}_{row['channel']}\",\n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "test_channels = measurement_connection_df.apply(\n",
    "    lambda row: f\"{row['tracking_channel_board']}_{row['tracking_channel_channel']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "invalid_channels = ~test_channels.isin(valid_channels)\n",
    "if invalid_channels.any():\n",
    "    print(\"\\nWARNING: Invalid tracking channel references found\")\n",
    "    print(\"Number of invalid references:\", invalid_channels.sum())\n",
    "    display(measurement_connection_df[invalid_channels].head())\n",
    "else:\n",
    "    print(\"✓ All tracking channel references are valid\")\n",
    "\n",
    "print(\"\\nReferential integrity validation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ad22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Validate Event Sequence Logic\n",
    "print(\"Running event sequence validation...\")\n",
    "\n",
    "def check_event_sequence(group):\n",
    "    \"\"\"Check if events follow proper sequence\"\"\"\n",
    "    events = group.sort_values('connection_datetime')\n",
    "    issues = []\n",
    "    \n",
    "    # Check for standalone disconnection\n",
    "    if len(events) == 1 and events.iloc[0]['event_type'] == 'DISCONNECTED':\n",
    "        issues.append(\"Disconnection without prior connection\")\n",
    "    \n",
    "    # Check for consecutive same events\n",
    "    last_event_type = None\n",
    "    for _, event in events.iterrows():\n",
    "        if event['event_type'] == last_event_type:\n",
    "            issues.append(f\"Consecutive {event['event_type']} events\")\n",
    "        last_event_type = event['event_type']\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Group by cell and pixel and check sequence\n",
    "sequence_issues = []\n",
    "for (cell_id, pixel), group in measurement_connection_df.groupby(['solar_cell_id', 'pixel']):\n",
    "    issues = check_event_sequence(group)\n",
    "    if issues:\n",
    "        sequence_issues.append({\n",
    "            'cell_id': cell_id,\n",
    "            'pixel': pixel,\n",
    "            'issues': issues\n",
    "        })\n",
    "\n",
    "if sequence_issues:\n",
    "    print(\"Event sequence issues found:\")\n",
    "    for issue in sequence_issues:\n",
    "        print(f\"\\nCell: {issue['cell_id']}, Pixel: {issue['pixel']}\")\n",
    "        for problem in issue['issues']:\n",
    "            print(f\"  - {problem}\")\n",
    "else:\n",
    "    print(\"✓ All event sequences are valid - no issues found\")\n",
    "    print(f\"  - Validated {len(measurement_connection_df)} events\")\n",
    "    print(f\"  - Checked {measurement_connection_df.groupby(['solar_cell_id', 'pixel']).ngroups} unique cell-pixel combinations\")\n",
    "\n",
    "print(\"\\nEvent sequence validation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Validate Chronological Order\n",
    "print(\"Running chronological order validation...\")\n",
    "\n",
    "def check_chronological_order(group):\n",
    "    \"\"\"Check if events are in proper chronological order\"\"\"\n",
    "    events = group.sort_values('connection_datetime')\n",
    "    issues = []\n",
    "    \n",
    "    dates = events['connection_datetime'].tolist()\n",
    "    types = events['event_type'].tolist()\n",
    "    \n",
    "    for i in range(len(dates)-1):\n",
    "        if types[i] == 'DISCONNECTED' and types[i+1] == 'CONNECTED':\n",
    "            if dates[i] >= dates[i+1]:\n",
    "                issues.append(f\"Disconnection ({dates[i]}) not before next connection ({dates[i+1]})\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Check chronological order for each cell/pixel\n",
    "chronology_issues = []\n",
    "total_events_checked = 0\n",
    "unique_pairs_checked = 0\n",
    "\n",
    "for (cell_id, pixel), group in measurement_connection_df.groupby(['solar_cell_id', 'pixel']):\n",
    "    issues = check_chronological_order(group)\n",
    "    if issues:\n",
    "        chronology_issues.append({\n",
    "            'cell_id': cell_id,\n",
    "            'pixel': pixel,\n",
    "            'issues': issues\n",
    "        })\n",
    "    total_events_checked += len(group)\n",
    "    unique_pairs_checked += 1\n",
    "\n",
    "if chronology_issues:\n",
    "    print(\"Chronological order issues found:\")\n",
    "    for issue in chronology_issues:\n",
    "        print(f\"\\nCell: {issue['cell_id']}, Pixel: {issue['pixel']}\")\n",
    "        for problem in issue['issues']:\n",
    "            print(f\"  - {problem}\")\n",
    "else:\n",
    "    print(\"✓ All events are in proper chronological order\")\n",
    "    print(f\"  - Validated {total_events_checked} events\")\n",
    "    print(f\"  - Checked {unique_pairs_checked} unique cell-pixel combinations\")\n",
    "\n",
    "print(\"\\nChronological order validation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Final Data Summary\n",
    "print(\"Final measurement connection event data summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total events: {len(measurement_connection_df)}\")\n",
    "print(f\"Connection events: {len(measurement_connection_df[measurement_connection_df['event_type'] == 'CONNECTED'])}\")\n",
    "print(f\"Disconnection events: {len(measurement_connection_df[measurement_connection_df['event_type'] == 'DISCONNECTED'])}\")\n",
    "print(f\"Unique cells: {measurement_connection_df['solar_cell_id'].nunique()}\")\n",
    "print(f\"Unique pixels: {measurement_connection_df.groupby('solar_cell_id')['pixel'].nunique().sum()}\")\n",
    "print(f\"Date range: {measurement_connection_df['connection_datetime'].min()} to {measurement_connection_df['connection_datetime'].max()}\")\n",
    "\n",
    "print(\"\\nColumns with null values:\")\n",
    "null_counts = measurement_connection_df.isnull().sum()\n",
    "print(null_counts[null_counts > 0])\n",
    "\n",
    "# Display sample of final data\n",
    "print(\"\\nSample of prepared data:\")\n",
    "display(measurement_connection_df.sample(n=min(5, len(measurement_connection_df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f46ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f94a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
