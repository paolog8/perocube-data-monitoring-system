{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7e94ca",
   "metadata": {},
   "source": [
    "# Perocube Logbook Data Upload Notebook\n",
    "\n",
    "This notebook processes and uploads logbook data from the Perocube Excel file to the TimescaleDB database.\n",
    "\n",
    "## Purpose\n",
    "- Read logbook data from 'PeroCube_logbook_example.xlsx'\n",
    "- Parse and validate the data\n",
    "- Upload the data to the TimescaleDB database\n",
    "- Avoid duplicate data entries\n",
    "\n",
    "## Prerequisites\n",
    "- Running TimescaleDB instance (configured in docker-compose.yml)\n",
    "- Access to the Perocube logbook Excel file\n",
    "- Environment variables configured in .env file (for database connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e0476",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import required libraries and install any missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# Database libraries\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af362b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install psycopg2-binary sqlalchemy pandas tqdm pathlib python-dotenv openpyxl\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41d919",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load configuration from environment variables and set up constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for the .env file two directories up from the notebook location\n",
    "dotenv_path = Path(\"../../.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Database configuration from environment variables with fallbacks\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'port': int(os.getenv('DB_PORT', 5432)),\n",
    "    'database': os.getenv('DB_NAME', 'perocube'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'postgres')\n",
    "}\n",
    "\n",
    "# Print database connection info (excluding password)\n",
    "print(f\"Database connection: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']} as {DB_CONFIG['user']}\")\n",
    "\n",
    "# Data directory and file configuration\n",
    "ROOT_DIRECTORY = os.getenv('DEFAULT_DATA_DIR', \"../../sample_data/datasets/PeroCube-sample-data\")\n",
    "LOGBOOK_FILE = \"PeroCube_logbook_example.xlsx\"\n",
    "LOGBOOK_SHEET = \"Perocube history\"\n",
    "\n",
    "# Batch size for database operations\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd22ffb",
   "metadata": {},
   "source": [
    "## 3. Read and Process Logbook Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full path to the logbook file\n",
    "logbook_path = Path(ROOT_DIRECTORY) / LOGBOOK_FILE\n",
    "\n",
    "# Read the Excel sheet, skip first row and use second row as header\n",
    "try:\n",
    "    df = pd.read_excel(logbook_path, sheet_name=LOGBOOK_SHEET, header=1)\n",
    "    print(f\"Successfully read {len(df)} rows from {LOGBOOK_FILE}\")\n",
    "    \n",
    "    # Display the first few rows and data info\n",
    "    print(\"\\nColumn names:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    print(\"\\nFirst few rows of the data:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nDataset information:\")\n",
    "    display(df.info())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading Excel file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze current dataframe state\n",
    "print(\"Checking for unnamed columns:\")\n",
    "unnamed_cols = [col for col in df.columns if 'Unnamed' in str(col)]\n",
    "print(f\"Unnamed columns found: {unnamed_cols}\")\n",
    "\n",
    "print(\"\\nCurrent data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nTotal rows with all missing values:\")\n",
    "print(df.isna().all(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframe\n",
    "\n",
    "# 1. Remove unnamed columns\n",
    "df = df.loc[:, ~df.columns.str.contains('Unnamed')]\n",
    "\n",
    "# 2. Drop rows where all values are missing\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "# Print cleaning results\n",
    "print(\"Dataframe shape after cleaning:\")\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "# Display updated column list\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display updated missing values count\n",
    "print(\"\\nMissing values per column after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display first few rows of cleaned data\n",
    "print(\"\\nFirst few rows of cleaned data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e47291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove completely empty Comment 2 column\n",
    "df = df.drop('Comment 2', axis=1)\n",
    "\n",
    "# Print updated dataframe info\n",
    "print(\"Dataframe shape after removing Comment 2:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# Display updated column list\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display first few rows of cleaned data\n",
    "print(\"\\nFirst few rows of cleaned data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee62e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data types\n",
    "\n",
    "# 1. Convert date columns to datetime\n",
    "df['Date removed'] = pd.to_datetime(df['Date removed'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Date installed'] = pd.to_datetime(df['Date installed'], format='%d.%m.%Y', errors='coerce')\n",
    "\n",
    "# 2. Convert numeric columns\n",
    "numeric_columns = ['Board', 'Channel', 'Status', 'Area', 'Init.PCE']\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 3. Ensure string columns are properly formatted (strip whitespace)\n",
    "string_columns = ['Cell type', 'Cell name', 'Pixel', 'Encap.', 'Structure', \n",
    "                  'Producer', 'Owner', 'Project', 'Temp sensor', 'Comment 1']\n",
    "for col in string_columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    # Replace 'nan' strings with actual NaN\n",
    "    df[col] = df[col].replace('nan', pd.NA)\n",
    "\n",
    "# Display the updated data types\n",
    "print(\"Updated data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display sample of the data to verify conversions\n",
    "print(\"\\nSample of converted data:\")\n",
    "display(df.head(25))\n",
    "\n",
    "# Check for any conversion issues (invalid dates or numbers)\n",
    "print(\"\\nCount of NaN values after type conversion:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dcff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where both dates are null\n",
    "original_rows = len(df)\n",
    "df = df.dropna(subset=['Date removed', 'Date installed'], how='all')\n",
    "\n",
    "# Print results\n",
    "print(f\"Removed {original_rows - len(df)} rows where both dates were missing\")\n",
    "print(f\"Remaining rows: {len(df)}\")\n",
    "\n",
    "# Display missing values count after removal\n",
    "print(\"\\nMissing values per column after removing rows with missing dates:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Display sample of remaining data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rows where installation date is missing\n",
    "missing_install_date = df[df['Date installed'].isna()]\n",
    "\n",
    "print(f\"Found {len(missing_install_date)} rows with missing installation date:\\n\")\n",
    "display(missing_install_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing installation dates with January 1st, 2020\n",
    "default_install_date = pd.to_datetime('2020-01-01')\n",
    "df['Date installed'] = df['Date installed'].fillna(default_install_date)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Checking for any remaining missing installation dates:\")\n",
    "print(f\"Missing installation dates: {df['Date installed'].isna().sum()}\")\n",
    "\n",
    "# Display the rows that were updated\n",
    "print(\"\\nVerifying the rows that were updated:\")\n",
    "display(df[df['Date installed'] == default_install_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Board and Channel to integers (as per database schema)\n",
    "df['Board'] = df['Board'].astype('Int64')  # Using Int64 to handle NaN values\n",
    "df['Channel'] = df['Channel'].astype('Int64')  # Using Int64 to handle NaN values\n",
    "\n",
    "# Verify the conversion\n",
    "print(\"Updated data types for Board and Channel:\")\n",
    "print(df[['Board', 'Channel']].dtypes)\n",
    "\n",
    "# Display a sample to verify the conversion\n",
    "print(\"\\nSample of Board and Channel data:\")\n",
    "display(df[['Board', 'Channel']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee39dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align numeric types with database schema\n",
    "df['Area'] = df['Area'].astype('float64')\n",
    "df['Init.PCE'] = df['Init.PCE'].astype('float64')\n",
    "\n",
    "# Validate and truncate string columns to match VARCHAR(255)\n",
    "string_columns = ['Cell type', 'Cell name', 'Pixel', 'Encap.', 'Structure', \n",
    "                 'Producer', 'Owner', 'Project', 'Temp sensor', 'Comment 1']\n",
    "\n",
    "for col in string_columns:\n",
    "    # Check if any string is longer than 255 characters\n",
    "    mask = df[col].str.len() > 255\n",
    "    if mask.any():\n",
    "        print(f\"Warning: Found {mask.sum()} values in {col} longer than 255 characters. Truncating...\")\n",
    "        df.loc[mask, col] = df.loc[mask, col].str.slice(0, 255)\n",
    "\n",
    "# Display updated data types\n",
    "print(\"\\nUpdated data types after database alignment:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for any values that might be too long\n",
    "print(\"\\nMaximum string lengths:\")\n",
    "for col in string_columns:\n",
    "    max_len = df[col].str.len().max()\n",
    "    print(f\"{col}: {max_len}\")\n",
    "\n",
    "# Display sample of numeric columns\n",
    "print(\"\\nSample of numeric columns:\")\n",
    "display(df[['Area', 'Init.PCE']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7d277",
   "metadata": {},
   "source": [
    "## 4. Database Mapping Reference\n",
    "\n",
    "Based on the database schema in `tables.sql`, our logbook data maps to the following tables:\n",
    "\n",
    "### 1. scientist table\n",
    "- Maps from: 'Owner', 'Producer' columns\n",
    "- Fields:\n",
    "  - scientist_id (UUID, generated)\n",
    "  - name (from 'Owner' and 'Producer')\n",
    "\n",
    "### 2. solar_cell_device table\n",
    "- Maps from multiple columns\n",
    "- Fields:\n",
    "  - nomad_id (UUID, generated)\n",
    "  - technology (from 'Cell type')\n",
    "  - area (from 'Area')\n",
    "  - initial_pce (from 'Init.PCE')\n",
    "  - date_produced (can use 'Date installed')\n",
    "  - encapsulation (from 'Encap.')\n",
    "  - owner_id (link to scientist table)\n",
    "  - producer_id (link to scientist table)\n",
    "\n",
    "### 3. solar_cell_pixel table\n",
    "- Maps from pixel-specific data\n",
    "- Fields:\n",
    "  - solar_cell_id (from generated device UUID)\n",
    "  - pixel (from 'Pixel' column)\n",
    "  - active_area (from 'Area')\n",
    "\n",
    "### 4. mpp_tracking_channel table\n",
    "- Maps from channel data\n",
    "- Fields:\n",
    "  - board (from 'Board')\n",
    "  - channel (from 'Channel')\n",
    "\n",
    "### 5. measurement_connection_event table\n",
    "- Maps connection events\n",
    "- Fields:\n",
    "  - solar_cell_id (from generated device UUID)\n",
    "  - pixel (from 'Pixel')\n",
    "  - tracking_channel_board (from 'Board')\n",
    "  - tracking_channel_channel (from 'Channel')\n",
    "  - temperature_sensor_id (from 'Temp sensor')\n",
    "  - connection_datetime (from 'Date installed')\n",
    "\n",
    "This mapping shows we'll need to:\n",
    "1. Generate UUIDs for new scientists and devices\n",
    "2. Handle the relationships between tables\n",
    "3. Convert dates to proper timestamp format\n",
    "4. Validate data against database constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d021e",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Database Upload\n",
    "\n",
    "We'll prepare the data for each table in the database schema, starting with the scientist table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Get unique scientists from both Owner and Producer columns\n",
    "scientists = pd.concat([df['Owner'].dropna(), df['Producer'].dropna()]).unique()\n",
    "\n",
    "# Create scientist dataframe\n",
    "scientist_df = pd.DataFrame({\n",
    "    'scientist_id': [uuid.uuid4() for _ in range(len(scientists))],\n",
    "    'name': scientists\n",
    "})\n",
    "\n",
    "# Create a mapping dictionary for later use\n",
    "scientist_id_map = dict(zip(scientist_df['name'], scientist_df['scientist_id']))\n",
    "\n",
    "# Display the results\n",
    "print(f\"Found {len(scientist_df)} unique scientists\")\n",
    "print(\"\\nScientist table preview:\")\n",
    "display(scientist_df)\n",
    "\n",
    "print(\"\\nValidating unique constraints:\")\n",
    "print(f\"Duplicate names: {scientist_df['name'].duplicated().sum()}\")\n",
    "print(f\"Duplicate UUIDs: {scientist_df['scientist_id'].duplicated().sum()}\")\n",
    "\n",
    "# Store the mapping for later use\n",
    "print(\"\\nScientist ID mapping (first few entries):\")\n",
    "for name, id in list(scientist_id_map.items())[:5]:\n",
    "    print(f\"{name}: {id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00252df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up scientist data\n",
    "\n",
    "# Remove NA values\n",
    "scientist_df = scientist_df.dropna()\n",
    "\n",
    "# Clean up names\n",
    "# 1. Strip whitespace\n",
    "# 2. Replace multiple spaces with single space\n",
    "# 3. Remove any trailing commas or periods\n",
    "scientist_df['name'] = scientist_df['name'].str.strip()\n",
    "scientist_df['name'] = scientist_df['name'].str.replace(r'\\s+', ' ', regex=True)\n",
    "scientist_df['name'] = scientist_df['name'].str.replace(r'[,.]$', '', regex=True)\n",
    "\n",
    "# Update the mapping dictionary\n",
    "scientist_id_map = dict(zip(scientist_df['name'], scientist_df['scientist_id']))\n",
    "\n",
    "# Display cleaned results\n",
    "print(f\"After cleaning, found {len(scientist_df)} scientists\")\n",
    "print(\"\\nCleaned scientist table:\")\n",
    "display(scientist_df)\n",
    "\n",
    "# Verify no duplicates or NA values remain\n",
    "print(\"\\nValidating cleaned data:\")\n",
    "print(f\"Null values: {scientist_df['name'].isna().sum()}\")\n",
    "print(f\"Duplicate names: {scientist_df['name'].duplicated().sum()}\")\n",
    "print(f\"Duplicate UUIDs: {scientist_df['scientist_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional cleaning: Remove <NA> strings and ensure no invalid values remain\n",
    "print(\"Checking for '<NA>' values...\")\n",
    "\n",
    "# Check for '<NA>' strings\n",
    "na_mask = scientist_df['name'].isin(['<NA>', 'NA', '<na>', 'na'])\n",
    "if na_mask.any():\n",
    "    print(f\"Found {na_mask.sum()} '<NA>' values. Removing them...\")\n",
    "    scientist_df = scientist_df[~na_mask]\n",
    "\n",
    "# Update the mapping dictionary again\n",
    "scientist_id_map = dict(zip(scientist_df['name'], scientist_df['scientist_id']))\n",
    "\n",
    "# Validate final results\n",
    "print(\"\\nFinal validation:\")\n",
    "print(f\"Total scientists after removing <NA>: {len(scientist_df)}\")\n",
    "print(f\"Any remaining NA values: {scientist_df['name'].isna().any()}\")\n",
    "print(f\"Any remaining <NA> strings: {scientist_df['name'].isin(['<NA>', 'NA', '<na>', 'na']).any()}\")\n",
    "\n",
    "# Display final cleaned data\n",
    "print(\"\\nFinal cleaned scientist table:\")\n",
    "display(scientist_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
