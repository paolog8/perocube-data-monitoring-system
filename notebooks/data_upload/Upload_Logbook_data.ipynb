{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7e94ca",
   "metadata": {},
   "source": [
    "# Perocube Logbook Data Upload Notebook\n",
    "\n",
    "This notebook processes and uploads logbook data from the Perocube Excel file to the TimescaleDB database.\n",
    "\n",
    "## Purpose\n",
    "- Read logbook data from 'PeroCube_logbook_example.xlsx'\n",
    "- Parse and validate the data\n",
    "- Upload the data to the TimescaleDB database\n",
    "- Avoid duplicate data entries\n",
    "\n",
    "## Prerequisites\n",
    "- Running TimescaleDB instance (configured in docker-compose.yml)\n",
    "- Access to the Perocube logbook Excel file\n",
    "- Environment variables configured in .env file (for database connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e0476",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import required libraries and install any missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# Database libraries\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af362b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install psycopg2-binary sqlalchemy pandas tqdm pathlib python-dotenv openpyxl\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41d919",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load configuration from environment variables and set up constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for the .env file two directories up from the notebook location\n",
    "dotenv_path = Path(\"../../.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Database configuration from environment variables with fallbacks\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'port': int(os.getenv('DB_PORT', 5432)),\n",
    "    'database': os.getenv('DB_NAME', 'perocube'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'postgres')\n",
    "}\n",
    "\n",
    "# Print database connection info (excluding password)\n",
    "print(f\"Database connection: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']} as {DB_CONFIG['user']}\")\n",
    "\n",
    "# Data directory and file configuration\n",
    "ROOT_DIRECTORY = os.getenv('DEFAULT_DATA_DIR', \"../../sample_data/datasets/PeroCube-sample-data\")\n",
    "LOGBOOK_FILE = \"PeroCube_logbook_example.xlsx\"\n",
    "LOGBOOK_SHEET = \"Perocube history\"\n",
    "\n",
    "# Batch size for database operations\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd22ffb",
   "metadata": {},
   "source": [
    "## 3. Read and Process Logbook Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full path to the logbook file\n",
    "logbook_path = Path(ROOT_DIRECTORY) / LOGBOOK_FILE\n",
    "\n",
    "# Read the Excel sheet, skip first row and use second row as header\n",
    "try:\n",
    "    df = pd.read_excel(logbook_path, sheet_name=LOGBOOK_SHEET, header=1)\n",
    "    print(f\"Successfully read {len(df)} rows from {LOGBOOK_FILE}\")\n",
    "    \n",
    "    # Display the first few rows and data info\n",
    "    print(\"\\nColumn names:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    print(\"\\nFirst few rows of the data:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nDataset information:\")\n",
    "    display(df.info())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading Excel file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze current dataframe state\n",
    "print(\"Checking for unnamed columns:\")\n",
    "unnamed_cols = [col for col in df.columns if 'Unnamed' in str(col)]\n",
    "print(f\"Unnamed columns found: {unnamed_cols}\")\n",
    "\n",
    "print(\"\\nCurrent data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nTotal rows with all missing values:\")\n",
    "print(df.isna().all(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframe\n",
    "\n",
    "# 1. Remove unnamed columns\n",
    "df = df.loc[:, ~df.columns.str.contains('Unnamed')]\n",
    "\n",
    "# 2. Drop rows where all values are missing\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "# Print cleaning results\n",
    "print(\"Dataframe shape after cleaning:\")\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "# Display updated column list\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display updated missing values count\n",
    "print(\"\\nMissing values per column after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display first few rows of cleaned data\n",
    "print(\"\\nFirst few rows of cleaned data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e47291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove completely empty Comment 2 column\n",
    "df = df.drop('Comment 2', axis=1)\n",
    "\n",
    "# Print updated dataframe info\n",
    "print(\"Dataframe shape after removing Comment 2:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# Display updated column list\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display first few rows of cleaned data\n",
    "print(\"\\nFirst few rows of cleaned data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee62e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data types\n",
    "\n",
    "# 1. Convert date columns to datetime\n",
    "df['Date removed'] = pd.to_datetime(df['Date removed'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Date installed'] = pd.to_datetime(df['Date installed'], format='%d.%m.%Y', errors='coerce')\n",
    "\n",
    "# 2. Convert numeric columns\n",
    "numeric_columns = ['Board', 'Channel', 'Status', 'Area', 'Init.PCE']\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 3. Ensure string columns are properly formatted (strip whitespace)\n",
    "string_columns = ['Cell type', 'Cell name', 'Pixel', 'Encap.', 'Structure', \n",
    "                  'Producer', 'Owner', 'Project', 'Temp sensor', 'Comment 1']\n",
    "for col in string_columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    # Replace 'nan' strings with actual NaN\n",
    "    df[col] = df[col].replace('nan', pd.NA)\n",
    "\n",
    "# Display the updated data types\n",
    "print(\"Updated data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display sample of the data to verify conversions\n",
    "print(\"\\nSample of converted data:\")\n",
    "display(df.head(25))\n",
    "\n",
    "# Check for any conversion issues (invalid dates or numbers)\n",
    "print(\"\\nCount of NaN values after type conversion:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dcff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where both dates are null\n",
    "original_rows = len(df)\n",
    "df = df.dropna(subset=['Date removed', 'Date installed'], how='all')\n",
    "\n",
    "# Print results\n",
    "print(f\"Removed {original_rows - len(df)} rows where both dates were missing\")\n",
    "print(f\"Remaining rows: {len(df)}\")\n",
    "\n",
    "# Display missing values count after removal\n",
    "print(\"\\nMissing values per column after removing rows with missing dates:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Display sample of remaining data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rows where installation date is missing\n",
    "missing_install_date = df[df['Date installed'].isna()]\n",
    "\n",
    "print(f\"Found {len(missing_install_date)} rows with missing installation date:\\n\")\n",
    "display(missing_install_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing installation dates with January 1st, 2020\n",
    "default_install_date = pd.to_datetime('2020-01-01')\n",
    "df['Date installed'] = df['Date installed'].fillna(default_install_date)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Checking for any remaining missing installation dates:\")\n",
    "print(f\"Missing installation dates: {df['Date installed'].isna().sum()}\")\n",
    "\n",
    "# Display the rows that were updated\n",
    "print(\"\\nVerifying the rows that were updated:\")\n",
    "display(df[df['Date installed'] == default_install_date])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
